\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{APM462: Nonlinear Optimization}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{APM462: Nonlinear Optimization}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    
    \section{Preliminaries}
    \subsection{Mean Value Theorems and Taylor Approximations.}
    \begin{definition}
        Let $f: S \subseteq \R^n \to \R$, the \textbf{gradient} of $f$ at $x \in S$, if exists, is a vector $\nabla f(x) \in \R^n$ characterized by the property
        \begin{equation}
            \lim_{v \to 0} \frac{f(x+v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{definition}
    
    \begin{theorem}[The First Order Mean Value Theorem]
        Let $f \in C^1(\R^n, \R)$, then for any $x, v \in \R^n$, there exists some $\theta \in (0, 1)$ such that
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x + \theta v) \cdot v
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        Let $x, v \in \R^n$, define $g(t) := f(x + tv) \in C^1(\R, \R)$. \\
        By the mean value theorem on $\R^\R$, there exists $\theta \in (0, 1)$ such that $g(0+1) = g(0) + g'(\theta)(1-0)$, that is, $f(x+v) = f(x) + g'(\theta)$. Note that $g'(\theta) = \nabla f(x + \theta v) \cdot v$.
    \end{proof}
    
    \begin{proposition}[The First Order Taylor Approximation]
        Let $f \in C^1(\R^n, \R)$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + o(\norm{v})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{f(x + v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the mean value theorem, $\exists \theta \in (0, 1)$ such that $f(x+v) - f(x) = \nabla f(x + \theta v) \cdot v$.\\
    	The limit becomes $\lim_{\norm{v} \to 0} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}} = \lim_{\norm{v} \to 0; x + \theta v \to x} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}}$.\\
    	Since $f \in C^1$, $\lim_{x + \theta v \to x} \nabla f(x + \theta v) = \nabla f(x)$.\\
    	And $\frac{v}{\norm{v}}$ is a unit vector, and every component of it is bounded, as the result, the limit of inner product vanishes instead of explodes.
    \end{proof}
    
    \begin{theorem}[The Second Order Mean Value Theorem]
        Let $f: \R^n \to \R$ be a $C^2$ function, then for any $x, v \in \R^n$, there exists $\theta \in (0, 1)$ satisfying
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v^T \nabla^2 f(x + \theta v) v
        \end{equation}
    \end{theorem}
    
    \begin{proposition}[The Second Order Taylor Approximation]
        Let $f: C^2(\R^n, \R)$ function, and $x, v \in \R^n$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v^T \nabla^2 f(x + \theta v) v + o(\red{\norm{v}^2})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{
                f(x + v) - f(x) - \nabla f(x) \cdot v - \frac{1}{2} v^T \nabla^2 f(x) v
            }{\norm{v}^2} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the second mean value theorem, there exists $\theta \in (0, 1)$ such that the limit is equivalent to
    	\begin{align}
    		\lim_{\norm{v} \to 0} \frac{1}{2} \left(\frac{v}{\norm{v}}\right)^T \left[\nabla^2 f(x + \theta v) - \nabla^2 f(x)\right] \frac{v}{\norm{v}}
    	\end{align}
    	Since $f \in C^2$, the limit of $\left[H_f(x + \theta v) - H_f(x)\right]$ is in fact $\textbf{0}_{n \times n}$. And every component of unit vector $\frac{v}{\norm{v}}$ is bounded, the quadratic form converges to zero as an immediate result.
    \end{proof}
    
    It is often noted that the gradient at a particular $x_0 \in dom(f) \subseteq \R^n$ gives the direction $f$ increases most rapidly.
        Let $x_0 \in dom(f)$, and $v$ be a \ul{unit vector} representing a \emph{feasible direction} of change. That is, there exists $\delta > 0$ such that $x_0 + t v \in dom(f)$ $\forall t \in [0, \delta)$. Then the rate of change of $f$ along feasible direction $v$ can be written as
        \begin{equation}
            \left.\frac{d}{dt}\right\vert_{t=0} f(x_0 + tv) = \nabla f(x_0) \cdot v = \norm{\nabla f(x_0)}\ \norm{v} \cos(\theta)
        \end{equation}
        where $\theta = \angle (v, \nabla f(x_0)$. And the derivative is maximized when $\theta=0$, that is, when $v$ and $\nabla f$ point the same direction.
    
    \subsection{Implicit Function Theorem}
    \begin{theorem}[Implicit Function Theorem]
        Let $f: C^1(\R^{n+1}, \R)$, let $(a, b) \in \R^n \times \R$ such that $f(a, b) = 0$. If $\nabla f(a, b) \neq 0$, then $\{(x, y) \in \R^n \times \R:f(x, y) = 0\}$ is locally a graph of a function $g: \R^n \to \R$.
    \end{theorem}
    
    \begin{remark}
        $\nabla f(x_0) \perp \tx{ level set of $f$ near $x_0$}$.
    \end{remark}
    
    \section{Convexity}
    \subsection{Terminologies}
    \begin{definition}
        Set $\Omega \subseteq \R^n$ is \textbf{convex} if and only if 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ \lambda x_1 + (1 - \lambda) x_2 \in \Omega
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subseteq \R^n \to \R$ is \textbf{convex} if and only if $\Omega$ is convex, and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subseteq \R^n \to \R$ is \textbf{strictly convex} if and only if $\Omega$ is convex and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in (0, 1),\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) < \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \subsection{Basic Properties of Convex Functions}
    
    \begin{definition}
        A function $f: \Omega \to \R$ is \textbf{concave} if and only if $-f$ is \textbf{convex}.
    \end{definition}
    
    \begin{proposition} Properties of convex functions:
        \begin{enumerate}[(i)]
            \item If $f_1, f_2$ are convex on $\Omega$, so is $f_1 + f_2$;
            \item If $f$ is convex on $\Omega$, then for any $a > 0$, $af$ is also convex on $\Omega$;
            \item Any \textbf{sub-level/lower contour set} of a convex function $f$ 
            \begin{align}
            	\mc{L}(c) := \{x \in \R^n: f(x) \leq c\}
            \end{align}
            is convex.
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}[Proof of (iii).]
    	Let $c \in \R$, and $x_1 ,x_2 \in SL(c)$. Let $s \in [0, 1]$. Since $x_1, x_2 \in \mc{L}(c)$, and $f(\cdot)$ is convex, $f(s x_1 + (1-s) x_2) \leq s f(x_1) + (1-s) f(x_2) \leq s c + (1-s) c = c$. Which implies $s x_1 + (1-s) x_2 \in \mc{L}(c)$.
    \end{proof}
    
    \begin{example}
    	$\ell_2$ norm $f(x): \R^n \to \R := \norm{x}_2$ is convex.
    \end{example}
    
    \begin{proof}
    	Note that for any $u, v \in \R^n$, by triangle inequality, $\norm{u - (-v)} \leq \norm{u - 0} + \norm{0 - (-v)} = \norm{u} + \norm{v}$. Consequently, let $u, v \in \R^n$ and $s \in [0, 1]$, then $\norm{s u + (1-s) v} \leq \norm{su} + \norm{(1-s) v} = s \norm{u} + (1-s) \norm{v}$. Therefore, $\norm{\cdot}$ is convex.
    \end{proof}
    
    \begin{proposition}
    	Any norm function $\norm{\cdot}$ defined on a vector space $\mc{X}(\R)$ is convex.
    \end{proposition}
    
    \begin{proof}
    	The proof follows the defining properties of norm,
    	\begin{align}
    		\norm{\lambda x + (1 - \lambda) y} &\leq \norm{\lambda x}  + \norm{(1 - \lambda) y} \\
    		&= \lambda \norm{x} + (1 - \lambda) \norm{y}
    	\end{align}
    \end{proof}
	
	\subsection{Characteristics of $C^1$ Convex Functions}
	
    \begin{theorem}[$C^1$ criterions for convexity]
        Let $f \in C^1$, then $f$ is convex on a convex set $\Omega$ \ul{if and only if}
        \begin{equation}
            \forall x, y \in \Omega,\ f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{equation}
        that is, \emph{the linear approximation is an underestimate of $f$}.
    \end{theorem}
    \begin{proof}
        ($\implies$) Suppose $f$ is convex on a convex set $\Omega$. Then $f(sy + (1-s) x) \leq sf(y) + (1-s)f(x)$ for every $x, y \in \Omega$ and $s \in [0, 1]$, which implies, for every $s \in (0, 1]$:
        \begin{equation}
            \frac{f(sy + (1-s) x) - f(x)}{s} \leq f(y) - f(x)
        \end{equation}
        By taking the limit of $s \to 0$,
        \begin{align}
            \lim_{s \to 0} \frac{f(x + s(y-x)) - f(x)}{s} &\leq f(y) - f(x) \\
            \implies \left.\frac{d}{ds}\right\vert_{s=0} f(x + s(y-x)) &\leq f(y) - f(x) \\
            \implies \nabla f(x) \cdot (y-x) &\leq f(y) - f(x)
        \end{align}
        ($\impliedby$) Let $x_0, x_1 \in \Omega$, let $s \in [0, 1]$. Define $x^* := s x_0 + (1-s) x_1$, then 
        \begin{align}
        	f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot (x_0 - x^*) \\
        	\implies f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot [(1-s)(x_0 - x_1)]
        \end{align}
        Similarly,
        \begin{align}
        	f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot (x_1 - x^*) \\
        	\implies f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot [s(x_1 - x_0)]
        \end{align}
        Therefore, $sf(x_0) + (1-s)f(x_1) \geq f(x^*)$.
    \end{proof}
    
    \begin{theorem}[$C^2$ criterion for convexity]
        $f \in C^2$ is a convex function on a convex set $\Omega \subseteq \R^n$ \ul{if and only if} $\nabla^2 f(x) \succcurlyeq 0$ (i.e. positive semidefinite) for all $x \in \Omega$.
    \end{theorem}
    
    \begin{corollary}
    	When $f$ is defined on $\R$, the $C^2$ criterion becomes $f''(x) \geq 0$.
    \end{corollary}
    
    \begin{proof}
        ($\impliedby$) Suppose $\nabla^2 f(x) \succcurlyeq 0$ for every $x \in \Omega$, let $x, y \in \Omega$. By the second order MVT,
        \begin{align}
        	f(y) &= f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y-x)^T \nabla^2 f(x + s (y - x)) (y - x)\tx{ for some } s \in [0, 1] \\
        	&\implies f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{align}
        So $f$ is convex by the $C^1$ criterion of convexity.\\
        ($\implies$) Let $v \in \R^n$. Suppose, for contradiction, that for some $x \in \Omega$, $\nabla^2 f(x) \centernot \succcurlyeq 0$.\\
        If such $x \in \partial \Omega$, note that $v^T \nabla^2 f(\cdot) v$ is continuous because $f \in C^2$, then there exists $\varepsilon > 0$ such that $\forall x' \in V_\varepsilon(x) \cap \Omega^{int},\ v^T \nabla^2 f(x') v < 0$.\\
        Hence, one may assume with loss of generality that such $x \in \Omega^{int}$.\\
        Because $x \in \Omega^{int}$, exists $\varepsilon' > 0$, such that $V_{\varepsilon'}(x) \subseteq \Omega^{int}$.\\
        Define $\hat{v} := \frac{v}{\sqrt{\varepsilon'}}$, then for every $s \in [0, 1]$, $\hat{v}^T \nabla^2 f(x + s\hat{v}) \hat{v} < 0$.\\
        Let $y = x + \hat{v}$, by the mean value theorem, 
        \begin{align}
        	f(y) = f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f [x + s (y - x)] (y - x)
        \end{align}
	 	for some $s \in [0, 1]$.\\
        This implies $f(y) < f(x) + \nabla f(x) \cdot (y - x)$, which contradicts the $C^1$ criterion for convexity.
    \end{proof}
    
    \subsection{Minimum and Maximum of Convex Functions}
    \begin{theorem}
        Let $\Omega \subseteq \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Let
        \begin{align}
        	\Gamma := \left\{x \in \Omega: f(x) = \min_{x \in \Omega} f(x) \right\} \equiv \argmin_{x \in \Omega} f(x)
        \end{align}
        If $\Gamma \neq \varnothing$, then 
        \begin{enumerate}[(i)]
        	\item $\Gamma$ is convex;
        	\item any local minimum of $f$ is the global minimum.
        \end{enumerate}
    \end{theorem}
    
    \begin{proof}[Proof (i).]
    	Let $x, y \in \Gamma$, $s \in [0, 1]$, then $sx+(1-s)y \in \Omega$ because $\Omega$ is convex. Since $f$ is convex, $f(sx+(1-s)y) \leq sf(x) + (1-s)f(y) = \min_{x \in \Omega} f(x)$. The inequality must be equality since it would contradicts the fact that $x, y \in \Gamma$. Therefore, $sx+(1-s)y \in \Gamma$.
    \end{proof}
    
    \begin{proof}[Proof (ii).]
    	Let $x \in \Omega$ be a local minimizer for $f$, but assume, for contradiction, it is not a global minimizer. That is, there exists some other $y$ such that $f(y) < f(x)$. Since $f$ is convex, \begin{align}
 				f(x + t(y-x)) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y) < f(x)
			\end{align}
			for every $t \in (0, 1]$. Therefore, for every $\varepsilon > 0$, there exists $t^* \in (0, 1]$ such that $x + t^*(y-x) \in V_\varepsilon(x)$ and $f(x + t^*(y-x)) < f(x)$, this contradicts the fact that $x$ is a local minimum.
    \end{proof}
    
    \begin{theorem}
        Let $\Omega \subseteq \R^n$ be a \ul{convex and compact} set, and $f: \Omega \to \R$ is a \ul{convex} function. Then 
        \begin{equation}
            \max_{x \in \Omega} f(x) = \max_{x \in \partial \Omega} f(x)
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        As we assumed, $\Omega$ is closed, therefore $\partial \Omega \subseteq \Omega$. Hence, $\max_{x \in \Omega} f \geq \max_{x \in \partial \Omega} f$.\\
        Suppose, for contradiction, $\max_{x \in \Omega} f > \max_{x \in \partial \Omega} f$, then let $x^* \in \argmax_{x \in \Omega} f$. \\
        Then we can construct a straight line through $x^*$ and intersects $\partial \Omega$ at two points, $y_1, y_2 \in \partial \Omega$, such that $x^* = s y_1 + (1-s) y_2$ for some $s \in (0, 1)$. Further, since $f$ is convex, $\max_{x \in \Omega}f(x) = f(x^*) \leq s f(y_1) + (1-s) f(y_2) \leq s \max_{\partial \Omega} f + (1-s) \max_{\partial \Omega} f = \max_{\partial \Omega} f$, which leads to a contradiction.\\
        Therefore, $\max_{x \in \Omega} f = \max_{x \in \partial \Omega} f$.
    \end{proof}
    
    \begin{proposition}
    	For $p, g > 1$ satisfying $\frac{1}{p} + \frac{1}{g} = 1$,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g}|b|^g
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	\begin{align}
    		(-\log) |ab| &= (-\log) |a| + (-\log) |b| \\
    		&= \frac{1}{p} (-\log) |a|^p + \frac{1}{g} (-\log) |b|^p \\
    		(\because (-\log) \tx{ is convex})\ &\geq (-\log)\left( \frac{1}{p} |a|^p + \frac{1}{g} |b|^p \right)
    	\end{align}
    	And since $(-\log)$ is monotonically decreasing,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g} |b|^p
    	\end{align}
    \end{proof}
    
    \begin{corollary}
    	\begin{align}
    		|ab| \leq \frac{|a|^2 + |b|^2}{2}
    	\end{align}
    \end{corollary}
	
	\section{Finite Dimensional Optimization}
	\subsection{Unconstraint Optimization}
    \begin{theorem}[Extreme Value Theorem]
        Let $f: \R^n \to \R$ is \ul{continuous} and $K \subseteq \R^n$ be a \ul{compact} set, then the minimization problem $\min_{x \in K} f(x)$ has a solution.
    \end{theorem}
    \begin{proof}
    	Suppose $f$ is continuous, let $K \subseteq \R^n$ be a compact set, then $f(K)$ is compact as well. Therefore, the $\sup f(K)$ exists (b/c bounded) and is attainable (b/c closed).
    \end{proof}
    
    \begin{remark}
    	$f: \Omega \to \R$ is convex does not imply $f$ is continuous.
    \end{remark}
    
    \begin{proposition}
    	A convex function $f$ defined on a \ul{convex open} set is continuous.
    \end{proposition}
    
    \begin{proof}
    	Let $f: \Omega \to \R$ be a convex function, where $\Omega \subseteq \R^n$ is open.
    	\todo{Is this true?}
    \end{proof}
    
    \begin{proposition}
    	A convex function $f$ defined on an \ul{open interval} in $\R$ is continuous.
    \end{proposition}
    
    \begin{proof}
    	See homework 1. The proof involves squeeze theorem.
    \end{proof}
    
    \begin{proof}[Detailed Proof of EVT.]
    	Let $f: K \to \R$ be a continuous function defined on a compact set $K$.\\
    	WLOG, we only prove the existence of $\min f$, since the existence of max can be easily proven by applying the exact same argument on $-f$. \\
    	That is, we claim the infimum of $f(K)$ is attained within $K$. \\
    	Because $K$ is compact, the continuity of $f$ implies $f(K)$ is compact. \\
    	By the completeness axiom of $\R$, $m := \inf_{x \in K} f(x)$ is well-defined. There exists a sequence $(x_i) \subseteq K$, such that $(f(x_i)) \to m$. Because $K$ is compact, there exists a subsequence $(x_{ik})$ of $(x_i)$ converges to some limit $x^* \in K$. \\
    	Since $f$ is continuous, $(f(x_{ik})) \to f(x^*)$, which is a subsequence of the convergent sequence $(f(x_i))$, and they must converge to the same limit. Hence, $f(x^*) = m$, and the infimum is attained at $x^* \in K$.
    \end{proof}
    
    \begin{theorem}[Heineâ€“Borel]
    	Let $K \subseteq \R^n$, then the following are equivalent:
    	\begin{enumerate}[(i)]
    		\item $K$ is compact (every open cover of $K$ has a finite sub-cover);
    		\item $K$ is closed and bounded;
    		\item Every sequence in $K$ has a convergent subsequence converges to a point in $K$.
    	\end{enumerate}
    \end{theorem}
    
	\begin{proposition}
		Let $\{h_i\}$ and $\{g_i\}$ be sets of continuous functions on $\R^n$, the the set of all points in $\R^n$ that satisfy
		\begin{equation}
			\begin{cases}
				h_i(x) = 0\ \forall i\\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{equation}
		is a closed set.
	\end{proposition}
	
	\begin{proof}
		For every equality constraint $h_i$, it can be represented as the conjunction of two inequality constraint, namely $h_i^\alpha (x) := -h_i(x) \leq 0 \land h_i^\beta (x) := h_i(x) \leq 0$. Then the constraint collection is equivalent to
		\begin{align}
			\begin{cases}
				h_i^\alpha (x) \leq 0\ \forall i \\
				h_i^\beta (x) \leq 0\ \forall i \\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{align}
		The subset of $\R^n$ qualified by each individual constraint is closed by the property of continuous functions (i.e. a continuous function's pre-image of closed set is closed). And the intersection of arbitrarily many closed sets is closed.
	\end{proof}
	
%    \begin{remark}
%    	Computer algorithms for solving minimization problems try to construct a sequence of $(x_i)$ such that $f(x_i)$ decreases to $\min f$ rapidly.
%    \end{remark}
    
%    \begin{corollary}
%        Let $f: \R^n \to \R$ be a continuous function, if there exists $a \in \R^n$ such that $f(x) \geq f(a)$ for every $x \notin \mc{B}(r, a)$, then $f$ attains its minimum in $\mc{B}(r, a)$.
%    \end{corollary}

    \par The optimization problems investigated in this section can be formulated as
    \begin{align} y
    	\min_{x \in \Omega} f(x)
    \end{align}
    where $\Omega \subseteq \R^n$. Typically, for simplicity, $\Omega$ are often $\R^n$, an open subset of $\R^n$, or the closure of some open subset of $\R^n$.
    \par Everything above minimization discussed in this section is applicable to maximization as well using the proposition below.

    \begin{proposition}
        When $\Omega = \R^n$, the unconstrained minimization has the following properties
        \begin{enumerate}[(i)]
            \item $\argmax f = \argmin (-f)$;
            \item $\max f = - \min (-f)$
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}
    	Immediate by applying definitions of maximum and minimum.
    \end{proof}
   	
   	\begin{definition}
   		A function $f: \Omega \to \R$ has \textbf{local minimum} at $x_0 \in \Omega$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega,\  f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strictly local minimum} at $x_0$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega \backslash \{x_0\}\ f(x_0) < f(x)
   		\end{align}
   		$f$ attains \textbf{global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega\ f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strict global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega \backslash \{x_0\} \ f(x_0) < f(x)
   		\end{align}
   		\ul{Note that strict global minimum is always unique by its definition.}
   	\end{definition}
   	
   	\begin{theorem}[Necessary Condition for Local Minimum]
   		Let $f \in C^1(\Omega, \R)$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every \emph{feasible direction} $v$ at $x_0$,
   		\begin{align}
   			\nabla f(x_0) \cdot v \geq 0
   		\end{align}
   		\emph{This theorem serves as the primary defining property of local minimum.}
   	\end{theorem}
   	
   	\begin{definition}
   		For $x_0 \in \Omega \subseteq \R^n$,  $v \in \R^n$ is a \textbf{feasible direction} at $x_0$ if
   		\begin{align}
   			\exists \overline{s} > 0\ s.t.\ \forall s \in [0, \overline{s}], x_0 + s v \in \Omega
   		\end{align}
   	\end{definition}
   	
   	\begin{proof}[Proof of Necessary Condition]
   		Let $x_0 \in \Omega$ be a local minimum, and let $v$ be a feasible direction.
   		Let $g(s) := f(x + sv)$. And since $g$ attains minimum at $s=0$, there exists some $\overline{s} > 0$ such that 
   		\begin{align}
   			g(s) - g(0) \geq 0\ \forall s \in V_{\overline{s}}(0)
   		\end{align}
   		Therefore
   		\begin{align}
   			g'(0) := \lim_{s \to 0^+} \frac{g(s) - g(0)}{s - 0} \geq 0
   		\end{align}
   		The alternative form of derivative can be derived using chain rule as
   		\begin{align}
   			g'(0) = \nabla f(x + sv) \cdot v\ |_{s=0} = \nabla f(x) \cdot v
   		\end{align}
   		By combing the two identities above, $\nabla f(x) \cdot v \geq 0$.
   	\end{proof}
   	
   	\begin{proof}[Alternative Proof of Necessary Condition (not that rigorous)]
   		The prove is almost immediate, if there exists a feasible direction $v^*$ such that $\nabla f(x_0) \cdot v^* < 0$, for every $\varepsilon > 0$, one can construct $x' := x^* + s v^*$ with sufficiently small $s$ so that $x' \in V_\varepsilon(x^*) \cap  \Omega$ and $f(x') < f(x^*)$.
   	\end{proof}
   	
   	\begin{corollary}
   		When $\Omega$ is open, then $x_0$ is a local minimum $\implies \nabla f(x_0) = 0$.
   	\end{corollary}
   	\begin{proof}
   		Since $\Omega$ is open, any sufficiently small $v \neq 0$ such that both $v$ and $-v$ are feasible directions at $x_0$, applying the necessary condition on both $v$ and $-v$ provides the equality.
   	\end{proof}
   	
%   	\begin{example}
%   		Minimize $f(x, y)=x^{2}-x y+y^{2}-3 y$ over $\Omega = \R^2$.
%   	\end{example}
%   	
%   	\begin{example}
%   		Minimize $f(x, y)=x^{2}-x+y+x y$ over $\Omega = \max\{(x, y) \in \R^2: x, y \geq 0\}$.
%   	\end{example}
   	
   	\begin{theorem}[Second Order Necessary Condition for Local Minimum]
   		Let $f: \in C^2(\Omega, \R)$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every non-zero feasible direction $v$ at $x_0$,
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) \cdot v \geq 0$;
   			\item $\nabla f(x_0) \cdot v = 0 \implies v^T \nabla^2 f(x_0) v \geq 0$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum and $v$ be a feasible direction at $\Omega$, and $s \in (0, \overline{s}]$. The first statement is the immediate result of the first order necessary condition. Now suppose $\nabla f(x_0) = 0$, by the Taylor's theorem,
   		\begin{align}
   			0 \leq f(x_0 + sv) - f(x_0) &= s \nabla f(x_0) \cdot v + \frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2) \\
   			&=\frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2)
   		\end{align}
   		Since $s^2 > 0$, divide both sides by $s^2$ and take limit,
   		\begin{align}
   			\lim_{s \to 0} \frac{f(x_0 + sv) - f(x_0)}{s^2} &= \lim_{s \to 0} 
   			\left \{\frac{1}{2} v^T \nabla^2 f(x_0) v + \frac{o(s^2)}{s^2} \right\}\\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + \lim_{s \to 0} \frac{o(s^2)}{s^2} \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v \geq 0
   		\end{align}
   	\end{proof}
   	
	\begin{example}
		$f(x, y) = x^2 - xy + y^2 - 3y: \Omega = \R^2 \to \R$. Then at $(x_0, y_0) = (1, 2)$, 
		\begin{align}
			\nabla f(x_0, y_0) &= (2x_0 - y, -x_0 + 2y_0 - 3) = (0, 0) \\
			\nabla^2 f(x_0, y_0) &= 
			\begin{pmatrix}
				2 & -1 \\ -1 & 2
			\end{pmatrix} \succcurlyeq 0 
		\end{align}
	\end{example}
	
	\begin{definition}
		Let $A \in \R^{n \times n}$, $A$ is 
		\begin{enumerate}[(i)]
			\item \textbf{Positive definite} (denoted as $A \succ 0$) if $x^T A x > 0\ \forall x \neq 0$, if and only if all eigenvalues $\lambda_i > 0$;
			\item \textbf{Positive Semi-definite} (denoted as $A \succcurlyeq 0$) if $x^T A x \geq\  \forall x \in \R^n$, if and only if all eigenvalues $\lambda_i \geq 0$.
		\end{enumerate}
	\end{definition}
   	
   	\begin{theorem}[Sylvester's Criterion]
   		Let $A \in \R^{n \times n}$ be a Hermitian matrix (i.e. $A = \overline{A^T}$)\footnote{$\overline{A^T}$ denotes the complex conjugate of the transpose, a matrix with \emph{real entries} is Hermitian if and only if it is symmetric.}, then
   		\begin{enumerate}
   			\item $A \succ 0$ $\iff$ all \emph{leading principal minors} have positive determinants;
   			\item $A \succcurlyeq 0$ $\iff$ all leading principal minors have non-negative determinants.
   		\end{enumerate}
   	\end{theorem}
   	
%   	\begin{example}
%   		Let $f(x, y) = x^2 - x + y + xy$ defined on $\Omega = \R^2_+$. Consider the  point $(x_0, y_0) = (1/2, 0)$ found, any feasible direction $(v, w)$ at $(x_0, y_0)$ satisfies $w \geq 0$. $\nabla f(1/2, 0) = (0 , 2/3)$.
%   		\begin{align}
%   			\nabla f(1/2, 0) &= (0, \frac{3}{2}).
%   		\end{align}
%   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Condition for Interior Local Minima]
   		Let $f: C^2(\Omega, \R)$, for some $x_0 \in \Omega$, if
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) = 0$,
   			\item \emph{(and)} $\nabla^2 f(x_0) \succ 0$.
   		\end{enumerate}
   		then $x_0$ is a \ul{strictly local minimizer}.
   	\end{theorem}
   	
   	\begin{lemma}
   		Suppose $\nabla^2 f(x_0)$ is positive definite, then 
   		\begin{align}
   			\exists\ a > 0\ s.t.\ v^T \nabla^2 f(x_0) v \geq a \norm{v}^2\quad \forall v
   		\end{align}
   		\emph{That is, the quadratic form of a positive definite matrix is bounded away from zero.}
   	\end{lemma}
   	
   	\begin{proof}[Proof of the Lemma]
   		Recall that a squared matrix $Q$ is called \textbf{orthogonal} when every column and row of it is an orthogonal unit vector. So that for every orthogonal matrix $Q$, $Q^T Q = I$, which implies $Q^T = Q^{-1}$. Further, note that 
   		\begin{align}
   			\norm{Qv}^2 &= (Qv)^T (Qv)
   			= v^T Q^T Q v 
   			= \norm{v}^2 \\
   			\implies \norm{Qv} &= \norm{v}\ \forall v \in \R^n
   		\end{align}
   		Let $v \in \R^n$, consider the eigenvector decomposition of $\nabla^2 f(x_0)$, let $w$ satisfy $v = Qw$:
   		\begin{align}
   			Q^T \nabla^2 f(x_0) Q &= \tx{diag}(\lambda_1, \cdots, \lambda_n) \\
   			\implies v^T \nabla^2 f(x_0) v &= (Qw)^T \nabla^2 f(x_0) (Qw) \\
   			&= w^T Q^T \nabla^2 f(x_0) Q w \\
   			&= w^T \tx{diag}(\lambda_1, \cdots, \lambda_n) w \\
   			&= \lambda_1 w_1^2 + \cdots + \lambda_n w_n^2
   		\end{align}
   		Let $a := \min\{\lambda_1, \cdots, \lambda_n\}$,
   		\begin{align}
   			... \geq a \norm{w}^2 = a\norm{Q^T v}^2 = a \norm{v}^2
   		\end{align}
   	\end{proof}
   	
   	\begin{proof}[Proof of the Theorem.]
   		Let $x \in \Omega$, suppose $\nabla f(x_0) = 0$ and $\nabla^2 f(x_0) \succcurlyeq 0$. By the second order Taylor approximation,
   		\begin{align}
   			f(x_0 + v) - f(x_0) &= \nabla f(x_0)^T v + \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&\geq \frac{a}{2} \norm{v}^2 + o(\norm{v}^2) \tx{ for some }a > 0 \\
   			&= \norm{v}^2 \left (
   			\frac{a}{2} + \frac{o(\norm{v}^2)}{\norm{v}} \right) \\
   			&>0\ \tx{ for sufficiently small } v
   		\end{align}
   		Therefore, $f(x_0) < f(x)\ \forall x \in V_\varepsilon(x_0)$.
   	\end{proof}
   	
   	\subsection{Equality Constraints: Lagrangian Multiplier}
   	\subsubsection{Tangent Space to a (Hyper) Surface at a Point}
   	\begin{definition}
   		A \textbf{surface} $\mc{M} \subseteq \R^n$ is defined as
   		\begin{align}
   			\mc{M} := \left \{
   			x \in \R^n : h_i (x) = 0\ \forall i
   			\right \}
   		\end{align}
   		where $h_i$ are all $C^1$ functions.
   	\end{definition}
   	
   	\begin{definition}
   		A \textbf{differentiable curve} on a surface $\mc{M}$ is a $C^1$ function mapping from $(-\varepsilon, \varepsilon)$ to $\mc{M}$. \\
   		\emph{Remark: in previous calculus courses, differentiable curves are often referred to as parameterizations.}
   	\end{definition}
   	Let $x(s)$ be a differentiable curve on $\mc{M}$ passes through $x_0 \in \mc{M}$, re-parameterize it so that $x(0) = x_0$. Then vector
   	\begin{align}
   		v := \frac{d}{ds} \bigg \vert_{s=0} x(s)
   	\end{align}
   	touches $\mc{M}$ \emph{tangentially}.
   	
   	\begin{definition}
   		Any vector $v$ generated by some differentiable curve on $\mc{M}$ and takes above form is a $\textbf{tangent vector}$ on $\mc{M}$ through $x_0$.
   	\end{definition}
   	
   	\begin{definition}
   		The \textbf{tangent space} to $\mc{M}$ at $x_0$ is defined to be the set of all tangent vectors:
   		\begin{align}
   			T_{x_0} \mc{M} := \left \{
   			v \in \R^n
   			:
   			v := \frac{d}{ds} \bigg \vert_{s=0} x(s) \tx{ for some } x \in C^1(V_\varepsilon(0), \mc{M})\ s.t.\ x(0) = x_0
   			\right \}
   		\end{align}
   	\end{definition}
   	
   	\begin{example}
   		Define 
   		\begin{align}
   			\mc{M} := \left\{x \in \R^2: \norm{x}_2 = 1 \right\}
   		\end{align}
   		By defining $C^1$ functions $g(x) := \norm{x}^2_2  - 1$, $\mc{M}$ is a surface. The tangent space of $\mc{M}$ at $x_0$ is
   		\begin{align}
   			T_{x_0} \mc{M} = \left \{
   			v \in \R^n:
   			\inner{v}{x_0} = 0
   			\right \}
   		\end{align}
   	\end{example}
   	
   	\begin{definition}
   		Let $\mc{M}$ be a surface defined using $C^1$ functions, a point $x_0 \in \mc{M}$ is a \ul{\textbf{regular point} of the constraints} if 
   		\begin{align}
   			\{\nabla h_1(x_0), \cdots, \nabla h_k(x_0)\}
   		\end{align}
   		are linearly independent. \\
   		\emph{Remark: if there is only one constraint $h$, then $x_0$ is regular if and only if $\nabla h(x_0) \neq 0$.}
   	\end{definition}
   	
   	\begin{notation}
   		Define the $T$ space on equality constraint as
   		\begin{align}
   			T_{x_0} := \{x \in \R^n: \inner{x_0}{\nabla h_i(x_0)} = 0\ \forall i \in [k]\} = \{\nabla_i(x_0)\}^\perp
   		\end{align}
   	\end{notation}
   	
   	\begin{example}[Counter example]
   		Define
   		\begin{align}
   			\mc{M} := \left \{
   			(x, y) \in \R^2:
   			h(x,y) = xy = 0
   			\right \}
   		\end{align}
   		Then it is easy to verify that $(0,0)$ is not a regular point. And
   		\begin{align}
   			T_{0,0} &= \{(x, y) \in \R^2: (x, y) \cdot (0,0) = 0\} = \R^2 \\
   			\neq T_{0,0}\mc{M} &= \{(x, y) \in \R^2: x = 0 \lor y = 0\}
   		\end{align}
   	\end{example}
   	
   	\begin{theorem}
   		Suppose $x_0$ is a \emph{regular point} of $\mc{M} := \{h_i(x) = 0, i=1,\cdots,k\}$, then $T_{x_0} = T_{x_0} \mc{M}$.
   	\end{theorem}
   	
   	\begin{proof}
   		\emph{Show} $T_{x_0} \mc{M} \subseteq T_{x_0}$.\\
   		Suppose $x_0$ is a regular point of $\mc{M}$.
   		Let $v \in T_{x_0} \mc{M}$, then there exists some differentiable curve $x(\cdot):V_\varepsilon(0) \to \mc{M}$ such that $x(0) = x_0$, such that
   		\begin{align}
   			v &= \frac{d}{ds} \bigg \vert_{s=0} x(s)
   		\end{align}
   		Note that $h_i(x(s)) = 0$ is constant for every $i \in [k]$, therefore
   		\begin{align}
   			\frac{d}{ds} \bigg \vert_{s=0} h_i(x(s))
   		\end{align}
   		By the chain rule, 
   		\begin{align}
   			\nabla h_i(x_0)\cdot v = 0\ \forall i
   		\end{align}
   		Therefore $v \in T_{x_0}$. \\
   		\emph{Show} $T_{x_0} \subseteq T_{x_0} \mc{M}$.
   		\begin{enumerate}[(i)]
   			\item $x_0$ is regular $\implies T_{x_0} \mc{M}$ is a vector space;
   			\item $T_{x_0} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$.
   		\end{enumerate}
   		\emph{Show} $T_{x_0} \subseteq \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$: \\
   		Let $v \in T_{x_0}$, then $v \perp \nabla h_i(x_0)$ for every $i$. Therefore $v$ is orthogonal to every linear combination of $\nabla h_i(x_0)$, and therefore orthogonal to the span.\\
   		\emph{Show} $\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp \subseteq T_{x_0}$: \\
   		Let $v$ in the perp of the span, then $v$ is orthogonal to every basis of the span, so $v \in T_{x_0}$.
   	\end{proof}
   	
   	\begin{lemma}
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on \ul{open} subset $\Omega \subseteq \R^n$. Define $\mc{M} := \{x \in \R^n: h_i(x) = 0\ \forall i \}$. Suppose $x_0 \in \mc{M}$ is a local minimum of $f$ on $\mc{M}$, then 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   	\end{lemma}
   	
   	\begin{proof}
   		WLOG $\Omega = \R^n$, take $v \in T_{x_0} \mc{M}$. Then there exists some differentiable curve $x$ on $\mc{M}$ satisfying $v = x'(0)$. Because $x_0$ is a local minimum of $f$ on $\Omega$, $s=0$ is a local minimum of $f(x(s))$, moreover, it is an interior minimum. By chain rule and the necessary condition of local minimum,
   		\begin{align}
   			D f(x(0)) &= \nabla f (x(0)) \cdot x'(0) = 0 \\
   			\implies \nabla f(x_0) \cdot v &= 0
   		\end{align}
   		Therefore $\nabla f(x_0) \perp T_{x_0} \mc{M}$.
   	\end{proof}
   	
   	\begin{theorem}[Lagrange Multipliers: First Order Necessary Condition]
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on open subset $\Omega \subseteq \R^n$. Let $x_0$ be a regular point of the constraint set $\mc{M} := \bigcap_{i=1}^k h^{-1}_i(0)$. Suppose $x_0$ is a local minimum of $\mc{M}$, then there exists $\lambda_1, \cdots, \lambda_k \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0
   		\end{align}
   		\emph{Remark: if we define Lagrangian $\mc{L}(x, \lambda_i) := f(x) + \sum_{i=1}^k h_i(x)$, then the theorem says the local minimum is a critical point of $\mc{L}$.}
   	\end{theorem}
   	
   	\begin{proof}
   		Because $x_0$ is a regular point, then by previous lemma, $\nabla f(x_0) \perp T_{x_0} \mc{M}$. Moreover,
   		\begin{align}
   			T_{x_0} \mc{M} = T_{x_0} = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^\perp
   		\end{align}
   		Also, because $x_0$ is a local minimum, 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   		Therefore, $\nabla f(x_0) \in (T_{x_0} \mc{M})^\perp = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^{\perp \perp} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}$, where the last equality holds in finite dimensional cases. Hence, it is obvious that we can write $\nabla f(x_0)$ as a linear combination of $\{\nabla h_i(x_0)\}$.
   	\end{proof}
   	
   	\begin{theorem}[Second Order Necessary Condition]
   		Let $f, h_i \in C^2$, if $x_0$ is a local minimum on previously defined surface $\mc{M}$, then there exists Lagrangian multipliers $\{\lambda_i\}$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0$ ($\nabla_x \mc{L} = 0$);
   			\item And $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) \succcurlyeq 0$ \red{on $T_{x_0} \mc{M}$} ($\nabla_x^2 \mc{L} \succcurlyeq 0$).
   		\end{enumerate}
   		\emph{Remark: whenever $x_0$ is a local minimum, it must be a critical point of $\mc{L}$, and $\mc{L}$ is positive semidefinite on the tangent space at $x_0$.}
   	\end{theorem}
   	
   	\begin{proof}
   		The first result is exactly the same as the first order condition proven above. \\
   		To show the second result, let $x(s) \in \mc{M}$ be an arbitrary differentiable curve on $\mc{M}$ such that $x(0) = x_0$. Then,
   		\begin{align}
   			\frac{d}{ds} f(x(s)) &= \nabla f(x(s)) \cdot x'(s) \\
   			\frac{d^2}{ds^2} f(x(s)) &= x'(s)^T \nabla^2 f(x(s)) x'(s) + \nabla f(x(s)) x''(s)
   		\end{align}
   		By the second order Taylor theorem, for every $s$ such that $x(s) \in \mc{M}$, 
   		\begin{align}
   			f(x(s)) - f(x_0) = s \nabla f(x_0) \cdot x'(0) + \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Note that by definition, $x'(0)$ is in the tangent space at $x_0$. Also, we've shown previously that $\nabla f(x_0)$ is orthogonal to the tangent space at $x_0$, therefore,
   		\begin{align}
   			f(x(s)) - f(x_0) = \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Also, by the definition of $\mc{M}$, all constraints hold with equality:
   		\begin{align}
   			f(x_0) &= f(x_0) + \sum_{i=1}^k \lambda_i h_i(x_0)
   		\end{align}
   		where $\lambda_i$'s are from the first result. Hence,
   		\begin{align}
   			f(x(s)) - f(x_0) &= \frac{s^2}{2} \left [
   			x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0)
   			+ \left(
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0)
   			\right)x''(0)
   			\right ] + o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2)
   		\end{align}
   		And above expression is greater or equal to zero because $x_0$ is a local minimum,
   		\begin{align}
   			\frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2) &\geq 0 \\
   			\implies x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + \frac{o(s^2)}{s^2} &\geq 0 \\
   			\overset{s\to 0}{\implies}x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) &\geq 0
   		\end{align}
   		Where $x'(0)$ is a vector in the tangent space at $x_0$ by definition. Moreover, the curve $x(s)$ was chosen arbitrarily, so the argument works for every curve and therefore every tangent vector, and what's desired is shown.
   	\end{proof}
   	
   	\begin{example}
   		\begin{align}
   			\min f(x, y) &= x^2 - y^2 \\
   			s.t.\ h(x, y) &= y = 0
   		\end{align}
   		First order condition suggests $(x_0, y_0) = (0, 0)$ Note that the tangent space at $(x_0, y_0)$ is $\tx{span}\{\nabla h_i\}^\perp$:
   		\begin{align}
   			T_{x_0}\mc{M} = \{ (u, 0): u \in \R\}
   		\end{align}
   		and 
   		\begin{align}
   			\nabla_x^2 \mc{L} = \begin{pmatrix}
   				2 & 0 \\ 0 & -2
   			\end{pmatrix}
   		\end{align}
   		is obviously positive semidefinite (actually positive definition) on the tangent space.
   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Conditions]
   		Let $f, h_i \in C^2$ on open $\Omega \subseteq \R^n$, and $x_0 \in \mc{M}$ is a regular point, if there exists $\lambda_i \in \R$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla_x \mc{L}(x_0, \lambda_i) = 0$;
   			\item $\nabla_x^2 \mc{L}(x_0, \lambda_i) \succ 0$ \red{on $T_{x_0} \mc{M}$},
   		\end{enumerate}
   		then $x_0$ is a \emph{strict} local minimum.
   	\end{theorem}
   	\begin{proof}
   		Recall that $\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)$ positive definite on $T_{x_0} \mc{M}$ implies there exists $a > 0$ ($a$ is taken to be equal to the least eigenvalue of $\nabla^2_x \mc{L}$) such that 
   		\begin{align}
   			v^T [\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)] v \geq a \norm{v}^2\quad \forall v \in T_{x_0} \mc{M}
   		\end{align}
   		Let $x(s) \in \mc{M}$ be a curve such that $x(0) = x_0$ and $v = x'(0)$. WLOG, $\norm{x'(0)} = 1$. By the second order Taylor expansion,
   		\begin{align}
   			f(x(s)) - f(x(0)) &= s \left. \frac{d}{ds} \right\vert_{s=0} f(x(s)) + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} f(x(s)) + o(s^2) \\
   			&= s \left. \frac{d}{ds} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + o(s^2) \\
   			&= s \nabla_x \mc{L}(x_0, \lambda_i) \cdot x'(0)
   			+ \frac{s^2}{2}
   			\left[x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + \nabla_x \mc{L}(x_0, \lambda_i) x''(0) \right]
   			+ o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + o(s^2) \\
   			&\geq \frac{s^2}{2} a \norm{x'(0)}^2 + o(s^2)\quad \tx{where } a > 0 \\
   			&= s^2 \left(\frac{a}{2} + \frac{o(s^2)}{s^2}\right) \\
   			&\overset{s \to 0}{>} 0
   		\end{align}
   		Therefore, for sufficiently small $s$, $f(x(s)) - f(x(0)) > 0$. And this is true for every curve $x$ on $\mc{M}$. So $x(0)$ is a strict local minimum.
   	\end{proof}
   	
   	\subsection{Remark on the Connection Between Constrained and Unconstrained Optimizations}
   	\begin{example}
   		Consider
   		\begin{align}
   			&\min f(x,y,z) \\
   			&s.t. g(x,y,z) = z - h(x,y) = 0
   		\end{align}
   		where $\mc{M}$ is the graph of $h$. Using Lagrangian multiplier provides necessary condition: $\nabla f + \lambda \nabla g = 0$,
   		\begin{align}
   			\begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix}
   			+ \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = 0
   		\end{align}
   		Convert the constrained optimization into an unconstrained optimization as 
   		\begin{align}
   			\min_{(x,y) \in \R^2} F(x, y) = f(x, y, h(x, y))
   		\end{align}
   		The necessary condition for unconstrained optimization is
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x + f_z h_x \\
   				f_y + f_z h_y
   			\end{pmatrix} \\
   			&= \begin{pmatrix}
   				f_x \\ f_y
   			\end{pmatrix} - f_z \begin{pmatrix}
   				-h_x \\ -h_y
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   		Define $\lambda := -f_z$.
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix} + \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   	\end{example}
   	\subsection{Inequality Constraints}
   	\begin{definition}
   		Let $x_0$ satisfy the set of constraints
   		\begin{align}
   			(\dag)\begin{cases}
	   			h_i(x) &= 0\quad i \in \{1, \cdots, k\} \\
   				g_j(x) &\leq 0\quad j \in \{1, \cdots, \ell\}
   			\end{cases}
   		\end{align}
   		we say that the constraint $g_i$ is \textbf{active} at $x_0$ if $g_i(x_0) = 0$, and is \textbf{inactive} at $x_0$ if $g_i(x_0) < 0$.
   	\end{definition}
   	
   	\begin{definition}
   		Split the collection of inequality constraints into active and inactive constraints, let $\Theta(x_0)$ denote the collection of active indices, that's:
   		\begin{align}
   			g_j (x_0) = 0\ \forall j \in \Theta(x_0) \\
   			g_j (x_0) < 0\ \forall j \notin \Theta(x_0)
   		\end{align}
   		Then $x_0$ is said to be a \textbf{regular point} of the constraint if 
   		\begin{align}
   			\{\nabla h_i(x_0)\ \forall i \in \{1, \cdots, k\}; \underbrace{\nabla g_j(x_0)\ \forall j \in \Theta(x_0)}_{\tx{Active Constraints}} \}
   		\end{align}
   		is linearly independent. 
   	\end{definition}
   	
   	\begin{theorem}[The First Order Necessary Condition for Local Minimum: Kuhn-Tucker Conditions]
   		Let $\Omega$ be an open subset of $\R^n$ with constraints $h_i$ and $g_i$ to be $C^1$ on $\Omega$. Suppose $x_0 \in \Omega$ is a regular point with respect to constraints, further suppose $x_0$ is a local minimum, then there exists some $\lambda_i \in \R$ and $\mu_j \in \red{\R_+}$ such that 
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$ (i.e. $\nabla_x \mc{L}(x, \lambda, \mu) = 0$);
   			\item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
   		\end{enumerate}
   		\emph{Remark 1: by complementary slackness, all $\mu_j$ corresponding to inactive inequality constraints are zero.} \\
   		\emph{Remark 2: it is possible for an active constraint to have zero multiplier.}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum for $f$ satisfying constraints, equivalently, it is a local minimum for equality constraints and active inequality constraints. \\
   		By the first order necessary condition for local minimum with equality constraints, there exists $\lambda_i, \mu_j \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j \in \Theta(x_0)} \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		Then by setting $\mu_j = 0$ for all $j \notin \Theta(x_0)$ one have
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		By construction, the complementary slackness is satisfied. At this stage, we have construct $\lambda_i \in \R$ and $\mu_j \in \R$ satisfying both conditions, we still need to argue that $\mu_j \geq 0$ for every $j$.
   	\end{proof}
   	
	\begin{theorem}[The Second Order Necessary Conditions]
		Let $\Omega$ be an open subset of $\R^n$, and $f, h_1, \cdots, h_k, g_1, \cdots, g_\ell \in C^2(\R^n, \R)$. Let $x_0$ be a regular point of the constraints ($\dag$).
%		\begin{align}
%			(\dag ) \begin{cases}
%				h_i(x) = 0\quad \forall i \\
%				g_j(x) \leq 0\quad \forall j
%			\end{cases}
%		\end{align}
		Suppose $x_0$ is a local minimum of $f$ subject to constraint ($\dag$), then there exists $\lambda_i \in \R$ and $\mu_j \geq 0$ such that
		\begin{enumerate}[(i)]
			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
			\item $\mu_j g_j(x_0) = 0$;
			\item $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla^2 g_j(x_0)$ is \ul{positive semidefinite} on the tangent space to \ul{activate constraints} at $x_0$.
		\end{enumerate}
	\end{theorem}
	\begin{proof}
		(i) and (ii) are immediate result from the first order necessary condition. \\
		Suppose $x_0$ is a local minimum for ($\dag$), then $x_0$ is a local minimum for active constraints at $x_0$. \\
		Therefore, $\nabla^2 \hat{\mc{L}} = \nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) + \sum_{j \in I(x_0)} \mu_j \nabla^2 g_j(x_0)$ is positive semidefinite on the tangent space to active constraints. Note that because $\mu_j = 0$ for inactive constraints, therefore $\nabla^2 \hat{\mc{L}} = \nabla^2 \mc{L}$ at $x_0$, and both of them are positive semidefinite on the tangent space corresponding to active constraints.
	\end{proof}
	
	\begin{theorem}[The Second Order Sufficient Conditions]
		Let $\Omega$ be an open subset of $\R^n$, let $f, h_i, q_j \in C^2(\Omega)$. Consider minimizing $f(x)$ with the constraint
		\begin{align}
			(\dag ) \begin{cases}
				h_i(x) = 0\quad \forall i \\
				g_j(x) \leq 0\quad \forall j \\
				x \in \Omega
			\end{cases}
		\end{align}
		Suppose there exists a feasible $x_0$ satisfying $(\dag)$ and $\lambda_i \in \R$ and $\mu_j \in \R_{+}$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
   			\item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
   		\end{enumerate}
   		If the Hessian matrix for Lagrangian $\nabla_x^2 \mc{L}(x_0)$ is \ul{positive definite} on $\ttilde{T}_{x_0}$, the space of \textbf{strongly active} constraints at $x_0$, then $x_0$ is a \ul{strict} local minimum.
	\end{theorem}
	
	\begin{definition}
		A constraint $g_j$ is \textbf{strongly active} at $x_0$ if $g_j(x_0) = 0$ (so it is active) and $\mu_j > 0$. 
	\end{definition}
	
	\begin{notation}
		For convenience, we can rearrange the collection of constraints such that, among the $\ell$ constrains in total, the first $\ell'$ constraints are \emph{active} at $x_0$ and the first $\ell''$ constraints are \emph{strongly active}. Note that $\ell'' \leq \ell ' \leq \ell$. \\
		Define 
		\begin{align}
			\tilde{T}_{x_0} &:= \{
			v \cdot \nabla h_i(x_0) = 0\ \forall i \land v \cdot \nabla g_j(x_0)\tx{ for all $g_j$ active.}
			\} \\
			\ttilde{T}_{x_0} &:= \{
			v \cdot \nabla h_i(x_0) = 0\ \forall i \land v \cdot \nabla g_j(x_0) \tx{ for all $g_j$ strongly active.}
			\}
		\end{align}
		Clearly, $\tilde{T}_{x_0} \subseteq \ttilde{T}_{x_0}$ because there are (weakly) more active constraints than strongly active constraints.
	\end{notation}
	
	\begin{proof}[Proof of the Sufficient Condition]
		Suppose, for contradiction, $x_0$ is not a strict local minimum. \\
		\textbf{Claim 1:} There exists unit vector $v \in \R^n$ such that 
		\begin{enumerate}[(a)]
			\item $\nabla f(x_0) \cdot v \leq 0$;
			\item $\nabla h_i(x_0) \cdot v = 0$ for every $i$;
			\item $\nabla g_j(x_0) \cdot v \leq 0$ for all $j \leq \ell'$ (active constraints).
		\end{enumerate}
		\begin{proof}[Proof of Claim 1]
			Because $x_0$ is not a strictly local minimum, one can construct a sequence of feasible points $(x_k) \to x_0$ by setting $\varepsilon = \frac{1}{k}$ for every $k \in \N$ such that $f(x_k) \leq f(x_0)$. \\
			Let $v_k := \frac{x_k - x_0}{\norm{x_k - x_0}}$, $s_k := \norm{x_k - x_0}$. Note that every $v_k$ is in unit sphere, which is compact. Therefore, there exists a subsequence of $(v_k)$ converges to some unit vector $v$.
			\begin{align}
				0 \geq f(x_k) - f(x_0) = f(x_0 + s_k v_k) - f(x_0)\ \forall k \in \N
			\end{align}
			The first order Taylor series suggests the following holds for every $k \in \N$:
			\begin{align}
				0 &\geq f(x_0 + s_k v_k) - f(x_0) \\
				&= s_k \nabla f(x_0) \cdot v_k + o(s_k) \\
				0 &= h_i(x_0 + s_k v_k) - h_i(x_0) = s_k \nabla h_i(x_0) \cdot v_k + o (s_k) \\
				0 &\geq g_j(x_0 + s_k v_k) - g_j(x_0) = s_k \nabla g_j(x_0) \cdot v_k + o(s_k)\quad \forall j \leq \ell'
			\end{align}
			Above inequalities are preserved by limit operation, therefore,
			\begin{align}
				\nabla f(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla f(x_0) \cdot v &\leq 0 \\
				\nabla h_i(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla h_i(x_0) \cdot v &= 0 \\
				\nabla g_j(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla g_j(x_0) \cdot v &\leq 0\quad \forall j \leq \ell'
			\end{align}
		\end{proof}
		\textbf{Claim 2:} $\nabla g_j(x_0) \cdot v = 0$ for $j = 1, \cdots, \ell''$.
		\begin{proof}[Proof of Claim 2]
			Suppose not, there exists $j \in \{1, \cdots, \ell''\}$ such that $\nabla g_j(x_0) \cdot v < 0$. Then by (i),
			\begin{align}
				0 \geq \nabla f(x_0) \cdot v &= - \sum_{i=1}^k \lambda_i \nabla h_i(x_0) \cdot v - \sum_{j=1}^{\ell} \mu_j \nabla g_j(x_0) \cdot v \\
				&= - \sum_{j=1}^{\ell} \mu_j \nabla g_j(x_0) \cdot v > 0
			\end{align}
			the last inequality is from the fact that $\mu_j \nabla g_j (x_0) \cdot v \leq 0$ for all active constraints and $\mu_j = 0$ for all inactive constraints. \\
		\end{proof}
		(b) and claim 2 suggests $v \in \ttilde{T}_{x_0}$. \\
		By the second order Taylor approximation,
		\begin{align}
			0 \geq f(x_k) - f(x_0) &= s_k \nabla f(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 f(x_0) \cdot v_k + o(s_k^2) \\
			0 = h_i(x_k) - h_i(x_0) &= s_k \nabla h_i(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 h_i(x_0) \cdot v_k + o(s_k^2)\quad \forall i\\
			0 \geq g(x_k) - g(x_0) &= s_k \nabla g_j(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 g_j(x_0) \cdot v_k + o(s_k^2)\quad \forall j \leq \ell'
		\end{align}
		Multiply the second equation by $\lambda_i$ and third equation by $\mu_j$, and use the fact that $\mu_j = 0$ for every $j > \ell'$. Also, given $\nabla \mc{L} = 0$ in (i): 
		\begin{align}
			0 &\geq \frac{s_k^2}{2} v_k \cdot \nabla^2 \mc{L} \cdot v_k + o(s_k^2)
		\end{align}
		Divide by $s_k^2$ and take the limit $(v_k) \to v$:
		\begin{align}
			v\cdot \nabla^2 \mc{L}\cdot v \leq 0
		\end{align}
		which contradicts the assumption that $\nabla^2 \mc{L}$ is positive definite in $\ttilde{T}_{x_0}$ because we've shown that $v \in \ttilde{T}_{x_0}$.
	\end{proof}
	\section{Iterative Algorithms for Optimization}
	\subsection{Newton's Method}
	\begin{example}[Motivation: a second order iterative algorithm]
		Let $f: I \subseteq \R \to \R$ where $I$ is an open interval. Let $x_i \in I$ be a starting point, consider the second order linear approximation of $f$ at $x_0$:
		\begin{align}
			g(x) &= f(x_0) + f'(x_0) (x - x_0) + \frac{1}{2} f''(x_0) (x - x_0)^2
		\end{align}
		By construction, the second order Taylor polynomial, $g(x)$, is the best second order approximation to $f$ at $x_0$ in the following sense:
		\begin{align}
			g(x_0) &= f(x_0) \\
			g'(x_0) &= f'(x_0) \\
			g''(x_0) &= f''(x_0)
		\end{align}
		The Newton's method aims to solve the critical point of $g(x)$ and define $x_1$ to be the critical point found:
		\begin{align}
			g'(x_1) &= f'(x_0) + f''(x_0) (x_1 - x_0) = 0 \\
			\implies x_1 &\leftarrow x_0 - \frac{f'(x_0)}{f''(x_0)}
		\end{align}
	\end{example}
	
	\begin{algorithm}[Newton's Method in $\R$]
		Given initial point $x_0 \in I$, while not terminated:
		\begin{align}
			x_{n+1} \leftarrow x_n - \frac{f'(x_n)}{f''(x_n)}
		\end{align}
	\end{algorithm}
	
	\begin{theorem}
		Let $f \in C^3$ on open interval $I \subseteq \R$. Suppose $x_* \in I$ satisfies $f'(x_*) = 0$ and $f''(x_*) \neq 0$, then the sequence of points $(x_n)$ generated by Newton's method converges to $x_*$ if \emph{$x_0$ is sufficiently close to $x_*$}.
	\end{theorem}
	
	\begin{example}
		Let $f(x) = x^2$, then $\frac{f'(x)}{f''(x)} = \frac{2x}{2}$. For any starting point $x_0$, $x_1 = x_0 - \frac{2x_0}{x_0} = 0$. That is, the algorithm converges to the global minimum in one iteration.
	\end{example}
	
	\begin{proof}
		Let $g(x) = f'(x)$ so that $x_{n+1} = x_n - \frac{g(x_n)}{g'(x_n)}$.\\
		Because $f \in C^3$, then $g \in C^2$. \\
		Note that by $g \in C^2$, $g' = f''$ is bounded away from zero near $x_*$. \\
		And by continuity again, $g'' = f^{(3)}$ is bounded near the bounded region $V_\varepsilon(x_*)$. \\
		That is, within small region near $x_*$, $V_\delta(x_*)$, there exists a sufficiently small $\alpha >0$ such that
		\begin{align}
			\begin{cases}
				\abs{g'(x_1)} > \alpha\ \forall x_1 \in V_\delta(x_*) \\
				\abs{g''(x_2)} < \frac{1}{\alpha}\ \forall x_2 \in V_\delta(x_*)
			\end{cases}
		\end{align}
		Further, note that $g(x_*) = f'(x_*) = 0$. \\
		WLOG, let $n \in \N$, suppose $x_n > x_*$:
		\begin{align}
			x_{n+1} - x_* &= x_n - \frac{g(x_n)}{g'(x_n)} - x_* \\
			&= x_n - x_* - \frac{g(x_n) - g(x_*)}{g'(x_n)} \\
			&= - \frac{g(x_n) - g(x_*) - g'(x_n)(x_n - x_*)}{g'(x_n)} \\
			&= - \frac{1}{2} \frac{g''(\xi)}{g'(x_n)} (x_n - x_*)^2\quad \tx{for some } \xi \in (x_*, x_n)
		\end{align}
		By taking the absolute values on both sides:
		\begin{align}
			\abs{x_{n+1} - x_*} &= \frac{1}{2} \frac{\abs{g''(\xi)}}{\abs{g'(x_n)}} \abs{x_n - x_*}^2 \\
			&< \frac{1}{2 \alpha^2} |x_n - x_*|^2
		\end{align}
		Let $\rho := \frac{1}{\alpha^2} \abs{x_0 - x_*}^2$, choose $x_0$ sufficiently close to $x_*$ such that $\rho < 1$.\\
		\emph{Remark: we are showing the iterative algorithm induces a contraction map.} \\
		Then,
		\begin{align}
			\abs{x_1 - x_*} &< \frac{1}{2\alpha^2}\abs{x_0 - x_*}^2 \\
			&= \frac{1}{2\alpha^2}\abs{x_0 - x_*} \abs{x_0 - x_*} \\
			&= \rho \abs{x_0 - x_*}
		\end{align}
		Inductively,
		\begin{align}
			\abs{x_2 - x_*} &< \frac{1}{2 \alpha^2} \abs{x_1 - x_*}^2 \\
			&< \frac{1}{2 \alpha^2} \rho^2 \abs{x_0 - x_*}^2 \\
			&= \rho^3 \abs{x_0 - x_*} \\
			&< \rho^2 \abs{x_0 - x_*}
		\end{align}
		By induction,
		\begin{align}
			\abs{x_n - x_*} < \rho^2 \abs{x_0 - x_*}
		\end{align}
		Therefore, as $n \to \infty$, $(x_n) \to x_*$.
	\end{proof}
	\begin{theorem}[2nd Order MVT]
		\begin{align}
			g(x) &= g(y) + g'(y) (x - y) + \frac{1}{2} g''(\xi) (x - y)^2\quad \xi \in (x, y)
		\end{align}
	\end{theorem}
	
	\begin{algorithm}[Newton's Method in $\R^n$]
		Let $f: \Omega \subseteq \R^n \to \R$ where $\Omega$ is open, let initial point $x_0 \in \Omega$. \\
		Suppose $\nabla^2 f(x_n)$ is invertible for every generated $n$, and $\nabla f(x_*) = 0$ so that algorithm stops at minimum. \\
		The iterative algorithm is defined as following:
		\begin{align}
			x_{n+1} \leftarrow x_n - [\nabla^2 f(x_n)]^{-1} \nabla f(x_n)
		\end{align}
	\end{algorithm}
	
	\begin{theorem}[Generalization]
		Suppose $x_* \in \Omega$ and $f \in C^3(\Omega, \R)$ such that $\nabla f(x_*) = 0$ and $\nabla^2 f(x_*)$ is invertible.\todo{check this} Then if initial point $x_0$ is sufficiently closed to $x_*$, then Newton's method converges to $x_*$.
	\end{theorem}
	
	\begin{proof}
		The basic idea is the same as the $\R$ case: prove the iterative algorithm induces a contraction mapping.
	\end{proof}
	
	\begin{example}[Newton's Method Fails to Converge]
		Even if $f$ has an unique global minimum $x_*$, and $x_0$ is arbitrarily close to the $x_*$, Newton's method could fail to converge. \\
		Consider
		\begin{align}
			f(x) = \frac{2}{3} \abs{x}^{\frac{3}{2}}
		\end{align}
		Note that
		\begin{align}
			f(x)&=\begin{cases}{\frac{2}{3} x^{\frac{3}{2}}} & {x \geq 0} \\ {-\frac{2}{3} x^{\frac{3}{2}}} & {x<0}\end{cases} \\
			f'(x) &= \begin{cases}
				x^{\frac{1}{2}} & x \geq 0 \\
				- x^{\frac{1}{2}} & x < 0
			\end{cases} \\
			f''(x) &= \begin{cases}
				\frac{1}{2}x^{-\frac{1}{2}} & x > 0 \\
				- \frac{1}{2}x^{-\frac{1}{2}} & x < 0 \\
				\tx{DNE} & x = 0
			\end{cases}
		\end{align}
		Therefore $f \notin C^2$. \\
		Let $\delta > 0$ arbitrarily small, take initial point $x_0 \in V_\delta(0)$. WLOG, $x_0 = \varepsilon \in V_\delta(0)$ with $\varepsilon > 0$. The algorithm will oscillate between $\pm \varepsilon$ and never converge.
	\end{example}
	
	\begin{remark}
		Newton's method does not necessarily converge to a global minimum, it may converge to local minimum or local maximum or even saddle point.
	\end{remark}
	
	\begin{example}[Newton's Method Converges to a Saddle Point]
		Consider $f(x) = x^3$, $x_{n+1} \to \frac{x_n}{2}$, which converges to 0 (a saddle point).
	\end{example}
	
	\begin{example}[Newton's Method on Quadratic Function]
		Let $Q$ be a symmetric $n \times n$ invertible matrix. Define quadratic form $f(x) := \frac{1}{2}x^T Q x: \R^n \to \R$. The optimal is $x = 0$. \\
		Let $x_0 \in \R^n$, then $x_1 := x_0 - H_f(x_0)^{-1} \nabla f(x_0) = x_0 - Q^{-1}Qx_0 = 0$. \emph{Therefore, Newton's method converges in one iteration.} 
	\end{example}
	
	\subsection{Steepest/Gradient Descent}
	\begin{algorithm}[Steepest Descent]
		Let $f: \Omega \to \R$ where $\Omega$ is an open subset of $\R^n$. Let initial point $x_0 \in \Omega$. \\
		To minimize $f$ on $\Omega$, iteratively update $x$ follows at each step $k$:
		\begin{align}
			x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)
		\end{align}
		where $\alpha_k = \argmin_{\alpha \geq 0} f(x_k - \alpha \nabla f(x_k)$. \\
		\emph{Remark: There might be multiple minimizing $\alpha$, in real world implementations, we take the least minimizer found.}
	\end{algorithm}
	
	\begin{theorem}[Gradient Descending is Descending]
		At every step $k$, if $\nabla f(x_k) = 0$, the algorithm terminates. Otherwise,
		\begin{align}
			f(x_{k+1}) < f(x_k)
		\end{align}
	\end{theorem}
	\begin{proof}
		Suppose $\nabla f(x_k) \neq 0$. \\
		Note that for the first minimizing $\alpha_k$ found:
		\begin{align}
			f(x_{k+1}) &= f(x_k - \alpha_k \nabla f(x_k)) \\
			&\leq f(x_k - \alpha \nabla f(x_k))\quad \forall 0 \leq \alpha \leq \alpha_k
		\end{align}
		Recall that
		\begin{align}
			\left. \frac{d}{ds} \right|_{s=0} f(x_k - s\nabla f(x_k)) = - \nabla f(x_k) \cdot \nabla f(x_k) = - \norm{\nabla f(x_k)}_2^2 < 0
		\end{align}
		Therefore,
		\begin{align}
			f(x_{k+1}) \leq f(x_k - \alpha \nabla f(x_k)) < f(x_k) \tx{ for small }\alpha
		\end{align}
	\end{proof}
	
	\begin{theorem}[Gradient Descending Induces Perpendicular Steps]
		The consecutive steps induced by gradient descending are perpendicular. That is
		\begin{align}
			(x_{k+2} - x_{k+1}) \cdot (x_{k+1} - x_k) = 0
		\end{align}
	\end{theorem}

	\begin{proof}
		Note that
		\begin{align}
			(x_{k+2} - x_{k+1}) \cdot (x_{k+1} - x_k) &= (- \alpha_{k+1} \nabla f(x_{k+1})) \cdot  (- \alpha_{k} \nabla f(x_{k})) \\
			&= \alpha_k \alpha_{k+1} \nabla f(x_{k}) \cdot \nabla f(x_{k+1})
		\end{align}
		If $\alpha_k = 0$, done. \\
		If $\alpha_k > 0$,
		\begin{align}
			f(x_{k+1}) &= f(x_k) - \alpha_k \nabla f(x_k) \\
			&= \min_{\alpha \geq 0} \{f(x_k - \alpha \nabla f(x_k))\} \\
			&= \min_{\red{\alpha > 0}} \{f(x_k - \alpha \nabla f(x_k))\} \\
			&\implies \left. \pd{}{\alpha} \right|_{\alpha = \alpha_k} f(x_k - \alpha \nabla f(x_k)) = 0 \\
			&\implies - \nabla f(x_k - \alpha_k \nabla f(x_k)) \cdot \nabla f(x_k) = 0 \\
			&\implies - \nabla f(x_{k+1}) \cdot \nabla f(x_k) = 0
		\end{align}
	\end{proof}
	
	\begin{theorem}[Sufficient Condition for Gradient Descent to Converge]
		Let $f \in C^1$ on open $\Omega \subseteq \R^n$. \\
		Let $\{x_k\}$ be the sequence generated by gradient descent: $x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)$. \\
		If $(x_k)$ is bounded in $\Omega$, that is, there exists a \ul{compact} set $K \subseteq \Omega$ such that $(x_k) \subseteq K$, \\
		then every convergent subsequence of $(x_k)$ converges to a critical point $x_* \in \Omega$ of $f$. 
	\end{theorem}
	
	\begin{proof}
		\todo{Need to fix this proof.}
		Let $x_k \in K$ compact. \\
		Then there exists subsequence $x_{k_i} \to x_* \in K$. \\
		\emph{Show:} $\nabla f(x_*) = 0$. \\
		Note that $f(x_k) \geq f(x_{k+1})$ for every $k \in \N$, therefore $f(x_{k_i}) \searrow f(x_*)$. Therefore, $f(x_{k}) \searrow f(x_*)$.
		\todo{Show this}.
		Suppose, for contradiction, $\nabla f(x_*) \neq 0$. \\
		By continuity of $\nabla f$, $(\nabla f(x_{k_i})) \to \nabla f(x_*)$. \\
		Let $y_{k_i} := x_{k_i} - \alpha_{k_i} \nabla f(x_{k_i}) = x_{k_{i+1}}$. \\
		Note that $y_{k_i}$ has a convergent subsequence converging to $y_*$. \\
		WLOG, $(y_{k_i}) \to y_*$. \\
		Observe
		\begin{align}
			\alpha_{k_i} &= \frac{\abs{y_{k_i} - x_{k_i}}}{\norm{\nabla f(x_{k_i})}} \\
			\implies \lim_{k_i \to \infty} a_{k_i} &= \frac{\abs{y_* - x_*}}{\norm{\nabla f(x_*)}} =: \alpha_*
		\end{align}
		Put back: $y_* = x_* - \alpha_* \nabla f(x_*)$. \\
		Now $f(y_{k_i}) = f(x_{k_{i+1}}) = \min_{\alpha \geq 0} f(x_{k_i} - \alpha \nabla f(x_{k_i}))$, which implies
		\begin{align}
			f(y_{k_i}) &\leq f(x_{k_i} - \alpha f(x_{k_i}))\ \forall \alpha \geq 0 \\
			\forall \alpha \geq 0\ \lim_{i \to \infty} f(y_{k_i}) &= f(y_*) \leq \lim_{i \to \infty} f(x_{k_i} - \alpha f(x_{k_i})) = f(x_* - \alpha \nabla f(x_*)) \\
			\implies f(y_*) &\leq \min_{\alpha \geq 0} f(x_* - \alpha \nabla f(x_*)) < f(x_*)
		\end{align}
		Further note that 
		\begin{align}
			f(y_*) = \lim_{i \to \infty} f(y_{k_i}) = \lim_{i \to \infty} f(x_{k_{i+1}}) = f(x_*)
		\end{align}
		Contradiction.
	\end{proof}
	
	\subsubsection{Steepest Descent: the Quadratic Case}
	\begin{example}
		Let $f$ follow the general quadratic form
		\begin{align}
			f(x) &= \frac{1}{2} x^T Q x - b^T x
		\end{align}
		with $b, x \in \R^n$ and $Q$ is \ul{positive definite}. \\
		Let $0 < \lambda = \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n = \Lambda$ be eigenvalues of $Q$.
	\end{example}
	
	\begin{proposition}
		Gradient descent on  strictly convex(concave) quadratic functions is guaranteed to converge to the unique global minimum(maximum). \\
		Essentially, we are going to define an auxiliary function $g$, preserving the optimizing behaviour in a sense that
		\begin{align}
			\argmin g(x) &= \argmin f(x)
		\end{align}
		and then show the convergence property on $f(x)$ indirectly by showing converging property on $g(x)$.
	\end{proposition}
	
	\begin{lemma}
		Recall that positive definite $Q$ implies the existence of \ul{unique} minimizer $x_*$. The minimizer satisfies the first order necessary condition.
		\begin{align}
			Q x_* - b &= 0 \\
			\iff x_* &= Q^{-1} b
		\end{align}
		Where the second equation came from the invertibility of positive definite matrices.
		One can rewrite auxiliary function capturing the optimization behaviours $f$ as following 
		\begin{align}
			g(x) &:= \frac{1}{2} (x - x_*)^T Q (x - x_*) \\
			&= \underbrace{\frac{1}{2} x^T Q x - \overbrace{x^T Q x_*}^{x^T b}}_{f(x)} + \frac{1}{2} x_*^T Q x_*
		\end{align}
		Because $Q$ is positive definite:
		\begin{align}
			g(x) &\geq 0\ \forall x \in \R^n \\
			g(x) &= 0 \iff x = x_*\\
			\nabla f(x) &= \nabla g(x) = Q x - b
		\end{align}
		Where the last equation came from the fact that $f(x)$ and $g(x)$ differ by a constant.\\
		Therefore, using the method of steepest descent,
		\begin{align}
			x_{k+1} &= x_k - \alpha_k \nabla g(x_k)
		\end{align}
		where $\alpha_k \in \argmin_{\alpha \geq 0}f(x_k - \alpha \nabla g(x_k))$. \\
		The necessary condition for minimizations suggests $\alpha_k$ must satisfy
		\begin{align}
			0 = \left. \frac{d}{d\alpha}\right|_{\alpha = \alpha_k} f(x_k - \alpha \nabla g(x_k)) 
			&= \nabla f(x_k - \alpha_k \nabla g(x_k)) \cdot (-\nabla g(x_k))\\
			&= - [Q(x_k - \alpha_k \nabla g(x_k)) - b] \cdot \nabla g(x_k) \\
			&= - [Qx_k - \alpha_k Q \nabla g(x_k) - b] \cdot \nabla g(x_k) \\
			&= - [\nabla g(x_k) - \alpha_k Q \nabla g(x_k)] \cdot \nabla g(x_k) \\
			&= - \norm{\nabla g(x_k)}_2^2 + \alpha_k \nabla g(x_k)^T Q \nabla g(x_k) \\
			\implies \alpha_k &= \frac{\norm{\nabla g(x_k)}_2^2}{\nabla g(x_k)^T Q \nabla g(x_k)}
		\end{align}
	\end{lemma}
	
	\begin{assumption}
		\todo{Need to check if this assumption is required.\\}
		Assume $Q$ is symmetric.
	\end{assumption}
	
	\begin{lemma}
		The iterative updating from gradient descent on $g(x)$ is a contraction mapping. That is,
		\begin{align}
			g(x_{k+1}) &= \underbrace{\left(1 - \frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{-1} \nabla g(x_k)]} \right)}_{\in [-1, 1]} g(x_k)
		\end{align}
	\end{lemma}
	\begin{proof}
		\begin{align}
			g(x_{k+1}) &\equiv g(x_k - \alpha_k \nabla g(x_k)) \\
			&\equiv \frac{1}{2} [x_k - \alpha_k \nabla g(x_k) - x_*]^T Q [x_k - \alpha_k \nabla g(x_k) - x_*] \\
			&= \frac{1}{2} [x_k - x_* - \alpha_k \nabla g(x_k)]^T Q [x_k - x_* - \alpha_k \nabla g(x_k)] \\
			&= \underbrace{\frac{1}{2} (x_k - x_*)^T Q (x_k - x_*)}_{g(x_k)} - \alpha_k \nabla g(x_k)^T Q (x_k - x_*) + \frac{1}{2} a_k^2 \nabla g(x_k)^T Q \nabla g(x_k) \\
			\implies g(x_k) - g(x_{k+1}) &= - \frac{1}{2} a_k^2 \nabla g(x_k)^T Q \nabla g(x_k) + \alpha_k \nabla g(x_k)^T Q \underbrace{(x_k - x_*)}_{=:y_k} \\
			\implies \frac{g(x_k) - g(x_{k+1})}{g(x_k)} &= \frac{- \frac{1}{2} a_k^2 \nabla g(x_k)^T Q \nabla g(x_k) + \alpha_k \nabla g(x_k)^T Q y_k}{\frac{1}{2} y_k^T Q y_k} \\
			&= \frac{2 \alpha_k \nabla g(x_k)^T Q y_k - a_k^2 \nabla g(x_k)^T Q \nabla g(x_k)}{y_k^T Q y_k}
		\end{align}
		Note that the first order condition implies $Qx_* = b$. \\
		Therefore, $\nabla g(x_k) = Qx_k - b = Q x_k - Qx_* = Qy_k$, which implies $y_k = Q^{-1} \nabla g(x_k)$.
		\begin{align}
			\frac{2 \alpha_k \nabla g(x_k)^T Q y_k - a_k^2 \nabla g(x_k)^T Q \nabla g(x_k)}{y_k^T Q y_k}
			&= \frac{2 \alpha_k \nabla g(x_k)^T Q Q^{-1}\nabla g(x_k) - a_k^2 \nabla g(x_k)^T Q \nabla g(x_k)}{\nabla g(x_k)^T Q^{-T} Q Q^{-1} \nabla g(x_k)} \\
			&= \frac{2 \alpha_k \norm{\nabla g(x_k)}_2^2 - a_k^2 \nabla g(x_k)^T Q \nabla g(x_k)}{\nabla g(x_k)^T Q^{-T} \nabla g(x_k)}
		\end{align}
		Plug in the $\alpha_k$ computed before:
		\begin{align}
			... &= \frac{2 \frac{\norm{\nabla g(x_k)}_2^2}{\nabla g(x_k)^T Q \nabla g(x_k)} \norm{\nabla g(x_k)}_2^2 -  \frac{\norm{\nabla g(x_k)}_2^4}{(\nabla g(x_k)^T Q \nabla g(x_k))^2} \nabla g(x_k)^T Q \nabla g(x_k)}{\nabla g(x_k)^T Q^{-T} \nabla g(x_k)} \\
			&= \frac{2 \frac{\norm{\nabla g(x_k)}_2^4}{\nabla g(x_k)^T Q \nabla g(x_k)} - \frac{\norm{\nabla g(x_k)}_2^4}{\nabla g(x_k)^T Q \nabla g(x_k)}}{\nabla g(x_k)^T Q^{-T} \nabla g(x_k)} \\
			&= \frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{-T} \nabla g(x_k)]} \\
			&=\frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{\red{-1}} \nabla g(x_k)]} \quad \because Q \in \mathbb{S}^n \\
			\implies g(x_k) - g(x_{k+1}) &= \left \{
				\frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{-1} \nabla g(x_k)]}
			\right \} g(x_k) \\
			\implies g(x_{k+1}) &= \left \{1 - \left [
				\frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{-1} \nabla g(x_k)]}
			\right ] \right \} g(x_k)
		\end{align}
		\begin{lemma}[Kantorovich Inequality]
			Let $Q$ be a $n\times n$ positive definite symmetric matrix with eigenvalues $0 < \lambda = \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n = \Lambda$. Then, for any $v \in \R^n$:
			\begin{align}
				\frac{\norm{v}^4_2}{(v^T Q v)(v^T Q^{-1} v)} \geq \frac{4 \lambda \Lambda}{(\lambda + \Lambda)^2}
			\end{align}
		\end{lemma}
		\begin{proof}[Proof of Kantorovich Inequality]
			\todo{Prove this lemma}
		\end{proof}
		Therefore, 
		\begin{align}
			g(x_{k+1}) &= \left \{1 - \left [
				\frac{\norm{\nabla g(x_k)}_2^4}{[\nabla g(x_k)^T Q \nabla g(x_k)][\nabla g(x_k)^T Q^{-1} \nabla g(x_k)]}
			\right ] \right \} g(x_k) \\
			&\leq \left \{1 -  \frac{4 \lambda \Lambda}{(\lambda + \Lambda)^2} \right \} g(x_k) \\
			&= \frac{(\lambda - \Lambda)^2}{(\lambda + \Lambda)^2} g(x_k)
		\end{align}
	\end{proof}
	\begin{theorem}
		For any initial point $x_0 \in \R^n$, gradient descent converges to the unique minimum point $x_*$ of the quadratic $f(x) = x^T Q x - b^T x$.
	\end{theorem}
	
	\begin{proof}
		Define $q(x) := \frac{1}{2} (x - x_*)^T Q (x) (x - x_*)$. \\
		Note that $q(x)$ and $f(x)$ differ by a constant, therefore $\argmin q(x) = \argmin f(x)$. \\
		Moreover, we've shown:
		\begin{align}
			q(x_{k+1}) &\leq \underbrace{\left(\frac{\Lambda - \lambda}{\Lambda + \lambda} \right)^2}_{\in [0, 1)} q(x_k)
		\end{align}
		It is easy to notice that
		\begin{align}
			q(x_k) &\leq r q(x_{k-1}) \\
			\implies q(x_k) &\leq r^k q(x_0) \\
			\implies q(x_k) &\in \{x \in \R^k: q(x) \leq r^k q(x_0)\} =: \mc{L}_k
		\end{align}
		Note that the sub-level set $\mc{L}_k$ is strictly decreasing (i.e. $\mc{L}_{k+1} \subsetneq \mc{L}_k$). \\
		Further, note that $x_*$ is the only point satisfying the inequality at the limit:
		\begin{align}
			q(x_*) = 0 = \lim_{k \to \infty} q(x_0)
		\end{align}
		Therefore, $\lim_{k \to \infty}\mc{L} = \{0\}$, and $(x_k) \to x_*$.
	\end{proof}
	
	\begin{remark}
		Note that 
		\begin{align}
			r &= \left(
			\frac{\frac{\Lambda}{\lambda} - 1}{\frac{\Lambda}{\lambda} + 1}
			\right)^2 \in [0, 1) \\
			&= \left(\frac{C - 1}{C + 1}\right)^2
		\end{align}
		where $C = \frac{\Lambda}{\lambda}$ is the \textbf{condition number} of $Q$. \\
		Clearly, when $\lambda = \Lambda$, $r = 0$ and gradient descent converges to the unique global minimum after only one epoch. \\
		While $C \gg 1$, $r \approx 1$ and the \ul{worst case} of gradient descent converges slowly.
	\end{remark}
	
	\subsection{Method of Conjugate Directions}
	\paragraph{Motivation} Method of conjugate directions is designed for quadratic functions with form $\frac{1}{2} x^T Q x - b^T x$. For other functional forms, one can approximate the function using quadratic form firstly and then apply method of conjugate directions.
	
	\begin{definition}
		Let $Q \in \mathbb{S}^n$, $d, d' \in \R^n$ are \textbf{$Q$-orthogonal or $Q$-conjugate} if 
		\begin{align}
			d^T Q d' = 0
		\end{align}
		\emph{Remark: note that $d^T Q d' = d'^T Q d$ because $Q$ is symmetric.}
	\end{definition}
	
	\begin{definition}
		Finite set $D = (d_0, \cdots, d_k) \subseteq \R^n$ is a \textbf{$Q$-orthogonal} set if
		\begin{align}
			\forall i \neq j,\ d_i^T Q d_j = 0
		\end{align}
		That is, $D$ is \emph{orthogonal in pairs}.
	\end{definition}
	
	\begin{example}
		When $Q = I_n$, the notional of $Q$-orthogonal becomes the conventional notion of orthogonality.
	\end{example}
	
	\begin{proposition}
		Let $d, d'$ be two eigenvectors of $Q$ with different eigenvalues $\lambda, \lambda'$, then $d, d'$ are $Q$-orthogonal.
	\end{proposition}
	
	\begin{proof}
		Let $Q v = \lambda v$ and $Q w = \lambda' w$ where $\lambda \neq \lambda'$. \\
		Note that 
		\begin{align}
			\inner{v}{Qw} &= \inner{v}{\lambda' w} \\
			&= \lambda' \inner{v}{w}
		\end{align}
		because inner product is bilinear. \\
		Similarly,
		\begin{align}
			\inner{v}{Qw} &= \inner{Q^T v}{w} \\
			&= \inner{Q v}{w} \\
			&= \inner{\lambda v}{w} \\
			&= \lambda \inner{v}{w}\\
			\implies (\lambda'  - \lambda) \inner{v}{w} &= 0
		\end{align}
		where $\lambda'  - \lambda \neq 0$. \\
		Therefore, $\inner{v}{w} = 0$. \\
		Further, 
		\begin{align}
			v^T Q w &= \inner{v}{Qw} \\
			&= \inner{v}{\lambda w} \\
			&= \lambda \inner{v}{w} = 0
		\end{align}
		So $v, w$ are $Q$-orthogonal.
	\end{proof}
	
	\begin{proposition}
		Let $Q \in \mathbb{S}^n$, then there exists a set of orthogonal eigen-basis of $Q$.
	\end{proposition}
	
	\begin{corollary}
		The orthogonal eigen-basis of $Q$ is also $Q$-orthogonal.
	\end{corollary}
	
	\begin{proof}
		\begin{align}
			\forall i \neq j,\ d_i^T Q d_j &= d_i^T \lambda_j d_j = \lambda_j \inner{d_i}{d_j} = 0
		\end{align}
	\end{proof}
	
	\begin{proposition}
		Let $Q \in \mathbb{S}_+^n$, let $d_0, \cdots, d_k \neq 0$ be a set of $Q$-orthogonal vectors with $k \leq n - 1$, then $d_0, \cdots, d_k$ are linearly independent.
	\end{proposition}
	
	\begin{proof}
		Suppose
		\begin{align}
			\alpha_0 d_0 + \alpha_1 d_1 + \cdots + \alpha_k d_k = 0
		\end{align}
		 For every $i \in \{0, 1, \cdots, k\}$, multiply $d_i^T Q$ on both sides of the equation:
		 \begin{align}
		 	\underbrace{\alpha_0 d_i^T Q d_0
		 	\alpha_1 + d_i^T Q d_1 
		 	+ \cdots}_{=0}
		 	+ \alpha_i d_i^T Q d_i
		 	+ \underbrace{\cdots 
		 	+\alpha_k d_i^T Q d_k}_{=0} = 0
		 \end{align}
		 Further, $d_i^T Q d_i > 0$ because $Q$ is positive definite. Hence, $\alpha_i = 0$ for every $i$, and $d_0, \cdots, d_k$ are linearly independent.
	\end{proof}
	
	\begin{lemma}[Theorems Covered so Far]
		Recall that 
		\begin{enumerate}[(i)]
			\item $d_i, d_j$ are $Q$-orthogonal if $d_i^T Q d_j = 0$;
			\item Eigen-vectors with different eigenvalues are $Q$-orthogonal;
			\item $Q$ symmetric $\implies$ there exists an orthogonal basis $\implies$ the set of basis is $Q$-orthogonal as well;
			\item $Q$-orthogonal vectors are linearly independent.
		\end{enumerate}
	\end{lemma}
	
	\begin{example}[Special Case: Method of Conjugate Direction on Quadratic Functions]
		Let $Q \in \mathbb{S}_{++}^n$ and minimizing the quadratic function
		\begin{align}
			\min f(x) = \frac{1}{2} x^T Q x - b^T 
		\end{align}
		Recall that the unique global minimum is $x^* = Q^{-1} b$. \\
		Let $d_0, d_1, \cdots, d_{n-1}$ be non-zero $Q$-orthogonal vectors. \\
		Note that they are linearly independent by the previous theorem. \\
		Therefore, they form a basis of $\R^n$. \\
		The global minimum can be represented as 
		\begin{align}
			x^* &= \sum_{j=1}^{n-1} \alpha_j d_j\ \alpha_j \in \R
		\end{align}
		For every $j$, the following holds
		\begin{align}
			d_j^T Q x^* &= \alpha_j d_j^T Q d_j \\
			\implies \alpha_j &= \frac{d_j^T Q x^*}{d_j^T Q d_j}
		\end{align}
	\end{example}
	
	\begin{algorithm}[Method of Conjugate Directions] \quad
		\begin{enumerate}[(i)]
			\item Let $Q \in \mathbb{S}_{++}^n$ and $\{d_j\}_{j=0}^{n-1}$ be a set of non-zero $Q$-orthogonal vectors, note that they form a basis of $\R^n$. \\
			\item Given initial point $x_0 \in \R^n$, the method of conjugate direction generates a sequence of points $\{x_k\}_{k=0}^n$ as the following:
			\begin{align}
				x_{k+1} &\leftarrow x_k + \alpha_k d_k \\
				\alpha_k &:= - \frac{\inner{g_k}{d_k}}{d_k^T Q d_k} \quad g_k := \nabla f(x_k)
			\end{align}
		\end{enumerate}
	\end{algorithm}
	
	\begin{theorem}
		Given the method of conjugate, the sequence of points generated eventually reaches the global minimum. That is, $x_n = x^*$.
	\end{theorem}
	
	\begin{proof}
		Let $x^*, x_0 \in \R^n$, consider
		\begin{align}
			x^* - x_0 &= \sum_{j=0}^{n-1} \beta_j d_j \\
			\iff x^* &= x_0 + \sum_{j=0}^{n-1} \beta_j d_j \\
			d_j^T Q (x^* - x_0) &= \beta_j d_j^T Q d_j \\
			\implies \beta_j &= \frac{d_j^T Q(x^* - x_0)}{d_j^T Q d_j}
		\end{align}
		Note that the algorithm generates the sequence as following:
		\begin{align}
			x_k &= x_0 + \sum_{j=0}^{k-1} \alpha_j d_j \\
			\implies (x_k - x_0) &= \sum_{j=0}^{k-1} \alpha_j d_j \\
			\implies d_k^T Q (x_k - x_0) &= \sum_{j=0}^{k-1} \alpha_j d_k^T Q d_j = 0
		\end{align}
		Therefore, 
		\begin{align}
			\beta_k &= \frac{d_k^T Q(x^* - x_0)}{d_k^T Q d_k} \\
			&= \frac{d_k^T Q(x^* - x_0) - d_k^T Q (x_k - x_0)}{d_k^T Q d_k} \\
			&= \frac{d_k^T Q(x^* - x_k)}{d_k^T Q d_k} \\
			&= \frac{d_k^T (Qx^* - Qx_k)}{d_k^T Q d_k}
		\end{align}
		The first order necessary condition suggests $Qx_* = b$,
		\begin{align}
			\beta_k &= \frac{d_k^T (Qx^* - Qx_k)}{d_k^T Q d_k} \\
			&= \frac{d_k^T (b - Qx_k)}{d_k^T Q d_k}  \\
			&= - \frac{d_k^T (Qx_k - b)}{d_k^T Q d_k} \\
			&= - \frac{d_k^T \nabla f(x_k)}{d_k^T Q d_k} = \alpha_k
		\end{align}
		Consequently, 
		\begin{align}
			x^* &= x_0 + \sum_{j=0}^{n-1} \beta_j d_j \\
			&= x_0 + \sum_{j=0}^{n-1} \alpha_j d_j \\
			&= x_n
		\end{align}
	\end{proof}
	
	\subsection{Geometric Interpretations of Method of Conjugate Directions}
	\begin{theorem}
		Let $f \in C^1(\Omega, \R)$, where $\Omega$ is a convex subset of $\R^n$, then $x_0$ is a local minimum of $f$ on $\Omega$ if and only if 
		\begin{align}
			\nabla f(x_0) \cdot (y - x_0) \geq 0\ \forall y \in \Omega
		\end{align}
	\end{theorem}
	
	\begin{corollary}
		Now consider the special case in which $\Omega$ is an affine hyperplane, that is,
		\begin{align}
			\Omega &= \{x \in \R^n: cx + b = 0\}
		\end{align}
		where dim($\Omega$) is $n-1$. \\
		Note that for every $y \in \Omega$, $\nabla f(x_0) \cdot (y - x_0) \geq 0$. For any feasible direction $a$ at point $x_0$, by the definition of hyperplane, $-a$ is a feasible direction as well. \\
		Consequently, $a \cdot \nabla f(x_0)$ for every feasible direction. That is, $\nabla f(x_0) \perp \Omega$.
	\end{corollary}
	
	\begin{notation}
		Let $d_0, d_1, \cdots, d_{n-1}$ be a set of non-zero $Q$-orthogonal vectors in $\R^n$.\\
		For every $k \in \{0, 1, \cdots, n\}$, let
		\begin{align}
			\mc{B}_k &= \tx{span} \{d_0, \cdots, d_{k-1}\}
		\end{align}
		In particular, $\mc{B}_0 = \{0\}$ and $\mc{B}_{n} = \R^n$.
	\end{notation}
	
	\begin{theorem}
		The sequence $\{x_k\}$ generated from $x_0 \in \R^n$ by conjugate direction method has the property that $x_k$ minimizes $f(x) = \frac{1}{2}x^T Q x - b^Tx$ on the affine hyperplane $x_0 + \mc{B}_k$. \\
		That is, 
		\begin{align}
			x_k \in \argmin_{x \in x_0 + \mc{B}_k} f(x)
		\end{align}
	\end{theorem}
	
	\begin{proof}
		Recall that $x_k$ is the the minimizer of $f$ on the affine hyperplane if and only if $\nabla f(x_k) \perp x_0 + \mc{B}_k$. \\
%		Note
%		\begin{align}
%			f(x) &= (x - x^*)^T Q (x - x^*) = \sum_{i=1}^n \lambda_i y_i^2
%		\end{align}
		It is enough to show that $\nabla f(x_k) \perp \mc{B}_k$.
		\emph{Base Case:} $g_0 := \nabla f(x_0) \perp \{0\}$ trivially. \\
		\emph{Inductive Step:} Assume $g_k \perp \mc{B}_k$, show $g_{k+1} \perp \mc{B}_{k+1}$:
		\begin{align}
			g_k &\perp \mc{B}_k \\
			\implies g_k &\perp \tx{span}\{d_0, \cdots, d_{k-1}\} \\
			\inner{g_{k+1}}{d_k} &= \inner{g_{k} + \alpha_k Q d_k}{d_k} \\
			&= \inner{g_{k}}{d_k} + \alpha_k \inner{Q d_k}{d_k} \\
			&= \inner{g_{k}}{d_k} - \frac{\inner{g_k}{d_k}}{d^T_k Q d_k} d_k^T Q d_k
			= 0 \\
			\implies g_{k+1} &\perp d_k \\
			\inner{g_{k+1}}{d_i} &= \inner{g_{k} + \alpha_k Q d_k}{d_i} \quad 0 \leq i < k \\
			&= \inner{g_{k}}{d_i} + \inner{\alpha_k Q d_k}{d_i} \\
			&= 0 + 0 = 0 \\
			\implies g_{k+1} &\perp \mc{B}_k
		\end{align}
	\end{proof}
	
	\begin{corollary}
		$x_n$ (the final output of the method of conjugate gradient) minimizes $f(x)$ on $x_0 + \mc{B}_n = \R^n$.
	\end{corollary}
	
	\begin{corollary}
		Let $q(\cdot)$ be a quadratic function, then
		\begin{align}
			0 \leq q(x_k) = \min_{x \in x_0 + \mc{B}_k} q(x) \leq q(x_k) = \min_{x \in x_0 + \mc{B}_{k-1}} q(x)
		\end{align}
	\end{corollary}
	
	\begin{proof}
		The result is immediate by noting that $x_0 + \mc{B}_{k-1} \subseteq x_0 + \mc{B}_k$.
	\end{proof}
	
	\section{Infinite Dimensional Optimization}
	\subsection{Calculus of Variation}
	\subsubsection{The Canonical Example}
	\paragraph{Problem Setup} Let
	\begin{align}
		\mc{A} := \{u:[0, 1] \to \R : u \in C^1 \land u(0) = u(1) = 1\}
	\end{align}
	And let $F[\cdot]: \mc{A} \to \R$ denote the \textbf{functional} defined as 
	\begin{align}
		F[u(\cdot)] := \frac{1}{2} \int_0^1 u^2(x) + u'^2(x)\ dxs
	\end{align}
	Consider the minimization problem over the \emph{space of functions}:
	\begin{align}
		\min F[u]\ s.t.\ u \in \mc{A}
	\end{align}
	\paragraph{Optimizing via Disturbance (Test Functions)} The general idea of solving such infinite dimensional problem would be \ul{reducing it to a collection of one dimensional problems}. \\
	Let $v(\cdot) \in C^1([0, 1], \R)$ such that $v(0) = v(1) = 0$. \\
	Suppose $u^*(\cdot)$ is the minimizer of the problem stated above. \\
	Consider
	\begin{align}
		u^*(\cdot) + s v(\cdot)\quad s \in \R
	\end{align}
	Notice that by construction, $u^*(\cdot) + s v(\cdot) \in \mc{A}$ for every $s \in \R$. \\
	Define the auxiliary function 
	\begin{align}
		f(s) := F[u^* + sv] \in \R^\R
	\end{align}
	The attained when $s=0$, the first order necessary condition of minimization suggests:
	\begin{align}
		f'(0) &= \left. \frac{d}{ds} \right|_{s=0} F[u^*(\cdot) + sv(\cdot)] \\
		&= \left. \frac{d}{ds} \right|_{s=0} \frac{1}{2}
		\int_0^1 \left[u_*(x) + sv(x) \right]^2 + \left[u_*'(x) + sv'(x) \right]^2\ dx \\
		&= \left. \frac{d}{ds} \right|_{s=0} \left \{
		\frac{1}{2} \int_0^1 [u_*(x)^2 + u_*'(x)^2]\ dx
		+ \int_0^1 s[u_*(x) v(x) + u_*'(x) v'(x)]\ dx 
		+ \frac{s^2}{2} \int_0^1 v^2(x) + v'^2(x)\ dx
		\right \} \\
		&= \left. \frac{d}{ds} \right|_{s=0} \int_0^1 s[u_*(x) v(x) + u_*'(x) v'(x)]\ dx \\
		&= \int_0^1\left. \frac{d}{ds} \right|_{s=0} s[u_*(x) v(x) + u_*'(x) v'(x)]\ dx \\
		&= \int_0^1 u_*(x) v(x) + u_*'(x) v'(x)\ dx = 0\quad (\dagger')
	\end{align}
	Where $(\dagger')$ is the primitive form of the first order necessary condition. \\
	Furthermore,
	\begin{align}
		\int_0^1 u_*(x) v(x) + u_*'(x) v'(x)\ dx
		&= \int_0^1 u_*(x) v(x)\ dx + \int_0^1 u_*'(x) v'(x)\ dx \\
		&= \int_0^1 u_*(x) v(x)\ dx
		+ \left. u_*(x) v(x) \right|_{0}^1
		+ \int_0^1 u_*''(x) v(x)\ dx \\
		&= \int_0^1 u_*(x) v(x)\ dx
		+ \int_0^1 u_*''(x) v(x)\ dx \\
		&= \int_0^1 [u_*(x) + u_*''(x)] v(x)\ dx
	\end{align}
	The first order necessary condition is amount to
	\begin{align}
		\int_0^1 [u_*(x) + u_*''(x)] v(x)\ dx = 0
	\end{align}
	for every function $v \in C^1([0, 1], \R)$ such that $v(0) = v(1) = 0$.
	
	\begin{lemma}[Du Bois-Reymond Lemma]
		Provided that $u_*(\cdot) \in \mc{A}$ is $C^1$, altogether with the fact that it is the minimizer, $u_*(\cdot)$ is automatically $C^2$.
	\end{lemma}
	
	\begin{notation}
		We call all $v \in C^1([0, 1], \R)$ such that $v(0) = v(1) = 0$ \textbf{test functions}.
	\end{notation}
	
	\begin{lemma}[The Fundamental Lemma of Calculus of Variation]
		Suppose $g$ is continuous on interval $[a, b]$ such that
		\begin{align}
			\int_a^b g(x) v(x)\ dx = 0\ \forall \tx{ test function } v(\cdot)
		\end{align}
		Then $g(x) \equiv 0$ on $[a, b]$.
	\end{lemma}
	
	\begin{proof}\renewcommand{\qedsymbol}{$\contradiction$}
		Suppose, for contradiction, $g(\eta) \neq 0$ for some $\eta \in [0, 1]$. \\
		WLOG, assume $g(\eta) > 0$. \\
		Note that if $\eta$ is at either endpoint, by continuity of $g$, one may choose another $\eta' \in V_\varepsilon(\eta)$ such that $\eta' \in (a, b)$. Therefore, we may assume $\eta \in (a, b)$ without loss of generality. \\
		Then there exists an open interval $(c, d) \subsetneq (a, b)$ such that $g(x) > 0\ \forall x \in (c, d)$. \\
		Define
		\begin{align}
			v(x) := \begin{cases}
				(x-c)^2 (x-d)^2 &\tx{ if } x \in (c, d) \\
				0 &\tx{ otherwise}
			\end{cases}
		\end{align}
		Clearly, $v(\cdot) \in C^1([0, 1], \R)$ by noticing
		\begin{align}
			v'(x) = \begin{cases}
				2(x-c)(x-d)^2 + 2 (x-c)^2(x-d) &\tx{ if } x \in (c, d) \\
				0 &\tx{ otherwise}
			\end{cases}
		\end{align}
		Obviously,
		\begin{align}
			\int_a^b g(x) v(x)\ dx = \int_c^d g(x) v(x)\ dx > 0
		\end{align}
	\end{proof}
	
	\begin{proposition}
		\begin{align}
			\int_0^1 (u_*(x) + u_*''(x))v(x)\ dx = 0\ \forall v \implies u_*(\cdot) + u_*''(\cdot) \equiv 0
		\end{align}
	\end{proposition}
	
	\begin{proof}
		By Du Bois-Reymond Lemma, $u_*(\cdot) + u_*''(\cdot)$ is continuous. Because $v(\cdot)$ is a test function and by the fundamental lemma of calculus of variation, $u_*(\cdot) + u_*''(\cdot) \equiv 0$.
	\end{proof}
	
	\subsection{A Classical Example from Physics: Brachistochrone}
	\paragraph{Brachistochrone}(Galileo, 1638) Find the curve connecting two points $A$ and $B$ on which a point mass moves without friction under the influence of gravity moves from $A$ to $B$ in the least time possible.
	
	\subsection{General Class of Problems in Calculus of Variations}
	\paragraph{Constraint}
	\begin{align}
		\mc{A} &:=\{
		u:[a, b] \to \R:
		u \in C^1, u(a) = A, u(b) = B
		\}
	\end{align}
	\paragraph{Objective function}
	\begin{align}
		F[u(\cdot)] &:= \int_a^b L(x, u(x), u'(x))\ dx
	\end{align}
	where $L(x, z, p): [a, b] \times \R \times \R \to \R$.
	
	\begin{example}
		\begin{align}
			L(x, z, p) &= \frac{1}{2}(z^2 + p^2) \\
			F[u(\cdot)] &= \int_0^1 \frac{u(x)^2 + u'(x)^2}{2}\ dx
		\end{align}
	\end{example}
	
	\begin{definition}
		Given $u(\cdot) \in \mc{A}$, suppose $\exists$ function $g(\cdot)$ on $[a, b]$ such that 
		\begin{align}
			\red{\left. \frac{d}{ds} \right |_{s=0} F[u(\cdot) + sv(\cdot)] = \int_a^b g(x) v(x)\ dx\ \forall \tx{ test functions} v(\cdot)}
		\end{align}
		Then $g(\cdot)$ is called the \textbf{variational derivative} of $F$ at $u(\cdot)$, often denoted as $\frac{\delta F}{\delta u}(u)(\cdot)$.
	\end{definition}
	
	\begin{remark}[Analogy to the finite dimensional case]
		In finite dimensional case:
		\begin{align}
			\left. \frac{d}{ds} \right |_{s=0} f(u + sv) = \nabla f(u) \cdot v = \sum_{i=1}^n \nabla_i f(u) v_i\ \forall v \in \R^n
		\end{align}
		And in the infinite dimensional case:
		\begin{align}
			\left. \frac{d}{ds} \right |_{s=0} F[u(\cdot) + s v(\cdot)] = \int_a^b \frac{\delta F}{\delta u}(u)(x) v(x)\ dx\ \forall v \in \mc{T}
		\end{align}
	\end{remark}
	
	\begin{lemma}[Necessary condition on the variational derivative]
		Let 
		\begin{align}
			\mc{A} &:=\{u:[a, b] \to \R:u \in C^1, u(a) = A, u(b) = B\}
		\end{align}
		If $u_*(\cdot) \in \mc{A}$ minimizes $F$ over $\mc{A}$, and if $\frac{\delta F}{\delta u}(u_*)(\cdot)$ exists and is continuous, then it must satisfy
		\begin{align}
			\frac{\delta F}{\delta u}(u_*)(\cdot) \equiv 0
		\end{align}
	\end{lemma}
	
	\begin{proof}
		Note that for every test function $v$, $u_*(\cdot) + s v(\cdot) \in \mc{A}$. \\
		Suppose $u_*$ minimizes $F$ over $\mc{A}$, equivalently,
		\begin{align}
			f(0) = F[u_*] &\leq F[u_* + sv] = f(s) \tx{ for every test function $v$} \\
			\iff f(0) &\leq f(s)\ \forall s \in \R \\
			\implies 0 = f'(0) 
			&= \left. \frac{d}{ds} \right |_{s=0} f(s)
			= \left. \frac{d}{ds} \right |_{s=0} F[u_* + sv] \\
			&= \int_a^b \frac{\delta F}{\delta u}(u)(x)v(x)\ dx \\
			\implies &\frac{\delta F}{\delta u}(u)(\cdot) \equiv 0
		\end{align}
	\end{proof}
	
	\begin{theorem}[Euler-Lagrange]
		Let 
		\begin{align}
			\mc{A} &:=\{u:[a, b] \to \R:u \in C^1, u(a) = A, u(b) = B\}
		\end{align}
		Let $L \in C^2$ such that
		\begin{align}
			F[u(\cdot)] &= \int_a^b L(x, u(x), u'(x))\ dx
		\end{align}
		Then, if $u(\cdot) \in C^1$, then $\frac{\delta F}{\delta u}(u)(\cdot)$ exists and is continuous. \\
		Moreover,
		\begin{align}
			\red{\frac{\delta F}{\delta u}(u)(\cdot)(x) = - \frac{d}{dx} [L_p(x, u(x), u'(x))] + L_z(x, u(x), u'(x))}
		\end{align}
		This equation is often referred to as the \textbf{Euler-Lagrange  equation}.
	\end{theorem}
	
	\begin{proof}
		Let $v$ be a test function.
		\begin{align}
			\left. \frac{d}{ds} \right|_{s=0} F[u(\cdot) + sv(\cdot)]
			&= \left. \frac{d}{ds} \right|_{s=0} \int_a^b
			L(x, u(x) + sv(x), u'(x) + sv'(x))\ dx \\
			&= \int_a^b \left. \frac{d}{ds} \right|_{s=0}
			L(x, z, p)\ dx \\
			&= \int_a^b L_z(\cdot) v(x) + L_p(\cdot) v'(x)\ dx \\
			&= \int_a^b L_z(\cdot) v(x)\ dx + \int_a^b L_p(\cdot) v'(x)\ dx \\
			&= \int_a^b L_z(\cdot) v(x)\ dx + L_p(\cdot) v(x) \Big |_a^b - \int_a^b \frac{d}{dx} L_p(\cdot) v(x)\ dx \\
			&= \int_a^b [-\frac{d}{dx}L_p(\cdot) + L_p(\cdot)] v(x)\ dx\ \forall \tx{ test functions }v(\cdot)
		\end{align}
		The result follows definition of variational derivative. \\
		Further, because $L(\cdot) \in C^2$, $-\frac{d}{dx}L_p(\cdot)$ is continuous. Moreover, $u(\cdot)$ 	and $u'(\cdot)$ are continuous, so is the composite function. Hence, the variational derivative is continuous.
	\end{proof}
	
	\begin{example}
		In the model example,
		\begin{align}
			L(x, z, p) &= \frac{1}{2}(z^2 + p^2) \\
			L_p(x, z, p) &= p \implies L_p(x, u(x), u'(x)) = u'(x) \\
			L_z(x, z, p) &= z \implies L_z(x, u(x), u'(x)) = u(x) \\
			\frac{\delta F}{\delta u}(u)(\cdot) &= - \frac{d}{dx}[u'(x)] + u(x) = -u''(x) + u(x)
		\end{align}
		By above theorem, if $u_*$ is the minimizer, it must be
		\begin{align}
			-u''(x) + u(x) \equiv 0
		\end{align} 
	\end{example}
	
	\begin{example}[Min Arclength]
		Suppose we are trying to minimize
		\begin{align}
			F[u] = \int_a^b \sqrt{1 + u'(x)^2}\ dx = \tx{arclength of } u(\cdot)
		\end{align}
		where 
		\begin{align}
			\mc{A} &:=\{u:[a, b] \to \R:u \in C^1, u(a) = A, u(b) = B\}
		\end{align}
		and
		\begin{align}
			L(x, z, p) &= \sqrt{1 + p^2} \\
			L_z &= 0 \\
			L_p &= \frac{p}{\sqrt{1+p^2}} \\
			\implies \frac{\delta F}{\delta u}(u)(\cdot) 
			&= - \frac{d}{dx} \frac{u'(x)}{\sqrt{1+u'(x)^2}} \equiv 0 \\
			\implies & \frac{u'(x)}{\sqrt{1+u'(x)^2}} = C \\
			\implies u'(x)^2 &= C (1+u'(x)^2) \\
			\implies u'(x)^2 &= C \\
			\implies u'(x) &= \alpha \\
			\implies u(x) &= \alpha x + \beta
		\end{align}
	\end{example}
	
	\begin{example}[Surface Area of Revolution]
		Suppose $u(\cdot) \in C^1$ on $[a, b]$, the surface area of rotating the curve $u$ connecting $a$ and $b$ can be computed as
		\begin{align}
			F[u(\cdot)] &= \int_a^b 2\pi u(x) \sqrt{1 + u'(x)^2}\ dx
		\end{align}
		For simplicity, assume $u > 0$. \\
		In this example, the space of feasible functions is 
		\begin{align}
			\mc{A} &:= \{u: [a, b] \to \R: u \in C^1, u(a) = A, u(b) = B, u > 0\}
		\end{align}
		If $u(\cdot)$ solves the minimization problem, it must be the case that
		\begin{align}
			\frac{\delta F}{\delta u}(u)(\cdot) &\equiv 0\quad (\dagger)
		\end{align}
		Notice
		\begin{align}
			L(x, z, p) &= 2 \pi z \sqrt{1 + p^2} \\
			L_z(x, z, p) &= 2 \pi \sqrt{1 + p^2} \\
			L_p(x, z, p) &= 2 \pi z \frac{p}{\sqrt{1 + p^2}}
		\end{align}
		\textbf{Claim}: the family of $u(\cdot) = \beta \cosh \left(\frac{x-\alpha}{\beta}\right)$ solves the necessary condition $(\dagger)$. \\
		\textbf{Instance 1} When $a=0, b=1, A=B=1$, plugging in the initial condition gives
		\begin{align}
			\begin{cases}
				\beta \cosh \left(\frac{0-\alpha}{\beta}\right) &= 1 \\
				\beta \cosh \left(\frac{1-\alpha}{\beta}\right) &= 1
			\end{cases}
		\end{align}
		solving above system of equations provides the solution. \\
		\textbf{Instance 2} When $a=0, b=1, A=1, B=0$, plugging in these initial conditions gives
		\begin{align}
			\begin{cases}
				\beta \cosh \left(\frac{0-\alpha}{\beta}\right) &= 1 \\
				\beta \cosh \left(\frac{1-\alpha}{\beta}\right) &= 0
			\end{cases}
		\end{align}
		because $\cosh > 0$, the second equation suggests $\beta=0$, but in this case the first equation would never hold. \\
		Therefore, there is no solution to this calculus of variation. \\
		In face, the surface area is minimized by
		\begin{align}
			u(x) = \begin{cases}
				1 &\tx{ if } x = 0 \\
				0 &\tx{ otherwise}
			\end{cases}
		\end{align}
	\end{example}
	
	\subsection{Euler-Lagrange Equations in $\R^n$}
	\paragraph{Setup}
	\begin{align}
		F[u(\cdot)] &= \int_a^b L(x, u(x), u'(x))\ dx \\
		u &: [a, b] \to \R^n \\
		L(x, z, p) &: [a, b] \times \R^n \times \R^n \to \R \\
		\mc{A} &:= \left\{
			u: [a, b] \to \R^n: u \in C^1, u(a) = \textbf{A}, u(b) = \textbf{B}
		\right\}
	\end{align}

	\begin{theorem}[Euler-Lagrange Equations in Vector Forms]
		\begin{align}
			- \frac{d}{dx} \nabla_p L(x, z, p) + \nabla_z L(x, z, p) &= \textbf{0} \in \R^n \quad (\dagger)
		\end{align}
	\end{theorem}
	
	\begin{example}[Classical Lagrangian Mechanics]
		\begin{align}
			V(x): \R^n \to \R &\tx{ potential energy} \\
			\frac{1}{2} m \norm{v}_2^2 &\tx{ kinetic energy} \\
			L(t, x, v):= \frac{1}{2} m \norm{v}_2^2 - V(x) &\tx{ difference between KE and PE}
		\end{align}
		Consider a path $x(t)$ in $\R^n$, define objective function as
		\begin{align}
			F[x(\cdot)] &= \int_a^b L(t, x(t), x'(t))\ dt \\
			&= \int_a^b \frac{1}{2} m \norm{\dot{x}(t)}_2^2 - V(x(t))\ dt
		\end{align}
		The Euler-Lagrange equation in vector form implies
		\begin{align}
			- \frac{d}{dt} \nabla_{(3)} L(t, x(t), \dot{x}(t))
			+ \nabla_{(2)} L(t, x(t), \dot{x}(t)) &= 0 \\
			\implies - \frac{d}{dt} m \dot{x}(t) - \nabla V(x(t)) &= 0 \\
			\implies m \ddot{x}(t) &= \nabla V(x(t))\quad (\dagger \dagger)
		\end{align}
	\end{example}
	
	\begin{remark}
		($\dagger \dagger$) is often referred to as \emph{Newtown's second law}: object moves along the path on which the total conversion between kinetic and potential energies is minimized.
	\end{remark}
	
	\begin{example}[3-Dimensional Pendulum]
		Suppose the pendulum is moving on a path such that the total conversion between kinetic and potential energies is minimized, that is
		\begin{align}
			\min \int_a^b L(t)\ dt &= \int_a^b \frac{1}{2} m (\dot{x}(t)^2 + \dot{y}(t)^2 + \dot{z}(t)^2) - m g z(t)\ dt
		\end{align}
		with the restriction that $\norm{\textbf{x}(t)} = \ell$, where $\ell$ is the radius of the sphere. \\
		The restriction can be embodied by framing the problem using spherical coordinates:
		\begin{align}
			x &:= \ell \cos \varphi \sin \theta \\
			y &:= \ell \sin \varphi \sin \theta \\
			z &:= - \ell \cos \theta
		\end{align}
		where the path of motion can be characterized using $(\theta(t), \varphi(t))$. \\
		The objective function is therefore
		\begin{align}
			L\left( t, \begin{pmatrix} \theta(t) \\ \varphi(t)	\end{pmatrix}, \begin{pmatrix} \dot{\theta}(t) \\ \dot{\varphi}(t) \end{pmatrix} \right) &= \frac{1}{2} m \ell^2 (\dot{\theta}^2 + \dot{\varphi}^2 \sin^2 (\theta)) + mg\ell \cos \theta
		\end{align}
		So the Euler-Lagrange equation can be written as
		\begin{align}
			- \frac{d}{dt} \nabla_{(3)} L \left( t, \begin{pmatrix} \theta(t) \\ \varphi(t)	\end{pmatrix}, \begin{pmatrix} \dot{\theta}(t) \\ \dot{\varphi}(t) \end{pmatrix} \right)
			+ \nabla_{(2)} L\left( t, \begin{pmatrix} \theta(t) \\ \varphi(t)	\end{pmatrix}, \begin{pmatrix} \dot{\theta}(t) \\ \dot{\varphi}(t) \end{pmatrix} \right)
			= \textbf{0}
		\end{align}
	\end{example}
	
	\subsection{Equality Constraints: Isoperimetric Case}
	\paragraph{Recall: Finite Dimensional Case} Given $f, g: \R^n \to \R$, consider
	\begin{align}
		\begin{cases}
			\min_{x \in \R^n} f(x) \\
			g(x) = \tx{contant}
		\end{cases}
	\end{align}
	Suppose $x_*$ is a regular point ($\nabla g(x_*) \neq 0$) is a minimizer, then there exists $\lambda \in \R$ such that
	\begin{align}
		\nabla f(x_*) + \lambda \nabla g(x_*) = 0 \quad \tx{ (Lagrange Multiplier)}
	\end{align}
	\begin{remark}
		$x_*$ is a critical point minimizing $f + \lambda g$ as well. Effectively, the method of Lagrange multiplier converts the constrained optimization to an unconstrained optimization problem.
	\end{remark}
	
	\paragraph{Infinite Dimensional Case} Consider the objective and constraint functions
	\begin{align}
		F[u(\cdot)] &= \int_a^b L^F(x, u(x), u'(x))\ dx \\
		G[u(\cdot)] &= \int_a^b L^G(x, u(x), u'(x))\ dx
	\end{align}
	and the optimization problem
	\begin{align}
		\begin{cases}
			\min_{u(\cdot) \in \mc{A}} F[u(\cdot)] \\
			G[u(\cdot)] = \tx{constant}
		\end{cases}
	\end{align}

	\begin{theorem}
		Suppose $u_*(\cdot)$ is a regular point, that is, the variational derivative $\frac{\delta G}{\delta u}(u_*) \neq 0$.\\
		Further, if $u_*(\cdot)$ is a minimizer of above constrained optimization problem, then $\exists \lambda \in \R$ such that:
		\begin{align}
			\red{\frac{\delta F}{\delta u}[u_*] + \lambda \frac{\delta G}{\delta u}[u_*] \equiv 0}
		\end{align}
	\end{theorem}

	\begin{example}
		\begin{align}
			\mc{A} &:= \{
			u: [-a, a] \to \R, u \in C^1, u(-a) = u(a) = 0
			\} \\
			L^G(x, z, p) &= \sqrt{1 + p^2} \\
			G[u(\cdot)] &= \int_a^b \sqrt{1 + u'(x)^2}\ dx = \ell > 0 \\
			L^F(x, z, p) &= z \\
			L^{-F}(x, z, p) &= -z \\
			F[u(\cdot)] &= \int_a^b u(x)\ dx \\
			&\begin{cases}
				\min_{u(\cdot) \in \mc{A}} (-F)[u(\cdot)] \\
				G[u(\cdot)] = \ell
			\end{cases}
		\end{align}
		Let $u_*(\cdot)$ be a minimizer, then Euler-Lagrange equation suggests 
		\begin{align}
			\frac{\delta (-F)}{\delta u} &= - \frac{d}{dx} L_p^{-F} + L_z^{-F} \equiv 0 \\
			\frac{\delta G}{\delta u} &= - \frac{d}{dx} L_p^G + L_z^G \equiv 0
		\end{align}
		By the necessary condition of minimization,
		\begin{align}
			\frac{\delta (-F)}{\delta u} + \lambda \frac{\delta G}{\delta u} &\equiv 0 \\
			\implies -1 + \lambda \left(
			- \frac{d}{dx} \frac{p}{\sqrt{1+p^2}} + 0 \right) &\equiv 0 \\
			\implies \lambda \frac{d}{dx} \frac{p}{\sqrt{1+p^2}} &\equiv -1 \\
			\implies \lambda \frac{p}{\sqrt{1+p^2}} &\equiv -x + C_1 \\
			\implies \lambda_*^2 \frac{u'_*(x)^2}{1 + u'_*(x)^2} &\equiv (C_1 - x)^2\quad  (\dagger)
		\end{align}
		\textbf{Claim}: Note that any solution $u_*(\cdot)$ to $(\dagger)$ satisfies
		\begin{align}
			(x-C_1)^2 + (u_*(x) - C_2)^2 &= \lambda^2
		\end{align}
		\emph{Check}: 
		\begin{align}
			\frac{d}{dx} \left[ 2(x - C_1) + 2(u_*(x) - C_2)u_*'(x) \right] &= 0
		\end{align}
		which implies
		\begin{align}
			u'_*(x) &= - \frac{x - C_1}{u_*(x) - C_2} \\
			\implies u'_*(x)^2 &= \frac{(x - C_1)^2}{(u_*(x) - c_2)^2}\quad (\S)
		\end{align}
		Also, 
		\begin{align}
			(u_*'(x)^2)(u_*(x) - C_2)^2  &= (x - C_1)^2 + (u_*(x) - C_2)^2 = \lambda^2 \\
			\implies (u_*(x) - C_2)^2 &= \frac{\lambda^2}{1 + u'_*(x)^2}\quad (\S \S)
		\end{align}
		Combine $(\S)$ and $(\S \S)$,
		\begin{align}
			\frac{\lambda^2}{1 + u_*'(x)^2} u_*'(x)^2 &= (x - C_1)^2
		\end{align}
	\end{example}
	
	\subsection{Equality Constraints: Holonomic Case}
	\paragraph{Setup (3-Dim Special Case)} Minimize
	\begin{align}
		F[x(\cdot), y(\cdot), z(\cdot)]
		&= \int_a^b L(t, x(t), y(t), z(t), \dot{x}(t), \dot{y}(t), \dot{z}(t))\ dt
	\end{align}
	with constraint
	\begin{align}
		H(x(t), y(t), z(t)) \equiv 0
	\end{align}
	
	\begin{theorem}[Euler-Lagrange Equations]
		Let $\vex_*(t) := \begin{pmatrix}x_*(t)\\y_*(t)\\z_*(t)\end{pmatrix}$ be the minimizer subject to constraint, then
		\begin{align}
			\begin{pmatrix}
				\frac{\delta F}{\delta x}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
				\frac{\delta F}{\delta y}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
				\frac{\delta F}{\delta z}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t)
			\end{pmatrix}
			+ \red{\lambda(t)}
			\begin{pmatrix}
				H_x[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
				H_y[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
				H_z[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t)
			\end{pmatrix} = 0\quad \forall t \in \R
		\end{align}
		where $\lambda(t)$ is a function here.
	\end{theorem}
	
	\begin{example}[3-Dim Pendulum]
		\begin{align}
			\min_{\vex(\cdot) \in \mc{A}} F[x, y, z] &= \int_a^b \frac{1}{2} m (\dot{x}(t)^2 + \dot{y}(t)^2 + \dot{z}(t)^2) - mgz(t)\ dt \\
			s.t.&\ H = \frac{1}{2} \left(x^2 + y^2 + z^2 - \ell^2 \right) = 0
		\end{align}
		In this case,
		\begin{align}
			&L(t, z_1, z_2, z_3, p_1, p_2, p_3) = \frac{1}{2} m (p_1^2 + p_2^2 + p_3^2) - mgz_3 \\
			&\begin{cases}
				L_{p_1} = mp_1 \quad
				L_{p_2} &= mp_2 \quad
				L_{p_3} = mp_3 \\
				L_{z_1} = 0 \quad \quad \ 
				L_{z_2} &= 0 \quad \quad \ 
				L_{z_3} = - mg
			\end{cases}
		\end{align}
		Because
		\begin{align}
			\frac{\delta F}{\delta x_i(t)} = - \frac{d}{dt} L_{p_i} + L_{z_i}
		\end{align}
		The Euler-Lagrange equation in this case is
		\begin{align}
			\begin{pmatrix}
				- m \ddot{x}(t) \\
				- m \ddot{y}(t) \\
				- m \ddot{z}(t) - mg
			\end{pmatrix}
			+ \lambda(t)
			\begin{pmatrix}
				x(t) \\
				y(t) \\
				z(t)
			\end{pmatrix} \equiv 0\quad (\dagger \dagger)
		\end{align}
		Combing ($\dagger \dagger$) and the constraint
		\begin{align}
			\begin{cases}
				m \ddot{x}(t) = x(t) \\
				m \ddot{y}(t) = y(t) \\
				m \ddot{z}(t) = z(t) \\
				x^2 + y^2 + z^2 = \ell^2
			\end{cases}
		\end{align}
		Solving above system of equations provides the minimizer function to the proposed problem.
	\end{example}
	
	\subsection{Geodesics on the Cylinder}
	\paragraph{Problem} Find the string connecting two points on a cylinder of radius $r$ such that the arc-length is minimized:
	\begin{align}
		\min\ F[x(t), y(t), z(t)] &:= \int_a^b \sqrt{\dot{x}(t)^2 + \dot{y}(t)^2 + \dot{z}(t)^2}\ dt \\
		s.t.\ H &:= x^2 + y^2 - r^2 = 0
	\end{align}
	The Euler-Lagrange equations in this case are
	\begin{align}
		\begin{cases}
			- \frac{d}{dt}L_{\dot{x}} + L_x &= - 2 \lambda(t) x(t) \\
			- \frac{d}{dt}L_{\dot{y}} + L_y &= - 2 \lambda(t) y(t) \\
			- \frac{d}{dt}L_{\dot{z}} + L_z &= - 2 \lambda(t) z(t)
		\end{cases} \\
		\tx{where } L := \sqrt{p_1^2 + p_2^2 + p_3^2}
	\end{align}
	The Euler-Lagrange equations can be reduced to
	\begin{align}
		\frac{d}{dt}
		\begin{pmatrix}
			x' / \sqrt{x'^2 + y'^2 + z'^2} \\
			y' / \sqrt{x'^2 + y'^2 + z'^2} \\
			z' / \sqrt{x'^2 + y'^2 + z'^2}
		\end{pmatrix} &= \lambda(t)
		\begin{pmatrix}
			2 x \\ 2y \\ 0
		\end{pmatrix}  \quad (\dagger)
	\end{align}
	
	\begin{definition}
		A curve is called \textbf{geodesic} if it satisfies the Euler-Lagrange necessary condition ($\dagger$).
	\end{definition}
	
	\begin{definition}
		Given two points on the cylinder, being geodesic is not a sufficient condition to show one curve is actually the distance minimizing curve connecting these two points.
	\end{definition}
	
	\begin{example}
		Spirals are geodesics.
	\end{example}
	
	\begin{proof}
		A spiral can be parametrized using time $t$ as
		\begin{align}
			\begin{cases}
				x(t) &= r \cos (t) \\
				y(t) &= r \sin (t) \\
				z(t) &= r \alpha
			\end{cases} \implies
			\begin{cases}
				x'(t) &= - r \sin (t) \\
				y'(t) &= r \cos (t) \\
				z'(t) &= \alpha
			\end{cases}
		\end{align}
		Therefore,
		\begin{align}
			\sqrt{x'^2 + y'^2 + z'^2} = \sqrt{r^2 + \alpha^2} = \tx{constant}
		\end{align}
		The Euler-Lagrange equations become 
		\begin{align}
			\frac{1}{\tx{constant}}\frac{d}{dt}
			\begin{pmatrix}
				-r \sin(t) \\
				r \cos(t) \\
				\alpha
			\end{pmatrix} &= 2 \lambda(t) \begin{pmatrix}
				r \cos (t) \\
				r \sin (t) \\
				0
			\end{pmatrix} \\
			\implies
			\frac{1}{\tx{constant}}
			\begin{pmatrix}
				- r \cos(t) \\
				- r \sin(t) \\
				\alpha
			\end{pmatrix} &= 2 \lambda(t) \begin{pmatrix}
				r \cos (t) \\
				r \sin (t) \\
				0
			\end{pmatrix} \\
			\implies
			\lambda(t) &= - \frac{1}{2\tx{ constant}} \\
			&= - \frac{1}{2 \sqrt{r^2 + \alpha^2}}
		\end{align}
	\end{proof}
	
	\begin{example}
		Straight line is also geodesic.
	\end{example}
	
	\begin{proof}
		Consider the straight line parameterized as 
		\begin{align}
			\begin{cases}
				x(t) &= \alpha \\
				y(t) &= \beta \\
				z(t) &= \gamma t
			\end{cases} \implies 
			\begin{cases}
				x'(t) &= 0 \\
				y'(t) &= 0 \\
				z'(t) &= \gamma
			\end{cases}
		\end{align}
		In this case, $\sqrt{x'^2 + y'^2 + z'^2} = \gamma$. \\
		The Euler-Lagrange equations are
		\begin{align}
			\frac{d}{dt} \frac{1}{\gamma} \begin{pmatrix}
				0 \\ 0 \\ \gamma
			\end{pmatrix} &= 2 \lambda(t) \begin{pmatrix}
				\alpha \\ \beta \\ 0
			\end{pmatrix} \\
			\implies \lambda(t) &= 0
		\end{align}
	\end{proof}
	
	\begin{example}[Another Example of Isoperimetric Constraints]
		Consider
		\begin{align}
			F[u(\cdot)] &= \int_0^a u(x) \sqrt{1 + u'(x)^2}\ dx \\
			L^F &= z \sqrt{1 + p^2} \\
			u \in \mc{A} &:= \{u: [0, a] \to \R: u \in C^1, u(x) \geq 0, u(0) = b, u(a) = 0 \} \\
			G[u(\cdot)] &= \int_0^a u(X)\ dx = \frac{ab}{2} \\
			L^G &= z - \frac{b}{2}
		\end{align}
		\textbf{Part 1:} Derive the Euler-Lagrange equations.
		\begin{align}
			\frac{\delta F}{\delta u} &= - \frac{d}{dx} \frac{z p}{\sqrt{1+p^2}} + \sqrt{1 + p^2} \\
			\frac{\delta G}{\delta u} &= 1
		\end{align}
		The Euler-Lagrange equations are
		\begin{align}
			\frac{d}{dx} \frac{z p}{\sqrt{1+p^2}} - \sqrt{1 + p^2} &= \lambda
		\end{align}
		\textbf{Part 2:} Show that the following function satisfies the necessary condition.
		\begin{align}
			u(x) &= b \left(1 - \frac{x}{a}\right) \\
			u'(x) &= - \frac{b}{a}
		\end{align}
		Therefore,
		\begin{align}
			\frac{d}{dx} \frac{z p}{\sqrt{1+p^2}} - \sqrt{1 + p^2} &= \frac{p^2}{\sqrt{1+p^2}} - \sqrt{1 + p^2} \\
			&= \frac{\frac{b^2}{a^2}}{\sqrt{1 + \frac{b^2}{a^2}}} - \sqrt{1 + \frac{b^2}{a^2}} \\
			&= \frac{\frac{b^2}{a}}{\sqrt{a^2 + b^2}} - \frac{1}{a} \sqrt{a^2 + b^2} \\\
			&= \frac{b^2}{a \sqrt{a^2 + b^2}} - \frac{a^2 + b^2}{a \sqrt{a^2 + b^2}} \\
			&= \frac{-a}{\sqrt{a^2 + b^2}} = \lambda
		\end{align}
		Obviously, $u(\cdot) \in C^1$ and $u(0) = b, u(a) = 0$. \\
		Also, 
		\begin{align}
			G[u(\cdot)] &= \int_0^a b(1 - \frac{x}{a})\ dx \\ 
			&= \int_0^a b - \frac{bx}{a}\ dx \\
			&= \left. bx - \frac{bx^2}{2a} \right|_0^a \\
			&= ab - \frac{ba^2}{2a} \\
			&= \frac{ab}{2}
		\end{align}
	\end{example}
	
	\begin{example}[The Brachistochrone Problem]
		Parametrize the problem using $c: [0, T] \to \R^2$ to describe the curve such that $c(0) = A = (0, a)$ and $c(T) = B = (b, 0)$. \\
		In particular, for each path of motion $x(t)$, the parametric representation can be constructed as
		\begin{align}
			c(t) &= (x(t), y(t)) = (x(t), u(x(t))) \\
			\implies v(t) &= \frac{d}{dt}c(t) = \dot{x}(t) \begin{pmatrix}
				1 \\ \dot{u}(x(t))
			\end{pmatrix}
		\end{align} 
		The kinetic energy at time $t$ is 
		\begin{align}
			T(t) &= \frac{1}{2} m \dot{x}(t)^2 [1 + \dot{u}(x(t))^2]
		\end{align}
		The potential energy at time $t$ is
		\begin{align}
			V(t) &= m g u(x(t))
		\end{align}
		By the conservation of energy, the total energy $E(t) := T(t) + V(t)$ is a constant function. \\
		Re-write the total energy function
		\begin{align}
			E(t) &= \frac{1}{2} m \dot{x}(t)^2 [1 + \dot{u}(x(t))^2] + m g u(x(t)) = m g a\ \forall t \in [0, T] \\
			\implies \dot{x}(t) &= \sqrt{\frac{2g(a - u(x(t)))}{1 + \dot{u}(x(t))^2}}
		\end{align}
		The total time for the point mass to travel from $A$ to $B$ is $T$:
		\begin{align}
			T &= \int_0^T 1\ dt = \int_0^T \frac{1}{\dot{x}(t)} \dot{x}(t)\ dt \\
			&= \int_0^T \sqrt{\frac{1 + \dot{u}(x(t))^2}{2g(a - u(x(t)))}} \dot{x}(t)\ dt \\
			&= \int_{x(t_0)}^{x(t_1)} \sqrt{\frac{1 + \dot{u}(x(t))^2}{2g(a - u(x(t)))}}\ dx \\
			&= \int_0^b \sqrt{\frac{1 + \dot{u}(x(t))^2}{2g(a - u(x(t)))}}\ dx
		\end{align}
		The minimization problem becomes 
		\begin{align}
			&\min F[u(\cdot)] = \int_0^b \sqrt{\frac{1 + \dot{u}(x(t))^2}{2g(a - u(x(t)))}}\ dx \\
			s.t.\ u(\cdot) \in \mc{A} &= \{u:[0, b] \to \R: u \in C^1, u(0) = a, u(b) = 0 \}
		\end{align}
		The Euler-Lagrange equation is
		\todo{verify this point}
		\begin{align}
			(1 + u'(x)^2)(a - u(x))c^2 = 1\ \tx{where $c=$ constant}
		\end{align}
		\textbf{Claim:} the solution to above differentiable equation takes the form
		\begin{align}
			u(x(t)) = a - k(1 - \cos t)
		\end{align}
		To verify the validation of the proposed solution
		\begin{align}
			\frac{d}{dt} u'(x(t)) x'(t) &= - k \sin t \\
			\implies u'(x(t)) &= -\frac{k \sin t}{x'(t)} \\
			\implies c^2 \left(1 + \frac{k^2 \sin^2 t}{x'(t)^2} \right) k (1 - \cos t) &= 1
		\end{align}
		In particular, choose $c^2 k = \frac{1}{2}$:
		\begin{align}
			\left(1 + \frac{k^2 \sin^2 t}{x'(t)^2} \right) (1 - \cos t) &= 2 \\
			\implies k^2 \sin^2 t (1 - \cos t) &= x'(t)^2 (1 + \cos t) \\
			\implies k^2 \sin^2 t (1 - \cos t)^2 &= x'(t)^2 (1 + \cos t) (1 - \cos t) \\
			&= x'(t)^2(1 - \cos^2 t) = x'(t) \sin^2 t \\
			\implies k^2(1 - \cos t)^2 &= x'(t)^2 \\
			\implies k (1 - \cos t) &= x'(t) \\
			\implies k t - k \sin t &= x(t) + C \\
			\implies c(t) &= \begin{pmatrix}
				x(t) \\ y(t)
			\end{pmatrix} = \begin{pmatrix}
				x(t) \\ u(x(t))
			\end{pmatrix} \\
			&= \begin{pmatrix}
				kt - k\sin t \\
				a - k(1 - \cos t)
			\end{pmatrix}
		\end{align}
		With initial conditions:
		\begin{align}
			c(0) &= (0, a) = A \\
			c(T) &= \begin{pmatrix}
				k(T - \sin T) \\
				a - k(1 - \cos T)
			\end{pmatrix} = \begin{pmatrix}
				b \\ 0
			\end{pmatrix}
		\end{align}
	\end{example}
	
\section{Appendix I: Some Basic ODEs}
	\begin{align}
		u'(x) = 0 &\implies u(x) = c \\
		u'(x) = u(x) &\implies u(x) = A e^x \\
		u''(x) = 0 &\implies u(x) = Ax + B \\
		u''(x) = u(x) &\implies u(x) = A e^x + B e^{-x} \\
		u''(x) = -u(x) &\implies u(x) = A \sin x + B \cos x
	\end{align}


\section{Appendix II: List of Theorems}
\subsection{Finite Dimensional: Unconstrained}
    \begin{theorem}[Necessary Condition for Local Minimum]
        Let $f \in C^1(\Omega, \R)$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every \emph{feasible direction} $v$ at $x_0$,
        \begin{align}
            \nabla f(x_0) \cdot v \geq 0
        \end{align}
        \emph{This theorem serves as the primary defining property of local minimum.}
    \end{theorem}

    \begin{theorem}[Second Order Necessary Condition for Local Minimum]
        Let $f: \in C^2(\Omega, \R)$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every non-zero feasible direction $v$ at $x_0$,
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) \cdot v \geq 0$;
            \item $\nabla f(x_0) \cdot v = 0 \implies v^T \nabla^2 f(x_0) v \geq 0$.
        \end{enumerate}
    \end{theorem}

    \begin{theorem}[Second Order Sufficient Condition for Interior Local Minima]
        Let $f: C^2(\Omega, \R)$, for some $x_0 \in \Omega$, if
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) = 0$,
            \item \emph{(and)} $\nabla^2 f(x_0) \succ 0$.
        \end{enumerate}
        then $x_0$ is a \ul{strictly local minimizer}.
    \end{theorem}

\subsection{Finite Dimensional: Equality Constraints}
    \begin{definition}
        The \textbf{tangent space} to $\mc{M}$ at $x_0$ is defined to be the set of all tangent vectors:
        \begin{align}
            T_{x_0} \mc{M} := \left \{
            v \in \R^n
            :
            v := \frac{d}{ds} \bigg \vert_{s=0} x(s) \tx{ for some } x \in C^1(V_\varepsilon(0), \mc{M})\ s.t.\ x(0) = x_0
            \right \}
        \end{align}
    \end{definition}

    \begin{notation}
        Define the $T$ space on equality constraint as
        \begin{align}
            T_{x_0} := \{x \in \R^n: \inner{x_0}{\nabla h_i(x_0)} = 0\ \forall i \in [k]\} = \{\nabla_i(x_0)\}^\perp
        \end{align}
    \end{notation}

    \begin{theorem}
        Suppose $x_0$ is a \emph{regular point} of $\mc{M} := \{h_i(x) = 0, i=1,\cdots,k\}$, then $T_{x_0} = T_{x_0} \mc{M}$.
    \end{theorem}

    \begin{theorem}[Lagrange Multipliers: First Order Necessary Condition]
        Let $f, h_1, \cdots, h_k \in C^1$ defined on open subset $\Omega \subseteq \R^n$. Let $x_0$ be a regular point of the constraint set $\mc{M} := \bigcap_{i=1}^k h^{-1}_i(0)$. Suppose $x_0$ is a local minimum of $\mc{M}$, then there exists $\lambda_1, \cdots, \lambda_k \in \R$ such that
        \begin{align}
            \nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0
        \end{align}
        \emph{Remark: if we define Lagrangian $\mc{L}(x, \lambda_i) := f(x) + \sum_{i=1}^k h_i(x)$, then the theorem says the local minimum is a critical point of $\mc{L}$.}
    \end{theorem}

    \begin{theorem}[Second Order Necessary Condition]
        Let $f, h_i \in C^2$, if $x_0$ is a local minimum on previously defined surface $\mc{M}$, then there exists Lagrangian multipliers $\{\lambda_i\}$ such that
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0$ ($\nabla_x \mc{L} = 0$);
            \item And $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) \succcurlyeq 0$ \red{on $T_{x_0} \mc{M}$} ($\nabla_x^2 \mc{L} \succcurlyeq 0$).
        \end{enumerate}
        \emph{Remark: whenever $x_0$ is a local minimum, it must be a critical point of $\mc{L}$, and $\mc{L}$ is positive semidefinite on the tangent space at $x_0$.}
    \end{theorem}

    \begin{theorem}[Second Order Sufficient Conditions]
        Let $f, h_i \in C^2$ on open $\Omega \subseteq \R^n$, and $x_0 \in \mc{M}$ is a regular point, if there exists $\lambda_i \in \R$ such that
        \begin{enumerate}[(i)]
            \item $\nabla_x \mc{L}(x_0, \lambda_i) = 0$;
            \item $\nabla_x^2 \mc{L}(x_0, \lambda_i) \succ 0$ \red{on $T_{x_0} \mc{M}$},
        \end{enumerate}
        then $x_0$ is a \emph{strict} local minimum.
    \end{theorem}

\subsection{Finite Dimensional: Inequality Constraints}
    \begin{theorem}[The First Order Necessary Condition for Local Minimum: Kuhn-Tucker Conditions]
        Let $\Omega$ be an open subset of $\R^n$ with constraints $h_i$ and $g_i$ to be $C^1$ on $\Omega$. Suppose $x_0 \in \Omega$ is a regular point with respect to constraints, further suppose $x_0$ is a local minimum, then there exists some $\lambda_i \in \R$ and $\mu_j \in \red{\R_+}$ such that 
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$ (i.e. $\nabla_x \mc{L}(x, \lambda, \mu) = 0$);
            \item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
        \end{enumerate}
        \emph{Remark 1: by complementary slackness, all $\mu_j$ corresponding to inactive inequality constraints are zero.} \\
        \emph{Remark 2: it is possible for an active constraint to have zero multiplier.}
    \end{theorem}

    \begin{theorem}[The Second Order Necessary Conditions]
        Let $\Omega$ be an open subset of $\R^n$, and $f, h_1, \cdots, h_k, g_1, \cdots, g_\ell \in C^2(\R^n, \R)$. Let $x_0$ be a regular point of the constraints ($\dag$).
        Suppose $x_0$ is a local minimum of $f$ subject to constraint ($\dag$), then there exists $\lambda_i \in \R$ and $\mu_j \geq 0$ such that
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
            \item $\mu_j g_j(x_0) = 0$;
            \item $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla^2 g_j(x_0)$ is \ul{positive semidefinite} on the tangent space to \ul{activate constraints} at $x_0$.
        \end{enumerate}
    \end{theorem}

    \begin{theorem}[The Second Order Sufficient Conditions]
        Let $\Omega$ be an open subset of $\R^n$, let $f, h_i, q_j \in C^2(\Omega)$. Consider minimizing $f(x)$ with the constraint
        \begin{align}
            (\dag ) \begin{cases}
                h_i(x) = 0\quad \forall i \\
                g_j(x) \leq 0\quad \forall j \\
                x \in \Omega
            \end{cases}
        \end{align}
        Suppose there exists a feasible $x_0$ satisfying $(\dag)$ and $\lambda_i \in \R$ and $\mu_j \in \R_{+}$ such that
        \begin{enumerate}[(i)]
            \item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
            \item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
        \end{enumerate}
        If the Hessian matrix for Lagrangian $\nabla_x^2 \mc{L}(x_0)$ is \ul{positive definite} on $\ttilde{T}_{x_0}$, the space of \textbf{strongly active} constraints at $x_0$, then $x_0$ is a \ul{strict} local minimum.
    \end{theorem}


\subsection{Iterative Algorithms}
    \begin{algorithm}[Newton's Method in $\R$]
        Given initial point $x_0 \in I$, while not terminated:
        \begin{align}
            x_{n+1} \leftarrow x_n - \frac{f'(x_n)}{f''(x_n)}
        \end{align}
    \end{algorithm}

    \begin{theorem}
        Let $f \in C^3$ on open interval $I \subseteq \R$. Suppose $x_* \in I$ satisfies $f'(x_*) = 0$ and $f''(x_*) \neq 0$, then the sequence of points $(x_n)$ generated by Newton's method converges to $x_*$ if \emph{$x_0$ is sufficiently close to $x_*$}.
    \end{theorem}

    \begin{algorithm}[Newton's Method in $\R^n$]
        Let $f: \Omega \subseteq \R^n \to \R$ where $\Omega$ is open, let initial point $x_0 \in \Omega$. \\
        Suppose $\nabla^2 f(x_n)$ is invertible for every generated $n$, and $\nabla f(x_*) = 0$ so that algorithm stops at minimum. \\
        The iterative algorithm is defined as following:
        \begin{align}
            x_{n+1} \leftarrow x_n - [\nabla^2 f(x_n)]^{-1} \nabla f(x_n)
        \end{align}
    \end{algorithm}

    \begin{algorithm}[Steepest Descent]
        Let $f: \Omega \to \R$ where $\Omega$ is an open subset of $\R^n$. Let initial point $x_0 \in \Omega$. \\
        To minimize $f$ on $\Omega$, iteratively update $x$ follows at each step $k$:
        \begin{align}
            x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)
        \end{align}
        where $\alpha_k = \argmin_{\alpha \geq 0} f(x_k - \alpha \nabla f(x_k)$. \\
        \emph{Remark: There might be multiple minimizing $\alpha$, in real world implementations, we take the least minimizer found.}
    \end{algorithm}

    \begin{theorem}
        For any initial point $x_0 \in \R^n$, gradient descent converges to the unique minimum point $x_*$ of the quadratic $f(x) = x^T Q x - b^T x$.
    \end{theorem}

    \begin{theorem}
        Given the method of conjugate, the sequence of points generated eventually reaches the global minimum. That is, $x_n = x^*$.
    \end{theorem}

    \begin{algorithm}[Method of Conjugate Directions] \quad
        \begin{enumerate}[(i)]
            \item Let $Q \in \mathbb{S}_{++}^n$ and $\{d_j\}_{j=0}^{n-1}$ be a set of non-zero $Q$-orthogonal vectors, note that they form a basis of $\R^n$. \\
            \item Given initial point $x_0 \in \R^n$, the method of conjugate direction generates a sequence of points $\{x_k\}_{k=0}^n$ as the following:
            \begin{align}
                x_{k+1} &\leftarrow x_k + \alpha_k d_k \\
                \alpha_k &:= - \frac{\inner{g_k}{d_k}}{d_k^T Q d_k} \quad g_k := \nabla f(x_k)
            \end{align}
        \end{enumerate}
    \end{algorithm}

\subsection{Infinite Dimensional Analysis: Calculus of Variation}
    \begin{lemma}[The Fundamental Lemma of Calculus of Variation]
        Suppose $g$ is continuous on interval $[a, b]$ such that
        \begin{align}
            \int_a^b g(x) v(x)\ dx = 0\ \forall \tx{ test function } v(\cdot)
        \end{align}
        Then $g(x) \equiv 0$ on $[a, b]$.
    \end{lemma}

    \begin{definition}
        Given $u(\cdot) \in \mc{A}$, suppose $\exists$ function $g(\cdot)$ on $[a, b]$ such that 
        \begin{align}
            \red{\left. \frac{d}{ds} \right |_{s=0} F[u(\cdot) + sv(\cdot)] = \int_a^b g(x) v(x)\ dx\ \forall \tx{ test functions} v(\cdot)}
        \end{align}
        Then $g(\cdot)$ is called the \textbf{variational derivative} of $F$ at $u(\cdot)$, often denoted as $\frac{\delta F}{\delta u}(u)(\cdot)$.
    \end{definition}

    \begin{theorem}[Euler-Lagrange]
        Let 
        \begin{align}
            \mc{A} &:=\{u:[a, b] \to \R:u \in C^1, u(a) = A, u(b) = B\}
        \end{align}
        Let $L \in C^2$ such that
        \begin{align}
            F[u(\cdot)] &= \int_a^b L(x, u(x), u'(x))\ dx
        \end{align}
        Then, if $u(\cdot) \in C^1$, then $\frac{\delta F}{\delta u}(u)(\cdot)$ exists and is continuous. \\
        Moreover,
        \begin{align}
            \red{\frac{\delta F}{\delta u}(u)(\cdot)(x) = - \frac{d}{dx} [L_p(x, u(x), u'(x))] + L_z(x, u(x), u'(x))}
        \end{align}
        This equation is often referred to as the \textbf{Euler-Lagrange  equation}.
    \end{theorem}

    \begin{theorem}[Euler-Lagrange Equations in Vector Forms]
        \begin{align}
            - \frac{d}{dx} \nabla_p L(x, z, p) + \nabla_z L(x, z, p) &= \textbf{0} \in \R^n \quad (\dagger)
        \end{align}
    \end{theorem}

    \begin{theorem}
        Suppose $u_*(\cdot)$ is a regular point, that is, the variational derivative $\frac{\delta G}{\delta u}(u_*) \neq 0$.\\
        Further, if $u_*(\cdot)$ is a minimizer of above constrained optimization problem, then $\exists \lambda \in \R$ such that:
        \begin{align}
            \red{\frac{\delta F}{\delta u}[u_*] + \lambda \frac{\delta G}{\delta u}[u_*] \equiv 0}
        \end{align}
    \end{theorem}

    \begin{theorem}[Euler-Lagrange Equations]
        Let $\vex_*(t) := \begin{pmatrix}x_*(t)\\y_*(t)\\z_*(t)\end{pmatrix}$ be the minimizer subject to constraint, then
        \begin{align}
            \begin{pmatrix}
                \frac{\delta F}{\delta x}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
                \frac{\delta F}{\delta y}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
                \frac{\delta F}{\delta z}[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t)
            \end{pmatrix}
            + \red{\lambda(t)}
            \begin{pmatrix}
                H_x[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
                H_y[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t) \\
                H_z[x_*(\cdot), y_*(\cdot), z_*(\cdot)](t)
            \end{pmatrix} = 0\quad \forall t \in \R
        \end{align}
        where $\lambda(t)$ is a function here.
    \end{theorem}
\end{document}
















