\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{APM462: Nonlinear Optimization}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{APM462: Nonlinear Optimization}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    
    \section{Preliminaries}
    \subsection{Mean Value Theorems and Taylor Approximations.}
    \begin{definition}
        Let $f: S \subseteq \R^n \to \R$, the \textbf{gradient} of $f$ at $x \in S$, if exists, is a vector $\nabla f(x) \in \R^n$ characterized by the property
        \begin{equation}
            \lim_{v \to 0} \frac{f(x+v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{definition}
    
    \begin{theorem}[The First Order Mean Value Theorem]
        Let $f \in C^1(\R^n, \R)$, then for any $x, v \in \R^n$, there exists some $\theta \in (0, 1)$ such that
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x + \theta v) \cdot v
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        Let $x, v \in \R^n$, define $g(t) := f(x + tv) \in C^1(\R, \R)$. \\
        By the mean value theorem on $\R^\R$, there exists $\theta \in (0, 1)$ such that $g(0+1) = g(0) + g'(\theta)(1-0)$, that is, $f(x+v) = f(x) + g'(\theta)$. Note that $g'(\theta) = \nabla f(x + \theta v) \cdot v$.
    \end{proof}
    
    \begin{proposition}[The First Order Taylor Approximation]
        Let $f \in C^1(\R^n, \R)$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + o(\norm{v})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{f(x + v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the mean value theorem, $\exists \theta \in (0, 1)$ such that $f(x+v) - f(x) = \nabla f(x + \theta v) \cdot v$.\\
    	The limit becomes $\lim_{\norm{v} \to 0} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}} = \lim_{\norm{v} \to 0; x + \theta v \to x} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}}$.\\
    	Since $f \in C^1$, $\lim_{x + \theta v \to x} \nabla f(x + \theta v) = \nabla f(x)$.\\
    	And $\frac{v}{\norm{v}}$ is a unit vector, and every component of it is bounded, as the result, the limit of inner product vanishes instead of explodes.
    \end{proof}
    
    \begin{theorem}[The Second Order Mean Value Theorem]
        Let $f: \R^n \to \R$ be a $C^2$ function, then for any $x, v \in \R^n$, there exists $\theta \in (0, 1)$ satisfying
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v^T \nabla^2 f(x + \theta v) v
        \end{equation}
    \end{theorem}
    
    \begin{proposition}[The Second Order Taylor Approximation]
        Let $f: C^2(\R^n, \R)$ function, and $x, v \in \R^n$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v^T \nabla^2 f(x + \theta v) v + o(\red{\norm{v}^2})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{
                f(x + v) - f(x) - \nabla f(x) \cdot v - \frac{1}{2} v^T \nabla^2 f(x) v
            }{\norm{v}^2} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the second mean value theorem, there exists $\theta \in (0, 1)$ such that the limit is equivalent to
    	\begin{align}
    		\lim_{\norm{v} \to 0} \frac{1}{2} \left(\frac{v}{\norm{v}}\right)^T \left[\nabla^2 f(x + \theta v) - \nabla^2 f(x)\right] \frac{v}{\norm{v}}
    	\end{align}
    	Since $f \in C^2$, the limit of $\left[H_f(x + \theta v) - H_f(x)\right]$ is in fact $\textbf{0}_{n \times n}$. And every component of unit vector $\frac{v}{\norm{v}}$ is bounded, the quadratic form converges to zero as an immediate result.
    \end{proof}
    
    It is often noted that the gradient at a particular $x_0 \in dom(f) \subseteq \R^n$ gives the direction $f$ increases most rapidly.
        Let $x_0 \in dom(f)$, and $v$ be a \ul{unit vector} representing a \emph{feasible direction} of change. That is, there exists $\delta > 0$ such that $x_0 + t v \in dom(f)$ $\forall t \in [0, \delta)$. Then the rate of change of $f$ along feasible direction $v$ can be written as
        \begin{equation}
            \left.\frac{d}{dt}\right\vert_{t=0} f(x_0 + tv) = \nabla f(x_0) \cdot v = \norm{\nabla f(x_0)}\ \norm{v} \cos(\theta)
        \end{equation}
        where $\theta = \angle (v, \nabla f(x_0)$. And the derivative is maximized when $\theta=0$, that is, when $v$ and $\nabla f$ point the same direction.
    
    \subsection{Implicit Function Theorem}
    \begin{theorem}[Implicit Function Theorem]
        Let $f: C^1(\R^{n+1}, \R)$, let $(a, b) \in \R^n \times \R$ such that $f(a, b) = 0$. If $\nabla f(a, b) \neq 0$, then $\{(x, y) \in \R^n \times \R:f(x, y) = 0\}$ is locally a graph of a function $g: \R^n \to \R$.
    \end{theorem}
    
    \begin{remark}
        $\nabla f(x_0) \perp \tx{ level set of $f$ near $x_0$}$.
    \end{remark}
    
    \section{Convexity}
    \subsection{Terminologies}
    \begin{definition}
        Set $\Omega \subseteq \R^n$ is \textbf{convex} if and only if 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ \lambda x_1 + (1 - \lambda) x_2 \in \Omega
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subseteq \R^n \to \R$ is \textbf{convex} if and only if $\Omega$ is convex, and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subseteq \R^n \to \R$ is \textbf{strictly convex} if and only if $\Omega$ is convex and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in (0, 1),\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) < \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \subsection{Basic Properties of Convex Functions}
    
    \begin{definition}
        A function $f: \Omega \to \R$ is \textbf{concave} if and only if $-f$ is \textbf{convex}.
    \end{definition}
    
    \begin{proposition} Properties of convex functions:
        \begin{enumerate}[(i)]
            \item If $f_1, f_2$ are convex on $\Omega$, so is $f_1 + f_2$;
            \item If $f$ is convex on $\Omega$, then for any $a > 0$, $af$ is also convex on $\Omega$;
            \item Any \textbf{sub-level/lower contour set} of a convex function $f$ 
            \begin{align}
            	\mc{L}(c) := \{x \in \R^n: f(x) \leq c\}
            \end{align}
            is convex.
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}[Proof of (iii).]
    	Let $c \in \R$, and $x_1 ,x_2 \in SL(c)$. Let $s \in [0, 1]$. Since $x_1, x_2 \in \mc{L}(c)$, and $f(\cdot)$ is convex, $f(s x_1 + (1-s) x_2) \leq s f(x_1) + (1-s) f(x_2) \leq s c + (1-s) c = c$. Which implies $s x_1 + (1-s) x_2 \in \mc{L}(c)$.
    \end{proof}
    
    \begin{example}
    	$\ell_2$ norm $f(x): \R^n \to \R := \norm{x}_2$ is convex.
    \end{example}
    
    \begin{proof}
    	Note that for any $u, v \in \R^n$, by triangle inequality, $\norm{u - (-v)} \leq \norm{u - 0} + \norm{0 - (-v)} = \norm{u} + \norm{v}$. Consequently, let $u, v \in \R^n$ and $s \in [0, 1]$, then $\norm{s u + (1-s) v} \leq \norm{su} + \norm{(1-s) v} = s \norm{u} + (1-s) \norm{v}$. Therefore, $\norm{\cdot}$ is convex.
    \end{proof}
    
    \begin{proposition}
    	Any norm function $\norm{\cdot}$ defined on a vector space $\mc{X}(\R)$ is convex.
    \end{proposition}
    
    \begin{proof}
    	The proof follows the defining properties of norm,
    	\begin{align}
    		\norm{\lambda x + (1 - \lambda) y} &\leq \norm{\lambda x}  + \norm{(1 - \lambda) y} \\
    		&= \lambda \norm{x} + (1 - \lambda) \norm{y}
    	\end{align}
    \end{proof}
	
	\subsection{Characteristics of $C^1$ Convex Functions}
	
    \begin{theorem}[$C^1$ criterions for convexity]
        Let $f \in C^1$, then $f$ is convex on a convex set $\Omega$ \ul{if and only if}
        \begin{equation}
            \forall x, y \in \Omega,\ f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{equation}
        that is, \emph{the linear approximation is never an overestimation of value of $f$}.
    \end{theorem}
    \begin{proof}
        ($\implies$) Suppose $f$ is convex on a convex set $\Omega$. Then $f(sy + (1-s) x) \leq sf(y) + (1-s)f(x)$ for every $x, y \in \Omega$ and $s \in [0, 1]$, which implies, for every $s \in (0, 1]$:
        \begin{equation}
            \frac{f(sy + (1-s) x) - f(x)}{s} \leq f(y) - f(x)
        \end{equation}
        By taking the limit of $s \to 0$,
        \begin{align}
            \lim_{s \to 0} \frac{f(x + s(y-x)) - f(x)}{s} &\leq f(y) - f(x) \\
            \implies \left.\frac{d}{ds}\right\vert_{s=0} f(x + s(y-x)) &\leq f(y) - f(x) \\
            \implies \nabla f(x) \cdot (y-x) &\leq f(y) - f(x)
        \end{align}
        ($\impliedby$) Let $x_0, x_1 \in \Omega$, let $s \in [0, 1]$. Define $x^* := s x_0 + (1-s) x_1$, then 
        \begin{align}
        	f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot (x_0 - x^*) \\
        	\implies f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot [(1-s)(x_0 - x_1)]
        \end{align}
        Similarly,
        \begin{align}
        	f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot (x_1 - x^*) \\
        	\implies f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot [s(x_1 - x_0)]
        \end{align}
        Therefore, $sf(x_0) + (1-s)f(x_1) \geq f(x^*)$.
    \end{proof}
    
    \begin{theorem}[$C^2$ criterion for convexity]
        $f \in C^2$ is a convex function on a convex set $\Omega \subseteq \R^n$ \ul{if and only if} $\nabla^2 f(x) \succcurlyeq 0$ (i.e. positive semidefinite) for all $x \in \Omega$.
    \end{theorem}
    
    \begin{corollary}
    	When $f$ is defined on $\R$, the $C^2$ criterion becomes $f''(x) \geq 0$.
    \end{corollary}
    
    \begin{proof}
        ($\impliedby$) Suppose $\nabla^2 f(x) \succcurlyeq 0$ for every $x \in \Omega$, let $x, y \in \Omega$. By the second order MVT,
        \begin{align}
        	f(y) &= f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y-x)^T \nabla^2 f(x + s (y - x)) (y - x)\tx{ for some } s \in [0, 1] \\
        	&\implies f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{align}
        So $f$ is convex by the $C^1$ criterion of convexity.\\
        ($\implies$) Let $v \in \R^n$. Suppose, for contradiction, that for some $x \in \Omega$, $\nabla^2 f(x) \centernot \succcurlyeq 0$.\\
        If such $x \in \partial \Omega$, note that $v^T \nabla^2 f(\cdot) v$ is continuous because $f \in C^2$, then there exists $\varepsilon > 0$ such that $\forall x' \in V_\varepsilon(x) \cap \Omega^{int},\ v^T \nabla^2 f(x') v < 0$.\\
        Hence, one may assume with loss of generality that such $x \in \Omega^{int}$.\\
        Because $x \in \Omega^{int}$, exists $\varepsilon' > 0$, such that $V_{\varepsilon'}(x) \subseteq \Omega^{int}$.\\
        Define $\hat{v} := \frac{v}{\sqrt{\varepsilon'}}$, then for every $s \in [0, 1]$, $\hat{v}^T \nabla^2 f(x + s\hat{v}) \hat{v} < 0$.\\
        Let $y = x + \hat{v}$, by the mean value theorem, 
        \begin{align}
        	f(y) = f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f [x + s (y - x)] (y - x)
        \end{align}
	 	for some $s \in [0, 1]$.\\
        This implies $f(y) < f(x) + \nabla f(x) \cdot (y - x)$, which contradicts the $C^1$ criterion for convexity.
    \end{proof}
    
    \subsection{Minimum and Maximum of Convex Functions}
    \begin{theorem}
        Let $\Omega \subseteq \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Let
        \begin{align}
        	\Gamma := \left\{x \in \Omega: f(x) = \min_{x \in \Omega} f(x) \right\} \equiv \argmin_{x \in \Omega} f(x)
        \end{align}
        If $\Gamma \neq \varnothing$, then 
        \begin{enumerate}[(i)]
        	\item $\Gamma$ is convex;
        	\item any local minimum of $f$ is the global minimum.
        \end{enumerate}
    \end{theorem}
    
    \begin{proof}[Proof (i).]
    	Let $x, y \in \Gamma$, $s \in [0, 1]$, then $sx+(1-s)y \in \Omega$ because $\Omega$ is convex. Since $f$ is convex, $f(sx+(1-s)y) \leq sf(x) + (1-s)f(y) = \min_{x \in \Omega} f(x)$. The inequality must be equality since it would contradicts the fact that $x, y \in \Gamma$. Therefore, $sx+(1-s)y \in \Gamma$.
    \end{proof}
    
    \begin{proof}[Proof (ii).]
    	Let $x \in \Omega$ be a local minimizer for $f$, but assume, for contradiction, it is not a global minimizer. That is, there exists some other $y$ such that $f(y) < f(x)$. Since $f$ is convex, \begin{align}
 				f(x + t(y-x)) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y) < f(x)
			\end{align}
			for every $t \in (0, 1]$. Therefore, for every $\varepsilon > 0$, there exists $t^* \in (0, 1]$ such that $x + t^*(y-x) \in V_\varepsilon(x)$ and $f(x + t^*(y-x)) < f(x)$, this contradicts the fact that $x$ is a local minimum.
    \end{proof}
    
    \begin{theorem}
        Let $\Omega \subseteq \R^n$ be a \ul{convex and compact} set, and $f: \Omega \to \R$ is a \ul{convex} function. Then 
        \begin{equation}
            \max_{x \in \Omega} f(x) = \max_{x \in \partial \Omega} f(x)
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        As we assumed, $\Omega$ is closed, therefore $\partial \Omega \subseteq \Omega$. Hence, $\max_{x \in \Omega} f \geq \max_{x \in \partial \Omega} f$.\\
        Suppose, for contradiction, $\max_{x \in \Omega} f > \max_{x \in \partial \Omega} f$, then $x^* := \argmax_{x \in \Omega} f \in \Omega^{int}$. \\
        Then we can construct a straight line through $x^*$ and intersects $\partial \Omega$ at two points, $y_1, y_2 \in \partial \Omega$, such that $x^* = s y_1 + (1-s) y_2$ for some $s \in (0, 1)$. Further, since $f$ is convex, $\max_{x \in \Omega}f(x) = f(x^*) \leq s f(y_1) + (1-s) f(y_2) \leq s \max_{\partial \Omega} f + (1-s) \max_{\partial \Omega} f = \max_{\partial \Omega} f$, which leads to a contradiction.\\
        Therefore, $\max_{x \in \Omega} f = \max_{x \in \partial \Omega} f$.
    \end{proof}
    
    \begin{proposition}
    	For $p, g > 1$ satisfying $\frac{1}{p} + \frac{1}{g} = 1$,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g}|b|^g
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	\begin{align}
    		(-\log) |ab| &= (-\log) |a| + (-\log) |b| \\
    		&= \frac{1}{p} (-\log) |a|^p + \frac{1}{g} (-\log) |b|^p \\
    		(\because (-\log) \tx{ is convex})\ &\geq (-\log)\left( \frac{1}{p} |a|^p + \frac{1}{g} |b|^p \right)
    	\end{align}
    	And since $(-\log)$ is monotonically decreasing,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g} |b|^p
    	\end{align}
    \end{proof}
    
    \begin{corollary}
    	\begin{align}
    		|ab| \leq \frac{|a|^2 + |b|^2}{2}
    	\end{align}
    \end{corollary}
	
	\section{Finite Dimensional Optimization}
	\subsection{Unconstraint Optimization}
    \begin{theorem}[Extreme Value Theorem]
        Let $f: \R^n \to \R$ is \ul{continuous} and $K \subseteq \R^n$ be a \ul{compact} set, then the minimization problem $\min_{x \in K} f(x)$ has a solution.
    \end{theorem}
    
    \begin{remark}
    	$f: \Omega \to \R$ is convex does not imply $f$ is continuous.
    \end{remark}
    
    \begin{proposition}
    	A convex function $f$ defined on a \ul{convex open} set is continuous.
    \end{proposition}
    
    \begin{proof}
    	Let $f: \Omega \to \R$ be a convex function, where $\Omega \subseteq \R^n$ is open.
    	\todo{Is this true?}
    \end{proof}
    
    \begin{proposition}
    	A convex function $f$ defined on an \ul{open interval} in $\R$ is continuous.
    \end{proposition}
    
    \begin{proof}
    	See homework 1. The proof involves squeeze theorem.
    \end{proof}
    
    \begin{proof}[Proof of EVT.]
    	Let $f: K \to \R$ be a continuous function defined on a compact set $K$.\\
    	WLOG, we only prove the existence of $\min f$, since the existence of max can be easily proven by applying the exact same argument on $-f$. \\
    	That is, we claim the infimum of $f(K)$ is attained within $K$. \\
    	Because $K$ is compact, the continuity of $f$ implies $f(K)$ is compact. \\
    	By the completeness axiom of $\R$, $m := \inf_{x \in K} f(x)$ is well-defined. There exists a sequence $(x_i) \subseteq K$, such that $(f(x_i)) \to m$. Because $K$ is compact, there exists a subsequence $(x_{ik})$ of $(x_i)$ converges to some limit $x^* \in K$. \\
    	Since $f$ is continuous, $(f(x_{ik})) \to f(x^*)$, which is a subsequence of the convergent sequence $(f(x_i))$, and they must converge to the same limit. Hence, $f(x^*) = m$, and the infimum is attained at $x^* \in K$.
    \end{proof}
    
    \begin{theorem}[Heineâ€“Borel]
    	Let $K \subseteq \R^n$, then the following are equivalent:
    	\begin{enumerate}[(i)]
    		\item $K$ is compact (every open cover of $K$ has a finite sub-cover);
    		\item $K$ is closed and bounded;
    		\item Every sequence in $K$ has a convergent subsequence converges to a point in $K$.
    	\end{enumerate}
    \end{theorem}
    
	\begin{proposition}
		Let $\{h_i\}$ and $\{g_i\}$ be sets of continuous functions on $\R^n$, the the set of all points in $\R^n$ that satisfy
		\begin{equation}
			\begin{cases}
				h_i(x) = 0\ \forall i\\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{equation}
		is a closed set. Moreover, if the qualified set is also bounded, then it is compact.
	\end{proposition}
	
	\begin{proof}
		For every equality constraint $h_i$, it can be represented as the conjunction of two inequality constraint, namely $h_i^\alpha (x) := -h_i(x) \leq 0 \land h_i^\beta (x) := h_i(x) \leq 0$. Then the constraint collection is equivalent to
		\begin{align}
			\begin{cases}
				h_i^\alpha (x) \leq 0\ \forall i \\
				h_i^\beta (x) \leq 0\ \forall i \\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{align}
		The subset of $\R^n$ qualified by each individual constraint is closed by the property of continuous functions (i.e. a continuous function's pre-image of closed set is closed). And the intersection of arbitrarily many closed sets is closed.
	\end{proof}
	
    \begin{remark}
    	Computer algorithms for solving minimization problems try to construct a sequence of $(x_i)$ such that $f(x_i)$ decreases to $\min f$ rapidly.
    \end{remark}
    
%    \begin{corollary}
%        Let $f: \R^n \to \R$ be a continuous function, if there exists $a \in \R^n$ such that $f(x) \geq f(a)$ for every $x \notin \mc{B}(r, a)$, then $f$ attains its minimum in $\mc{B}(r, a)$.
%    \end{corollary}

    \par The optimization problems investigated in this section can be formulated as
    \begin{align}
    	\min_{x \in \Omega} f(x)
    \end{align}
    where $\Omega \subseteq \R^n$. Typically, for simplicity, $\Omega$ are often $\R^n$, an open subset of $R^n$, or the closure of some open subset of $\R^n$.
    \par Everything above minimization discussed in this section is applicable to maximization as well using the proposition below.

    \begin{proposition}
        When $\Omega = \R^n$, the unconstrained minimization has the following properties
        \begin{enumerate}[(i)]
            \item $\argmax f = \argmin (-f)$;
            \item $\max f = - \min (-f)$
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}
    	Immediate by applying definitions of maximum and minimum.
    \end{proof}
   	
   	\begin{definition}
   		A function $f: \Omega \to \R$ has \textbf{local minimum} at $x_0 \in \Omega$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega,\  f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strictly local minimum} at $x_0$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega \backslash \{x_0\}\ f(x_0) < f(x)
   		\end{align}
   		$f$ attains \textbf{global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega\ f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strict global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega \backslash \{x_0\} \ f(x_0) < f(x)
   		\end{align}
   		\ul{Note that strict global minimum is always unique.}
   	\end{definition}
   	
   	\begin{theorem}[Necessary Condition for Local Minimum]
   		Let $C^1 \ni f: \Omega \to \R$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every \emph{feasible direction} $v$ at $x_0$,
   		\begin{align}
   			\nabla f(x_0) \cdot v \geq 0
   		\end{align}
   		\emph{This theorem serves as the primary defining property of local minimum.}
   	\end{theorem}
   	
   	\begin{definition}
   		For $x_0 \in \Omega \subseteq \R^n$,  $v \in \R^n$ is a \textbf{feasible direction} at $x_0$ if
   		\begin{align}
   			\exists \overline{s} > 0\ s.t.\ \forall s \in [0, \overline{s}], x_0 + s v \in \Omega
   		\end{align}
   	\end{definition}
   	
   	\begin{proof}[Proof of Necessary Condition]
   		Let $x_0 \in \Omega$ be a local minimum, and let $v$ be a 
   		Define auxiliary function $g(s) := f(x + sv)$. And since $g$ attains minimum at $s=0$, there exists some $\overline{s} > 0$ such that 
   		\begin{align}
   			g(s) - g(0) \geq 0\ \forall s \in [0, \overline{s}]
   		\end{align}
   		Therefore
   		\begin{align}
   			g'(0) := \lim_{s \to 0} \frac{g(s) - g(0)}{s - 0} \geq 0
   		\end{align}
   		The alternative form of derivative can be derived using chain rule as
   		\begin{align}
   			g'(0) = \nabla f(x + sv) \cdot v\ |_{s=0} = \nabla f(x) \cdot v
   		\end{align}
   		By combing the two identities above, $\nabla f(x) \cdot v \geq 0$.
   	\end{proof}
   	
   	\begin{proof}[Alternative Proof of Necessary Condition (not that rigorous)]
   		The prove is almost immediate, if there exists a feasible direction $v^*$ such that $\nabla f(x_0) \cdot v^* < 0$, for every $\varepsilon > 0$, one can construct $x' := x^* + s v^*$ with sufficiently small $s$ so that $x' \in V_\varepsilon(x^*) \cap  \Omega$ and $f(x') < f(x^*)$.
   	\end{proof}
   	
   	\begin{corollary}
   		When $\Omega$ is open, then $x_0$ is a local minimum $\implies \nabla f(x_0) = 0$.
   	\end{corollary}
   	\begin{proof}
   		Since $\Omega$ is open, any sufficiently small $v \neq 0$ such that both $v$ and $-v$ are feasible directions at $x_0$, applying the necessary condition on both $v$ and $-v$ provides the equality.
   	\end{proof}
   	
%   	\begin{example}
%   		Minimize $f(x, y)=x^{2}-x y+y^{2}-3 y$ over $\Omega = \R^2$.
%   	\end{example}
%   	
%   	\begin{example}
%   		Minimize $f(x, y)=x^{2}-x+y+x y$ over $\Omega = \max\{(x, y) \in \R^2: x, y \geq 0\}$.
%   	\end{example}
   	
   	\begin{theorem}[Second Order Necessary Condition for Local Minimum]
   		Let $C^2 \ni f: \Omega \to \R$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every non-zero feasible direction $v$ at $x_0$,
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) \cdot v \geq 0$;
   			\item $\nabla f(x_0) \cdot v = 0 \implies v^T \nabla^2 f(x_0) v \geq 0$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum and $v$ be a feasible direction at $\Omega$, and $s \in (0, \overline{s}]$. The first statement is the immediate result of the first order necessary condition. Now suppose $\nabla f(x_0) = 0$, by the Taylor's theorem,
   		\begin{align}
   			0 \leq f(x_0 + sv) - f(x_0) &= s \nabla f(x_0) \cdot v + \frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2) \\
   			&=\frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2)
   		\end{align}
   		Since $s^2 > 0$, divide both sides by $s^2$ and take limit,
   		\begin{align}
   			\lim_{s \to 0} \frac{f(x_0 + sv) - f(x_0)}{s^2} &= \lim_{s \to 0} 
   			\left \{\frac{1}{2} v^T \nabla^2 f(x_0) v + \frac{o(s^2)}{s^2} \right\}\\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + \lim_{s \to 0} \frac{o(s^2)}{s^2} \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v \geq 0
   		\end{align}
   	\end{proof}
   	
	\begin{example}
		$f(x, y) = x^2 - xy + y^2 - 3y: \Omega = \R^2 \to \R$. Then at $(x_0, y_0) = (1, 2)$, 
		\begin{align}
			\nabla f(x_0, y_0) &= (2x_0 - y, -x_0 + 2y_0 - 3) = (0, 0) \\
			\nabla^2 f(x_0, y_0) &= 
			\begin{pmatrix}
				2 & -1 \\ -1 & 2
			\end{pmatrix} \succcurlyeq 0 
		\end{align}
	\end{example}
	
	\begin{definition}
		Let $A \in \R^{n \times n}$, $A$ is 
		\begin{enumerate}[(i)]
			\item \textbf{Positive definite} (denoted as $A \succ 0$) if $x^T A x > 0\ \forall x \neq 0$, if and only if all eigenvalues $\lambda_i > 0$;
			\item \textbf{Positive Semi-definite} (denoted as $A \succcurlyeq 0$) if $x^T A x \geq\  \forall x \in \R^n$, if and only if all eigenvalues $\lambda_i \geq 0$.
		\end{enumerate}
	\end{definition}
   	
   	\begin{theorem}[Sylvester's Criterion]
   		Let $A \in \R^{n \times n}$ be a Hermitian matrix (i.e. $A = \overline{A^T}$)\footnote{$\overline{A^T}$ denotes the complex conjugate of the transpose, a matrix with \emph{real entries} is Hermitian if and only if it is symmetric.}, then
   		\begin{enumerate}
   			\item $A \succ 0$ $\iff$ all \emph{leading principal minors} have positive determinants;
   			\item $A \succcurlyeq 0$ $\iff$ all leading principal minors have non-negative determinants.
   		\end{enumerate}
   	\end{theorem}
   	
%   	\begin{example}
%   		Let $f(x, y) = x^2 - x + y + xy$ defined on $\Omega = \R^2_+$. Consider the  point $(x_0, y_0) = (1/2, 0)$ found, any feasible direction $(v, w)$ at $(x_0, y_0)$ satisfies $w \geq 0$. $\nabla f(1/2, 0) = (0 , 2/3)$.
%   		\begin{align}
%   			\nabla f(1/2, 0) &= (0, \frac{3}{2}).
%   		\end{align}
%   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Condition for Interior Local Minima]
   		Let $f: C^2(\Omega, \R)$, for some $x_0 \in \Omega$, if
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) = 0$,
   			\item \emph{(and)} $\nabla^2 f(x_0) \succ 0$.
   		\end{enumerate}
   		then $x_0$ is a \ul{strictly local minimizer}.
   	\end{theorem}
   	
   	\begin{lemma}
   		Suppose $\nabla^2 f(x_0)$ is positive definite, then 
   		\begin{align}
   			\exists\ a > 0\ s.t.\ v^T \nabla^2 f(x_0) v \geq a \norm{v}^2\quad \forall v
   		\end{align}
   		\emph{That is, the quadratic form of a positive definite matrix is bounded away from zero.}
   	\end{lemma}
   	
   	\begin{proof}[Proof of the Lemma]
   		Recall that a squared matrix $Q$ is called \textbf{orthogonal} when every column and row of it is an orthogonal unit vector. So that for every orthogonal matrix $Q$, $Q^T Q = I$, which implies $Q^T = Q^{-1}$. Further, note that 
   		\begin{align}
   			\norm{Qv}^2 &= (Qv)^T (Qv)
   			= v^T Q^T Q v 
   			= \norm{v}^2 \\
   			\implies \norm{Qv} &= \norm{v}\ \forall v \in \R^n
   		\end{align}
   		Let $v \in \R^n$, consider the eigenvector decomposition of $\nabla^2 f(x_0)$, let $w$ satisfy $v = Qw$:
   		\begin{align}
   			Q^T \nabla^2 f(x_0) Q &= \tx{diag}(\lambda_1, \cdots, \lambda_n) \\
   			\implies v^T \nabla^2 f(x_0) v &= (Qw)^T \nabla^2 f(x_0) (Qw) \\
   			&= w^T Q^T \nabla^2 f(x_0) Q w \\
   			&= w^T \tx{diag}(\lambda_1, \cdots, \lambda_n) w \\
   			&= \lambda_1 w_1^2 + \cdots + \lambda_n w_n^2
   		\end{align}
   		Let $a := \min\{\lambda_1, \cdots, \lambda_n\}$,
   		\begin{align}
   			... \geq a \norm{w}^2 = a\norm{Q^T v}^2 = a \norm{v}^2
   		\end{align}
   	\end{proof}
   	
   	\begin{proof}[Proof of the Theorem.]
   		Let $x \in \Omega$, suppose $\nabla f(x_0) = 0$ and $\nabla^2 f(x_0) \succcurlyeq 0$. By the second order Taylor approximation,
   		\begin{align}
   			f(x_0 + v) - f(x_0) &= \nabla f(x_0)^T v + \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&\geq \frac{a}{2} \norm{v}^2 + o(\norm{v}^2) \tx{ for some }a > 0 \\
   			&= \norm{v}^2 \left (
   			\frac{a}{2} + \frac{o(\norm{v}^2)}{\norm{v}} \right) \\
   			&>0\ \tx{ for sufficiently small } v
   		\end{align}
   		Therefore, $f(x_0) < f(x)\ \forall x \in V_\varepsilon(x_0)$.
   	\end{proof}
   	
   	\subsection{Equality Constraints: Lagrangian Multiplier}
   	\subsubsection{Tangent Space to a (Hyper) Surface at a Point}
   	\begin{definition}
   		A \textbf{surface} $\mc{M} \subseteq \R^n$ is defined as
   		\begin{align}
   			\mc{M} := \left \{
   			x \in \R^n : h_i (x) = 0\ \forall i
   			\right \}
   		\end{align}
   		where $h_i$ are all $C^1$ functions.
   	\end{definition}
   	
   	\begin{definition}
   		A \textbf{differentiable curve} on a surface $\mc{M}$ is a $C^1$ function mapping from $(-\varepsilon, \varepsilon)$ to $\mc{M}$. \\
   		\emph{Remark: in previous calculus courses, differentiable curves are often referred to as parameterizations.}
   	\end{definition}
   	Let $x(s)$ be a differentiable curve on $\mc{M}$ passes through $x_0 \in \mc{M}$, re-parameterize it so that $x(0) = x_0$. Then vector
   	\begin{align}
   		v := \frac{d}{ds} \bigg \vert_{s=0} x(s)
   	\end{align}
   	touches $\mc{M}$ \emph{tangentially}.
   	
   	\begin{definition}
   		Any vector $v$ generated by some differentiable curve on $\mc{M}$ and takes above form is a $\textbf{tangent vector}$ on $\mc{M}$ through $x_0$.
   	\end{definition}
   	
   	\begin{definition}
   		The \textbf{tangent space} to $\mc{M}$ at $x_0$ is defined to be the set of all tangent vectors:
   		\begin{align}
   			T_{x_0} \mc{M} := \left \{
   			v \in \R^n
   			:
   			v := \frac{d}{ds} \bigg \vert_{s=0} x(s) \tx{ for some } x \in C^1((-\varepsilon, \varepsilon), \mc{M})\ s.t.\ x(0) = x_0
   			\right \}
   		\end{align}
   	\end{definition}
   	
   	\begin{example}
   		Define 
   		\begin{align}
   			\mc{M} := \left\{x \in \R^2: \norm{x}_2 = 1 \right\}
   		\end{align}
   		By defining $C^1$ functions $g(x) := \norm{x}^2_2  - 1$, $\mc{M}$ is a surface. The tangent space of $\mc{M}$ at $x_0$ is
   		\begin{align}
   			T_{x_0} \mc{M} = \left \{
   			v \in \R^n:
   			\inner{v}{x_0} = 0
   			\right \}
   		\end{align}
   	\end{example}
   	
   	\begin{definition}
   		Let $\mc{M}$ be a surface defined using $C^1$ functions, a point $x_0 \in \mc{M}$ is a \ul{\textbf{regular point} of the constraints} if 
   		\begin{align}
   			\{\nabla h_1(x_0), \cdots, \nabla h_k(x_0)\}
   		\end{align}
   		are linearly independent. \\
   		\emph{Remark: if there is only one constraint $h$, then $x_0$ is regular if and only if $\nabla h(x_0) \neq 0$.}
   	\end{definition}
   	
   	\begin{notation}
   		Define the $T$ space on equality constraint as
   		\begin{align}
   			T_{x_0} := \{x \in \R^n: \inner{x_0}{\nabla h_i(x_0)} = 0\ \forall i \in [k]\}
   		\end{align}
   	\end{notation}
   	
   	\begin{example}[Counter example]
   		Define
   		\begin{align}
   			\mc{M} := \left \{
   			(x, y) \in \R^2:
   			h(x,y) = xy = 0
   			\right \}
   		\end{align}
   		Then it is easy to verify that $(0,0)$ is not a regular point. And
   		\begin{align}
   			T_{0,0} &= \{(x, y) \in \R^2: (x, y) \cdot (0,0) = 0\} = \R^2 \\
   			\neq T_{0,0}\mc{M} &= \{(x, y) \in \R^2: x = 0 \lor y = 0\}
   		\end{align}
   	\end{example}
   	
   	\begin{theorem}
   		Suppose $x_0$ is a \emph{regular point} of $\mc{M} := \{h_i(x) = 0, i=1,\cdots,k\}$, then $T_{x_0} = T_{x_0} \mc{M}$.
   	\end{theorem}
   	
   	\begin{proof}
   		\emph{Show} $T_{x_0} \mc{M} \subseteq T_{x_0}$.\\
   		Suppose $x_0$ is a regular point of $\mc{M}$.
   		Let $v \in T_{x_0} \mc{M}$, then there exists some differentiable curve $x(\cdot):V_\varepsilon(0) \to \mc{M}$ such that $x(0) = x_0$, such that
   		\begin{align}
   			v &= \frac{d}{ds} \bigg \vert_{s=0} x(s)
   		\end{align}
   		Note that $h_i(x(s)) = 0$ is constant for every $i \in [k]$, therefore
   		\begin{align}
   			\frac{d}{ds} \bigg \vert_{s=0} h_i(x(s))
   		\end{align}
   		By the chain rule, 
   		\begin{align}
   			\nabla h_i(x_0)\cdot v = 0\ \forall i
   		\end{align}
   		Therefore $v \in T_{x_0}$. \\
   		\emph{Show} $T_{x_0} \subseteq T_{x_0} \mc{M}$.
   		\begin{enumerate}[(i)]
   			\item $x_0$ is regular $\implies T_{x_0} \mc{M}$ is a vector space;
   			\item $T_{x_0} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$.
   		\end{enumerate}
   		\emph{Show} $T_{x_0} \subseteq \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$: \\
   		Let $v \in T_{x_0}$, then $v \perp \nabla h_i(x_0)$ for every $i$. Therefore $v$ is orthogonal to every linear combination of $\nabla h_i(x_0)$, and therefore orthogonal to the span.\\
   		\emph{Show} $\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp \subseteq T_{x_0}$: \\
   		Let $v$ in the perp of the span, then $v$ is orthogonal to every basis of the span, so $v \in T_{x_0}$.
   	\end{proof}
   	
   	\begin{lemma}
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on \ul{open} subset $\Omega \subseteq \R^n$. Define $\mc{M} := \{x \in \R^n: h_i(x) = 0\ \forall i \}$. Suppose $x_0 \in \mc{M}$ is a local minimum of $f$ on $\mc{M}$, then 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   	\end{lemma}
   	
   	\begin{proof}
   		WLOG $\Omega = \R^n$, take $v \in T_{x_0} \mc{M}$. Then there exists some differentiable curve $x$ on $\mc{M}$ satisfying $v = x'(0)$. Because $x_0$ is a local minimum of $f$ on $\Omega$, $s=0$ is a local minimum of $f(x(s))$, moreover, it is an interior minimum. By chain rule and the necessary condition of local minimum,
   		\begin{align}
   			D f(x(0)) &= \nabla f (x(0)) \cdot x'(0) = 0 \\
   			\implies \nabla f(x_0) \cdot v &= 0
   		\end{align}
   		Therefore $\nabla f(x_0) \perp T_{x_0} \mc{M}$.
   	\end{proof}
   	
   	\begin{theorem}[Lagrange Multipliers: First Order Necessary Condition]
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on open subset $\Omega \subseteq \R^n$. Let $x_0$ be a regular point of the constraint set $\mc{M} := \bigcap_{i=1}^k h^{-1}_i(0)$. Suppose $x_0$ is a local minimum of $\mc{M}$, then there exists $\lambda_1, \cdots, \lambda_k \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0
   		\end{align}
   		\emph{Remark: if we define Lagrangian $\mc{L}(x, \lambda_i) := f(x) + \sum_{i=1}^k h_i(x)$, then the theorem says the local minimum is a critical point of $\mc{L}$.}
   	\end{theorem}
   	
   	\begin{proof}
   		Because $x_0$ is a regular point, then by previous lemma, $\nabla f(x_0) \perp T_{x_0} \mc{M}$. Moreover,
   		\begin{align}
   			T_{x_0} \mc{M} = T_{x_0} = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^\perp
   		\end{align}
   		Also, because $x_0$ is a local minimum, 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   		Therefore, $\nabla f(x_0) \in (T_{x_0} \mc{M})^\perp = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^{\perp \perp} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}$, where the last equality holds in finite dimensional cases. Hence, it is obvious that we can write $\nabla f(x_0)$ as a linear combination of $\{\nabla h_i(x_0)\}$.
   	\end{proof}
   	
   	\begin{theorem}[Second Order Necessary Condition]
   		Let $f, h_i \in C^2$, if $x_0$ is a local minimum on previously defined surface $\mc{M}$, then there exists Lagrangian multipliers $\{\lambda_i\}$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0$ ($\nabla_x \mc{L} = 0$);
   			\item And $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) \succcurlyeq 0$ \red{on $T_{x_0} \mc{M}$} ($\nabla_x^2 \mc{L} \succcurlyeq 0$).
   		\end{enumerate}
   		\emph{Remark: whenever $x_0$ is a local minimum, it must be a critical point of $\mc{L}$, and $\mc{L}$ is positive semidefinite on the tangent space at $x_0$.}
   	\end{theorem}
   	
   	\begin{proof}
   		The first result is exactly the same as the first order condition proven above. \\
   		To show the second result, let $x(s) \in \mc{M}$ be an arbitrary differentiable curve on $\mc{M}$ such that $x(0) = x_0$. Then,
   		\begin{align}
   			\frac{d}{ds} f(x(s)) &= \nabla f(x(s)) \cdot x'(s) \\
   			\frac{d^2}{ds^2} f(x(s)) &= x'(s)^T \nabla^2 f(x(s)) x'(s) + \nabla f(x(s)) x''(s)
   		\end{align}
   		By the second order Taylor theorem, for every $s$ such that $x(s) \in \mc{M}$, 
   		\begin{align}
   			f(x(s)) - f(x_0) = s \nabla f(x_0) \cdot x'(0) + \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Note that by definition, $x'(0)$ is in the tangent space at $x_0$. Also, we've shown previously that $\nabla f(x_0)$ is orthogonal to the tangent space at $x_0$, therefore,
   		\begin{align}
   			f(x(s)) - f(x_0) = \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Also, by the definition of $\mc{M}$, all constraints hold with equality:
   		\begin{align}
   			f(x_0) &= f(x_0) + \sum_{i=1}^k \lambda_i h_i(x_0)
   		\end{align}
   		where $\lambda_i$'s are from the first result. Hence,
   		\begin{align}
   			f(x(s)) - f(x_0) &= \frac{s^2}{2} \left [
   			x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0)
   			+ \left(
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0)
   			\right)x''(0)
   			\right ] + o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2)
   		\end{align}
   		And above expression is greater or equal to zero because $x_0$ is a local minimum,
   		\begin{align}
   			\frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2) &\geq 0 \\
   			\implies x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + \frac{o(s^2)}{s^2} &\geq 0 \\
   			\overset{s\to 0}{\implies}x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) &\geq 0
   		\end{align}
   		Where $x'(0)$ is a vector in the tangent space at $x_0$ by definition. Moreover, the curve $x(s)$ was chosen arbitrarily, so the argument works for every curve and therefore every tangent vector, and what's desired is shown.
   	\end{proof}
   	
   	\begin{example}
   		\begin{align}
   			\min f(x, y) &= x^2 - y^2 \\
   			s.t.\ h(x, y) &= y = 0
   		\end{align}
   		First order condition suggests $(x_0, y_0) = (0, 0)$ Note that the tangent space at $(x_0, y_0)$ is $\tx{span}\{\nabla h_i\}^\perp$:
   		\begin{align}
   			T_{x_0}\mc{M} = \{ (u, 0): u \in \R\}
   		\end{align}
   		and 
   		\begin{align}
   			\nabla_x^2 \mc{L} = \begin{pmatrix}
   				2 & 0 \\ 0 & -2
   			\end{pmatrix}
   		\end{align}
   		is obviously positive semidefinite (actually positive definition) on the tangent space.
   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Conditions]
   		Let $f, h_i \in C^2$ on open $\Omega \subseteq \R^n$, and $x_0 \in \mc{M}$ is a regular point, if there exists $\lambda_i \in \R$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla_x \mc{L}(x_0, \lambda_i) = 0$;
   			\item $\nabla_x^2 \mc{L}(x_0, \lambda_i) \succ 0$ \red{on $T_{x_0} \mc{M}$},
   		\end{enumerate}
   		then $x_0$ is a \emph{strict} local minimum.
   	\end{theorem}
   	\begin{proof}
   		Recall that $\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)$ positive definite on $T_{x_0} \mc{M}$ implies there exists $a > 0$ ($a$ is taken to be equal to the least eigenvalue of $\nabla^2_x \mc{L}$) such that 
   		\begin{align}
   			v^T [\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)] v \geq a \norm{v}^2\quad \forall v \in T_{x_0} \mc{M}
   		\end{align}
   		Let $x(s) \in \mc{M}$ be a curve such that $x(0) = x_0$ and $v = x'(0)$. WLOG, $\norm{x'(0)} = 1$. By the second order Taylor expansion,
   		\begin{align}
   			f(x(s)) - f(x(0)) &= s \left. \frac{d}{ds} \right\vert_{s=0} f(x(s)) + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} f(x(s)) + o(s^2) \\
   			&= s \left. \frac{d}{ds} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + o(s^2) \\
   			&= s \nabla_x \mc{L}(x_0, \lambda_i) \cdot x'(0)
   			+ \frac{s^2}{2}
   			\left[x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + \nabla_x \mc{L}(x_0, \lambda_i) x''(0) \right]
   			+ o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + o(s^2) \\
   			&\geq \frac{s^2}{2} a \norm{x'(0)}^2 + o(s^2)\quad \tx{where } a > 0 \\
   			&= s^2 \left(\frac{a}{2} + \frac{o(s^2)}{s^2}\right) \\
   			&\overset{s \to 0}{>} 0
   		\end{align}
   		Therefore, for sufficiently small $s$, $f(x(s)) - f(x(0)) > 0$. And this is true for every curve $x$ on $\mc{M}$. So $x(0)$ is a strict local minimum.
   	\end{proof}
   	
   	\subsection{Remark on the Connection Between Constrained and Unconstrained Optimizations}
   	\begin{example}
   		Consider
   		\begin{align}
   			&\min f(x,y,z) \\
   			&s.t. g(x,y,z) = z - h(x,y) = 0
   		\end{align}
   		where $\mc{M}$ is the graph of $h$. Using Lagrangian multiplier provides necessary condition: $\nabla f + \lambda \nabla g = 0$,
   		\begin{align}
   			\begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix}
   			+ \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = 0
   		\end{align}
   		Convert the constrained optimization into an unconstrained optimization as 
   		\begin{align}
   			\min_{(x,y) \in \R^2} F(x, y) = f(x, y, h(x, y))
   		\end{align}
   		The necessary condition for unconstrained optimization is
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x + f_z h_x \\
   				f_y + f_z h_y
   			\end{pmatrix} \\
   			&= \begin{pmatrix}
   				f_x \\ f_y
   			\end{pmatrix} - f_z \begin{pmatrix}
   				-h_x \\ -h_y
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   		Define $\lambda := -f_z$.
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix} + \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   	\end{example}
   	\subsection{Inequality Constraints}
   	\begin{definition}
   		Let $x_0$ satisfy the set of constraints
   		\begin{align}
   			(\dag)\begin{cases}
	   			h_i(x) &= 0\quad i \in \{1, \cdots, k\} \\
   				g_j(x) &\leq 0\quad j \in \{1, \cdots, \ell\}
   			\end{cases}
   		\end{align}
   		we say that the constraint $g_i$ is \textbf{active} at $x_0$ if $g_i(x_0) = 0$, and is \textbf{inactive} at $x_0$ if $g_i(x_0) < 0$.
   	\end{definition}
   	
   	\begin{definition}
   		Split the collection of inequality constraints into active and inactive constraints, let $\Theta(x_0)$ denote the collection of active indices, that's:
   		\begin{align}
   			g_j (x_0) = 0\ \forall j \in \Theta(x_0) \\
   			g_j (x_0) < 0\ \forall j \notin \Theta(x_0)
   		\end{align}
   		Then $x_0$ is said to be a \textbf{regular point} of the constraint if 
   		\begin{align}
   			\{\nabla h_i(x_0)\ \forall i \in \{1, \cdots, k\}; \underbrace{\nabla g_j(x_0)\ \forall j \in \Theta(x_0)}_{\tx{Active Constraints}} \}
   		\end{align}
   		is linearly independent. 
   	\end{definition}
   	
   	\begin{theorem}[The First Order Necessary Condition for Local Minimum: Kuhn-Tucker Conditions]
   		Let $\Omega$ be an open subset of $\R^n$ with constraints $h_i$ and $g_i$ to be $C^1$ on $\Omega$. Suppose $x_0 \in \Omega$ is a regular point with respect to constraints, further suppose $x_0$ is a local minimum, then there exists some $\lambda_i \in \R$ and $\mu_j \in \red{\R_+}$ such that 
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$ (i.e. $\nabla_x \mc{L}(x, \lambda, \mu) = 0$);
   			\item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
   		\end{enumerate}
   		\emph{Remark 1: by complementary slackness, all $\mu_j$ corresponding to inactive inequality constraints are zero.} \\
   		\emph{Remark 2: it is possible for an active constraint to have zero multiplier.}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum for $f$ satisfying constraints, equivalently, it is a local minimum for equality constraints and active inequality constraints. \\
   		By the first order necessary condition for local minimum with equality constraints, there exists $\lambda_i, \mu_j \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j \in \Theta(x_0)} \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		Then by setting $\mu_j = 0$ for all $j \notin \Theta(x_0)$ one have
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		By construction, the complementary slackness is satisfied. At this stage, we have construct $\lambda_i \in \R$ and $\mu_j \in \R$ satisfying both conditions, we still need to argue that $\mu_j \geq 0$ for every $j$.
   	\end{proof}
   	
	\begin{theorem}[The Second Order Necessary Conditions]
		Let $\Omega$ be an open subset of $\R^n$, and $f, h_1, \cdots, h_k, g_1, \cdots, g_\ell \in C^2(\R^n, \R)$. Let $x_0$ be a regular point of the constraints ($\dag$).
%		\begin{align}
%			(\dag ) \begin{cases}
%				h_i(x) = 0\quad \forall i \\
%				g_j(x) \leq 0\quad \forall j
%			\end{cases}
%		\end{align}
		Suppose $x_0$ is a local minimum of $f$ subject to constraint ($\dag$), then there exists $\lambda_i \in \R$ and $\mu_j \geq 0$ such that
		\begin{enumerate}[(i)]
			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
			\item $\mu_j g_j(x_0) = 0$;
			\item $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla^2 g_j(x_0)$ is \ul{positive semidefinite} on the tangent space to \ul{activate constraints} at $x_0$.
		\end{enumerate}
	\end{theorem}
	\begin{proof}
		(i) and (ii) are immediate result from the first order necessary condition. \\
		Suppose $x_0$ is a local minimum for ($\dag$), then $x_0$ is a local minimum for active constraints at $x_0$. \\
		Therefore, $\nabla^2 \hat{\mc{L}} = \nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) + \sum_{j \in I(x_0)} \mu_j \nabla^2 g_j(x_0)$ is positive semidefinite on the tangent space to active constraints. Note that because $\mu_j = 0$ for inactive constraints, therefore $\nabla^2 \hat{\mc{L}} = \nabla^2 \mc{L}$ at $x_0$, and both of them are positive semidefinite on the tangent space corresponding to active constraints.
	\end{proof}
	
	\begin{theorem}[The Second Order Sufficient Conditions]
		Let $\Omega$ be an open subset of $\R^n$, let $f, h_i, q_j \in C^2(\Omega)$. Consider minimizing $f(x)$ with the constraint
		\begin{align}
			(\dag ) \begin{cases}
				h_i(x) = 0\quad \forall i \\
				g_j(x) \leq 0\quad \forall j \\
				x \in \Omega
			\end{cases}
		\end{align}
		Suppose there exists a feasible $x_0$ satisfying $(\dag)$ and $\lambda_i \in \R$ and $\mu_j \in \R_{+}$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
   			\item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
   		\end{enumerate}
   		If the Hessian matrix for Lagrangian $\nabla_x^2 \mc{L}(x_0)$ is \ul{positive definite} on $\ttilde{T}_{x_0}$, the space of \textbf{strongly active} constraints at $x_0$, then $x_0$ is a \ul{strict} local minimum.
	\end{theorem}
	
	\begin{definition}
		A constraint $g_j$ is \textbf{strongly active} at $x_0$ if $g_j(x_0) = 0$ (so it is active) and $\mu_j > 0$. 
	\end{definition}
	
	\begin{notation}
		For convenience, we can rearrange the collection of constraints such that, among the $\ell$ constrains in total, the first $\ell'$ constraints are \emph{active} at $x_0$ and the first $\ell''$ constraints are \emph{strongly active}. Note that $\ell'' \leq \ell ' \leq \ell$. \\
		Define 
		\begin{align}
			\tilde{T}_{x_0} &:= \{
			v \cdot \nabla h_i(x_0) = 0\ \forall i \land v \cdot \nabla g_j(x_0)\tx{ for all $g_j$ active.}
			\} \\
			\ttilde{T}_{x_0} &:= \{
			v \cdot \nabla h_i(x_0) = 0\ \forall i \land v \cdot \nabla g_j(x_0) \tx{ for all $g_j$ strongly active.}
			\}
		\end{align}
		Clearly, $\tilde{T}_{x_0} \subseteq \ttilde{T}_{x_0}$ because there are (weakly) more active constraints than strongly active constraints.
	\end{notation}
	
	\begin{proof}[Proof of the Sufficient Condition]
		Suppose, for contradiction, $x_0$ is not a strict local minimum. \\
		\textbf{Claim 1:} There exists unit vector $v \in \R^n$ such that 
		\begin{enumerate}[(a)]
			\item $\nabla f(x_0) \cdot v \leq 0$;
			\item $\nabla h_i(x_0) \cdot v = 0$ for every $i$;
			\item $\nabla g_j(x_0) \cdot v \leq 0$ for all $j \leq \ell'$ (active constraints).
		\end{enumerate}
		\begin{proof}[Proof of Claim 1]
			Because $x_0$ is not a strictly local minimum, one can construct a sequence of feasible points $(x_k) \to x_0$ by setting $\varepsilon = \frac{1}{k}$ for every $k \in \N$ such that $f(x_k) \leq f(x_0)$. \\
			Let $v_k := \frac{x_k - x_0}{\norm{x_k - x_0}}$, $s_k := \norm{x_k - x_0}$. Note that every $v_k$ is in unit sphere, which is compact. Therefore, there exists a subsequence of $(v_k)$ converges to some unit vector $v$.
			\begin{align}
				0 \geq f(x_k) - f(x_0) = f(x_0 + s_k v_k) - f(x_0)\ \forall k \in \N
			\end{align}
			The first order Taylor series suggests the following holds for every $k \in \N$:
			\begin{align}
				0 &\geq f(x_0 + s_k v_k) - f(x_0) \\
				&= s_k \nabla f(x_0) \cdot v_k + o(s_k) \\
				0 &= h_i(x_0 + s_k v_k) - h_i(x_0) = s_k \nabla h_i(x_0) \cdot v_k + o (s_k) \\
				0 &\geq g_j(x_0 + s_k v_k) - g_j(x_0) = s_k \nabla g_j(x_0) \cdot v_k + o(s_k)\quad \forall j \leq \ell'
			\end{align}
			Above inequalities are preserved by limit operation, therefore,
			\begin{align}
				\nabla f(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla f(x_0) \cdot v &\leq 0 \\
				\nabla h_i(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla h_i(x_0) \cdot v &= 0 \\
				\nabla g_j(x_0) \cdot v_k + \frac{o(s_k)}{s_k} \to \nabla g_j(x_0) \cdot v &\leq 0\quad \forall j \leq \ell'
			\end{align}
		\end{proof}
		\textbf{Claim 2:} $\nabla g_j(x_0) \cdot v = 0$ for $j = 1, \cdots, \ell''$.
		\begin{proof}[Proof of Claim 2]
			Suppose not, there exists $j \in \{1, \cdots, \ell''\}$ such that $\nabla g_j(x_0) \cdot v < 0$. Then by (i),
			\begin{align}
				0 \geq \nabla f(x_0) \cdot v &= - \sum_{i=1}^k \lambda_i \nabla h_i(x_0) \cdot v - \sum_{j=1}^{\ell} \mu_j \nabla g_j(x_0) \cdot v \\
				&= - \sum_{j=1}^{\ell} \mu_j \nabla g_j(x_0) \cdot v > 0
			\end{align}
			the last inequality is from the fact that $\mu_j \nabla g_j (x_0) \cdot v \leq 0$ for all active constraints and $\mu_j = 0$ for all inactive constraints. \\
		\end{proof}
		(b) and claim 2 suggests $v \in \ttilde{T}_{x_0}$. \\
		By the second order Taylor approximation,
		\begin{align}
			0 \geq f(x_k) - f(x_0) &= s_k \nabla f(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 f(x_0) \cdot v_k + o(s_k^2) \\
			0 = h_i(x_k) - h_i(x_0) &= s_k \nabla h_i(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 h_i(x_0) \cdot v_k + o(s_k^2)\quad \forall i\\
			0 \geq g(x_k) - g(x_0) &= s_k \nabla g_j(x_0)\cdot v_k + \frac{s_k^2}{2} v_k \cdot \nabla^2 g_j(x_0) \cdot v_k + o(s_k^2)\quad \forall j \leq \ell'
		\end{align}
		Multiply the second equation by $\lambda_i$ and third equation by $\mu_j$, and use the fact that $\mu_j = 0$ for every $j > \ell'$. Also, given $\nabla \mc{L} = 0$ in (i): 
		\begin{align}
			0 &\geq \frac{s_k^2}{2} v_k \cdot \nabla^2 \mc{L} \cdot v_k + o(s_k^2)
		\end{align}
		Divide by $s_k^2$ and take the limit $(v_k) \to v$:
		\begin{align}
			v\cdot \nabla^2 \mc{L}\cdot v \leq 0
		\end{align}
		which contradicts the assumption that $\nabla^2 \mc{L}$ is positive definite in $\ttilde{T}_{x_0}$ because we've shown that $v \in \ttilde{T}_{x_0}$.
	\end{proof}
	\section{Iterative Algorithms for Optimization}
	\subsection{Newton's Method}
	\begin{example}[Motivation: a second order iterative algorithm]
		Let $f: I \subseteq \R \to \R$ where $I$ is an open interval. Let $x_i \in I$ be a starting point, consider the second order linear approximation of $f$ at $x_0$:
		\begin{align}
			g(x) &= f(x_0) + f'(x_0) (x - x_0) + \frac{1}{2} f''(x_0) (x - x_0)^2
		\end{align}
		By construction, the second order Taylor polynomial, $g(x)$, is the best second order approximation to $f$ at $x_0$ in the following sense:
		\begin{align}
			g(x_0) &= f(x_0) \\
			g'(x_0) &= f'(x_0) \\
			g''(x_0) &= f''(x_0)
		\end{align}
		The Newton's method aims to solve the critical point of $g(x)$ and define $x_1$ to be the critical point found:
		\begin{align}
			g'(x_1) &= f'(x_0) + f''(x_0) (x_1 - x_0) = 0 \\
			\implies x_1 &\leftarrow x_0 - \frac{f'(x_0)}{f''(x_0)}
		\end{align}
	\end{example}
	
	\begin{algorithm}[Newton's Method in $\R$]
		Given initial point $x_0 \in I$, while not terminated:
		\begin{align}
			x_{n+1} \leftarrow x_n - \frac{f'(x_n)}{f''(x_n)}
		\end{align}
	\end{algorithm}
	
	\begin{theorem}
		Let $f \in C^3$ on open interval $I \subseteq \R$. Suppose $x_* \in I$ satisfies $f'(x_*) = 0$ and $f''(x_*) \neq 0$, then the sequence of points $(x_n)$ generated by Newton's method converges to $x_*$ if \emph{$x_0$ is sufficiently close to $x_*$}.
	\end{theorem}
	
	\begin{example}
		Let $f(x) = x^2$, then $\frac{f'(x)}{f''(x)} = \frac{2x}{2}$. For any starting point $x_0$, $x_1 = x_0 - \frac{2x_0}{x_0} = 0$. That is, the algorithm converges to the global minimum in one iteration.
	\end{example}
	
	\begin{proof}
		Let $g(x) = f'(x)$ so that $x_{n+1} = x_n - \frac{g(x_n)}{g'(x_n)}$.\\
		Because $f \in C^3$, then $g \in C^2$. \\
		Note that by $g \in C^2$, $g' = f''$ is bounded away from zero near $x_*$. \\
		And by continuity again, $g'' = f^{(3)}$ is bounded near the bounded region $V_\varepsilon(x_*)$. \\
		That is, within small region near $x_*$, $V_\delta(x_*)$, there exists a sufficiently small $\alpha >0$ such that
		\begin{align}
			\begin{cases}
				\abs{g'(x_1)} > \alpha\ \forall x_1 \in V_\delta(x_*) \\
				\abs{g''(x_2)} < \frac{1}{\alpha}\ \forall x_2 \in V_\delta(x_*)
			\end{cases}
		\end{align}
		Further, note that $g(x_*) = f'(x_*) = 0$. \\
		WLOG, let $n \in \N$, suppose $x_n > x_*$:
		\begin{align}
			x_{n+1} - x_* &= x_n - \frac{g(x_n)}{g'(x_n)} - x_* \\
			&= x_n - x_* - \frac{g(x_n) - g(x_*)}{g'(x_n)} \\
			&= - \frac{g(x_n) - g(x_*) - g'(x_n)(x_n - x_*)}{g'(x_n)} \\
			&= - \frac{1}{2} \frac{g''(\xi)}{g'(x_n)} (x_n - x_*)^2\quad \tx{for some } \xi \in (x_*, x_n)
		\end{align}
		By taking the absolute values on both sides:
		\begin{align}
			\abs{x_{n+1} - x_*} &= \frac{1}{2} \frac{\abs{g''(\xi)}}{\abs{g'(x_n)}} \abs{x_n - x_*}^2 \\
			&< \frac{1}{2 \alpha^2} |x_n - x_*|^2
		\end{align}
		Let $\rho := \frac{1}{\alpha^2} \abs{x_0 - x_*}^2$, choose $x_0$ sufficiently close to $x_*$ such that $\rho < 1$.\\
		\emph{Remark: we are showing the iterative algorithm induces a contraction map.} \\
		Then,
		\begin{align}
			\abs{x_1 - x_*} &< \frac{1}{2\alpha^2}\abs{x_0 - x_*}^2 \\
			&= \frac{1}{2\alpha^2}\abs{x_0 - x_*} \abs{x_0 - x_*} \\
			&= \rho \abs{x_0 - x_*}
		\end{align}
		Inductively,
		\begin{align}
			\abs{x_2 - x_*} &< \frac{1}{2 \alpha^2} \abs{x_1 - x_*}^2 \\
			&< \frac{1}{2 \alpha^2} \rho^2 \abs{x_0 - x_*}^2 \\
			&= \rho^3 \abs{x_0 - x_*} \\
			&< \rho^2 \abs{x_0 - x_*}
		\end{align}
		By induction,
		\begin{align}
			\abs{x_n - x_*} < \rho^2 \abs{x_0 - x_*}
		\end{align}
		Therefore, as $n \to \infty$, $(x_n) \to x_*$.
	\end{proof}
	\begin{theorem}[2nd Order MVT]
		\begin{align}
			g(x) &= g(y) + g'(y) (x - y) + \frac{1}{2} g''(\xi) (x - y)^2\quad \xi \in (x, y)
		\end{align}
	\end{theorem}
	
	\begin{algorithm}[Newton's Method in $\R^n$]
		Let $f: \Omega \subseteq \R^n \to \R$ where $\Omega$ is open, let initial point $x_0 \in \Omega$. \\
		Suppose $\nabla^2 f(x_n)$ is invertible for every generated $n$, and $\nabla f(x_*) = 0$ so that algorithm stops at minimum. \\
		The iterative algorithm is defined as following:
		\begin{align}
			x_{n+1} \leftarrow x_n - [\nabla^2 f(x_n)]^{-1} \nabla f(x_n)
		\end{align}
	\end{algorithm}
	
	\begin{theorem}[Generalization]
		Suppose $x_* \in \Omega$ and $f \in C^3(\Omega, \R)$ such that $\nabla f(x_*) = 0$ and $\nabla^2 f(x_*)$ is invertible.\todo{check this} Then if initial point $x_0$ is sufficiently closed to $x_*$, then Newton's method converges to $x_*$.
	\end{theorem}
	
	\begin{proof}
		The basic idea is the same as the $\R$ case: prove the iterative algorithm induces a contraction mapping.
	\end{proof}
	
	\begin{example}[Newton's Method Fails to Converge]
		Even if $f$ has an unique global minimum $x_*$, and $x_0$ is arbitrarily close to the $x_*$, Newton's method could fail to converge. \\
		Consider
		\begin{align}
			f(x) = \frac{2}{3} \abs{x}^{\frac{3}{2}}
		\end{align}
		Note that
		\begin{align}
			f(x)&=\begin{cases}{\frac{2}{3} x^{\frac{3}{2}}} & {x \geq 0} \\ {-\frac{2}{3} x^{\frac{3}{2}}} & {x<0}\end{cases} \\
			f'(x) &= \begin{cases}
				x^{\frac{1}{2}} & x \geq 0 \\
				- x^{\frac{1}{2}} & x < 0
			\end{cases} \\
			f''(x) &= \begin{cases}
				\frac{1}{2}x^{-\frac{1}{2}} & x > 0 \\
				- \frac{1}{2}x^{-\frac{1}{2}} & x < 0 \\
				\tx{DNE} & x = 0
			\end{cases}
		\end{align}
		Therefore $f \notin C^2$. \\
		Let $\delta > 0$ arbitrarily small, take initial point $x_0 \in V_\delta(0)$. WLOG, $x_0 = \varepsilon \in V_\delta(0)$ with $\varepsilon > 0$. The algorithm will oscillate between $\pm \varepsilon$ and never converge.
	\end{example}
	
	\begin{remark}
		Newton's method does not necessarily converge to a global minimum, it may converge to local minimum or local maximum or even saddle point.
	\end{remark}
	
	\begin{example}[Newton's Method Converges to a Saddle Point]
		Consider $f(x) = x^3$, $x_{n+1} \to \frac{x_n}{2}$, which converges to 0 (a saddle point).
	\end{example}
	
	\begin{example}[Newton's Method on Quadratic Function]
		Let $Q$ be a symmetric $n \times n$ invertible matrix. Define quadratic form $f(x) := \frac{1}{2}x^T Q x: \R^n \to \R$. The optimal is $x = 0$. \\
		Let $x_0 \in \R^n$, then $x_1 := x_0 - H_f(x_0)^{-1} \nabla f(x_0) = x_0 - Q^{-1}Qx_0 = 0$. \emph{Therefore, Newton's method converges in one iteration.} 
	\end{example}
	
	\subsection{Steepest/Gradient Descent}
	\begin{algorithm}[Steepest Descent]
		Let $f: \Omega \to \R$ where $\Omega$ is an open subset of $\R^n$. Let initial point $x_0 \in \Omega$. \\
		To minimize $f$ on $\Omega$, iteratively update $x$ follows at each step $k$:
		\begin{align}
			x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)
		\end{align}
		where $\alpha_k = \argmin_{\alpha \geq 0} f(x_k - \alpha \nabla f(x_k)$. \\
		\emph{Remark: There might be multiple minimizing $\alpha$, in real world implementations, we take the least minimizer found.}
	\end{algorithm}
	
	\begin{theorem}[Gradient Descending is Descending]
		At every step $k$, if $\nabla f(x_k) = 0$, the algorithm terminates. Otherwise,
		\begin{align}
			f(x_{k+1}) < f(x_k)
		\end{align}
	\end{theorem}
	\begin{proof}
		Suppose $\nabla f(x_k) \neq 0$. \\
		Note that for the first minimizing $\alpha_k$ found:
		\begin{align}
			f(x_{k+1}) &= f(x_k - \alpha_k \nabla f(x_k)) \\
			&\leq f(x_k - \alpha \nabla f(x_k))\quad \forall 0 \leq \alpha \leq \alpha_k
		\end{align}
		Recall that
		\begin{align}
			\left. \frac{d}{ds} \right|_{s=0} f(x_k - s\nabla f(x_k)) = - \nabla f(x_k) \cdot \nabla f(x_k) = - \norm{\nabla f(x_k)}_2^2 < 0
		\end{align}
		Therefore,
		\begin{align}
			f(x_{k+1}) \leq f(x_k - \alpha \nabla f(x_k)) < f(x_k) \tx{ for small }\alpha
		\end{align}
	\end{proof}
	
	\begin{theorem}[Gradient Descending Induces Perpendicular Steps]
		The consecutive steps induced by gradient descending are perpendicular. That is
		\begin{align}
			(x_{k+2} - x_{k+1}) \cdot (x_{k+1} - x_k) = 0
		\end{align}
	\end{theorem}

	\begin{proof}
		Note that
		\begin{align}
			(x_{k+2} - x_{k+1}) \cdot (x_{k+1} - x_k) &= (- \alpha_{k+1} \nabla f(x_{k+1})) \cdot  (- \alpha_{k} \nabla f(x_{k})) \\
			&= \alpha_k \alpha_{k+1} \nabla f(x_{k}) \cdot \nabla f(x_{k+1})
		\end{align}
		If $\alpha_k = 0$, done. \\
		If $\alpha_k > 0$,
		\begin{align}
			f(x_{k+1}) &= f(x_k) - \alpha_k \nabla f(x_k) \\
			&= \min_{\alpha \geq 0} \{f(x_k - \alpha \nabla f(x_k))\} \\
			&= \min_{\red{\alpha > 0}} \{f(x_k - \alpha \nabla f(x_k))\} \\
			&\implies \left. \pd{}{\alpha} \right|_{\alpha = \alpha_k} f(x_k - \alpha \nabla f(x_k)) = 0 \\
			&\implies - \nabla f(x_k - \alpha_k \nabla f(x_k)) \cdot \nabla f(x_k) = 0 \\
			&\implies - \nabla f(x_{k+1}) \cdot \nabla f(x_k) = 0
		\end{align}
	\end{proof}
	
	\begin{theorem}[Sufficient Condition for Gradient Descent to Converge]
		Let $f \in C^1$ on open $\Omega \subseteq \R^n$. \\
		Let $\{x_k\}$ be the sequence generated by gradient descent: $x_{k+1} \leftarrow x_k - \alpha_k \nabla f(x_k)$. \\
		If $(x_k)$ is bounded in $\Omega$, that is, there exists a \ul{compact} set $K \subseteq \Omega$ such that $(x_k) \subseteq K$, \\
		then every convergent subsequence of $(x_k)$ converges to a critical point $x_* \in \Omega$ of $f$. 
	\end{theorem}
	
	\begin{proof}
		\todo{Need to fix this proof.}
		Let $x_k \in K$ compact. \\
		Then there exists subsequence $x_{k_i} \to x_* \in K$. \\
		\emph{Show:} $\nabla f(x_*) = 0$. \\
		Note that $f(x_k) \geq f(x_{k+1})$ for every $k \in \N$, therefore $f(x_{k_i}) \searrow f(x_*)$. Therefore, $f(x_{k}) \searrow f(x_*)$.
		\todo{Show this}.
		Suppose, for contradiction, $\nabla f(x_*) \neq 0$. \\
		By continuity of $\nabla f$, $(\nabla f(x_{k_i})) \to \nabla f(x_*)$. \\
		Let $y_{k_i} := x_{k_i} - \alpha_{k_i} \nabla f(x_{k_i}) = x_{k_{i+1}}$. \\
		Note that $y_{k_i}$ has a convergent subsequence converging to $y_*$. \\
		WLOG, $(y_{k_i}) \to y_*$. \\
		Observe
		\begin{align}
			\alpha_{k_i} &= \frac{\abs{y_{k_i} - x_{k_i}}}{\norm{\nabla f(x_{k_i})}} \\
			\implies \lim_{k_i \to \infty} a_{k_i} &= \frac{\abs{y_* - x_*}}{\norm{\nabla f(x_*)}} =: \alpha_*
		\end{align}
		Put back: $y_* = x_* - \alpha_* \nabla f(x_*)$. \\
		Now $f(y_{k_i}) = f(x_{k_{i+1}}) = \min_{\alpha \geq 0} f(x_{k_i} - \alpha \nabla f(x_{k_i}))$, which implies
		\begin{align}
			f(y_{k_i}) &\leq f(x_{k_i} - \alpha f(x_{k_i}))\ \forall \alpha \geq 0 \\
			\forall \alpha \geq 0\ \lim_{i \to \infty} f(y_{k_i}) &= f(y_*) \leq \lim_{i \to \infty} f(x_{k_i} - \alpha f(x_{k_i})) = f(x_* - \alpha \nabla f(x_*)) \\
			\implies f(y_*) &\leq \min_{\alpha \geq 0} f(x_* - \alpha \nabla f(x_*)) < f(x_*)
		\end{align}
		Further note that 
		\begin{align}
			f(y_*) = \lim_{i \to \infty} f(y_{k_i}) = \lim_{i \to \infty} f(x_{k_{i+1}}) = f(x_*)
		\end{align}
		Contradiction.
	\end{proof}
\end{document}













